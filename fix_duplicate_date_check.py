#!/usr/bin/env python3
"""
ëª¨ë“  ìŠ¤í¬ë˜í¼ì˜ ì¤‘ë³µ ì²´í¬ ë¡œì§ì— ë‚ ì§œ í™•ì¸ ì¶”ê°€
changwon_scraper.jsì™€ ë™ì¼í•œ ë¡œì§ì„ ë‹¤ë¥¸ ëª¨ë“  ìŠ¤í¬ë˜í¼ì— ì ìš©
"""

import os
import re
import glob

def fix_scraper_duplicate_check(filepath):
    """ìŠ¤í¬ë˜í¼ íŒŒì¼ì˜ ì¤‘ë³µ ì²´í¬ ë¡œì§ ìˆ˜ì •"""
    
    with open(filepath, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # ì´ë¯¸ ìˆ˜ì •ëœ íŒŒì¼ì¸ì§€ í™•ì¸
    if 'ì¤‘ë³µ ê²Œì‹œë¬¼ì´ì§€ë§Œ ë‚ ì§œê°€' in content:
        print(f"  âœ“ ì´ë¯¸ ìˆ˜ì •ë¨: {os.path.basename(filepath)}")
        return False
    
    # íŒ¨í„´ 1: processedTitles.has ì²´í¬í•˜ëŠ” ë¶€ë¶„ ì°¾ê¸°
    pattern1 = r'(if\s*\(\s*this\.processedTitles\.has\s*\([^)]+\)\s*(?:\|\|[^{]+)?\)\s*\{[^}]*console\.log\s*\([^)]*ì¤‘ë³µ[^)]*\);?\s*return\s+false;\s*\})'
    
    # íŒ¨í„´ 2: ë©”ëª¨ë¦¬ ê¸°ë°˜ ì²´í¬ í¬í•¨
    pattern2 = r'(// ë©”ëª¨ë¦¬ ê¸°ë°˜ ì²´í¬\s*\n\s*if\s*\(\s*this\.processedTitles\.has\s*\([^)]+\)\s*(?:\|\|[^{]+)?\)\s*\{[^}]*console\.log\s*\([^)]*ì¤‘ë³µ[^)]*\);?\s*return\s+false;\s*\})'
    
    modified = False
    
    # íŒ¨í„´ 2 ë¨¼ì € ì‹œë„ (ë” êµ¬ì²´ì )
    matches = list(re.finditer(pattern2, content))
    if not matches:
        # íŒ¨í„´ 1 ì‹œë„
        matches = list(re.finditer(pattern1, content))
    
    if matches:
        # ë§ˆì§€ë§‰ ë§¤ì¹˜ë¶€í„° ì—­ìˆœìœ¼ë¡œ ì²˜ë¦¬ (ì¸ë±ìŠ¤ ë³´ì¡´)
        for match in reversed(matches):
            old_code = match.group(1)
            
            # return false; ì•ì— ë‚ ì§œ ì²´í¬ ì½”ë“œ ì¶”ê°€
            new_code = old_code.replace(
                'return false;',
                '''// ì¤‘ë³µ ê²Œì‹œë¬¼ì´ì–´ë„ ë‚ ì§œ ì²´í¬ëŠ” ìˆ˜í–‰ (targetDate ì´ì „ì¸ì§€ í™•ì¸)
                // ë§ì€ ì¤‘ë³µì´ ì—°ì†ìœ¼ë¡œ ë‚˜íƒ€ë‚  ê²½ìš° ì¢…ë£Œ ì¡°ê±´ íŒë‹¨ì„ ìœ„í•¨
                if (this.targetDate && listDate) {
                    const targetMoment = moment(this.targetDate, 'YYYYMMDD');
                    if (listDate.isBefore(targetMoment)) {
                        console.log(`ì¤‘ë³µ ê²Œì‹œë¬¼ì´ì§€ë§Œ ë‚ ì§œê°€ ${listDate.format('YYYY-MM-DD')}ë¡œ ëŒ€ìƒ ë‚ ì§œ ì´ì „ì…ë‹ˆë‹¤. ì¢…ë£Œ ì‹ í˜¸.`);
                        return true; // ìŠ¤í¬ë˜í•‘ ì¤‘ë‹¨
                    }
                }
                return false;'''
            )
            
            content = content[:match.start()] + new_code + content[match.end():]
            modified = True
    
    if modified:
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(content)
        print(f"  âœ… ìˆ˜ì • ì™„ë£Œ: {os.path.basename(filepath)}")
        return True
    else:
        print(f"  âš ï¸ íŒ¨í„´ ëª» ì°¾ìŒ: {os.path.basename(filepath)}")
        return False

def main():
    """ëª¨ë“  ìŠ¤í¬ë˜í¼ íŒŒì¼ ì²˜ë¦¬"""
    
    # í˜„ì¬ ê²½ë¡œì—ì„œ node/scraper ë””ë ‰í† ë¦¬ ì°¾ê¸°
    scraper_dir = os.path.join(os.path.dirname(__file__), 'node', 'scraper')
    
    scraper_files = glob.glob(os.path.join(scraper_dir, '*_scraper.js'))
    
    # config.js ì œì™¸
    scraper_files = [f for f in scraper_files if not f.endswith('config.js')]
    
    print(f"\nğŸ”§ {len(scraper_files)}ê°œ ìŠ¤í¬ë˜í¼ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘\n")
    
    modified_count = 0
    skipped_count = 0
    failed_count = 0
    
    for filepath in sorted(scraper_files):
        try:
            if fix_scraper_duplicate_check(filepath):
                modified_count += 1
            elif 'ì¤‘ë³µ ê²Œì‹œë¬¼ì´ì§€ë§Œ ë‚ ì§œê°€' in open(filepath, 'r', encoding='utf-8').read():
                skipped_count += 1
            else:
                failed_count += 1
        except Exception as e:
            print(f"  âŒ ì˜¤ë¥˜: {os.path.basename(filepath)} - {e}")
            failed_count += 1
    
    print(f"\nğŸ“Š ì²˜ë¦¬ ê²°ê³¼:")
    print(f"  âœ… ìˆ˜ì •ë¨: {modified_count}ê°œ")
    print(f"  âœ“ ì´ë¯¸ ìˆ˜ì •ë¨: {skipped_count}ê°œ")
    print(f"  âš ï¸ ì²˜ë¦¬ ì‹¤íŒ¨: {failed_count}ê°œ")
    print(f"  ğŸ“ ì „ì²´: {len(scraper_files)}ê°œ")

if __name__ == "__main__":
    main()