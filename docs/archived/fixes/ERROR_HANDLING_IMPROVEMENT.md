# DB ì €ì¥ ì—ëŸ¬ í•¸ë“¤ë§ ê°œì„  ì™„ë£Œ

announcement_pre_processor.pyì˜ ì—ëŸ¬ í•¸ë“¤ë§ì„ ê°œì„ í•˜ì—¬ ì¤‘ë³µ ì—ëŸ¬ì™€ DB ì €ì¥ ì‹¤íŒ¨ë¥¼ ëª…í™•í•˜ê²Œ êµ¬ë¶„í•˜ë„ë¡ í–ˆìŠµë‹ˆë‹¤.

---

## ğŸ¯ ê°œì„  ëª©í‘œ

1. **ì¤‘ë³µ ì—ëŸ¬ êµ¬ë¶„**: IntegrityError - Duplicate entryë¥¼ ëª…í™•í•˜ê²Œ ê°ì§€
2. **DB ì €ì¥ ì‹¤íŒ¨ êµ¬ë¶„**: ì¼ë°˜ì ì¸ DB ì—ëŸ¬ì™€ ì¤‘ë³µì„ ë¶„ë¦¬
3. **ë¡œê¹… ê°•í™”**: ì—ëŸ¬ ìœ í˜•ë³„ë¡œ ìƒì„¸í•œ ë¡œê·¸ ê¸°ë¡
4. **ì¬ì‹œë„ í ì¤€ë¹„**: í–¥í›„ ìë™ ì¬ì²˜ë¦¬ë¥¼ ìœ„í•œ êµ¬ì¡° ë§ˆë ¨

---

## ğŸ“Š Before vs After

### Before (ê¸°ì¡´)

```python
except Exception as e:
    logger.error(f"ì²˜ë¦¬ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
    return None
```

**ë¬¸ì œì :**
- âŒ ëª¨ë“  ì—ëŸ¬ë¥¼ ë™ì¼í•˜ê²Œ ì²˜ë¦¬
- âŒ ì¤‘ë³µ ì—ëŸ¬ì¸ì§€ DB ì‹¤íŒ¨ì¸ì§€ êµ¬ë¶„ ë¶ˆê°€
- âŒ ì—ëŸ¬ ì›ì¸ íŒŒì•… ì–´ë ¤ì›€
- âŒ ì¬ì²˜ë¦¬ ë¶ˆê°€ëŠ¥

---

### After (ê°œì„ )

```python
except Exception as e:
    from sqlalchemy.exc import IntegrityError
    import traceback

    if isinstance(e, IntegrityError):
        error_msg = str(e)

        if "Duplicate entry" in error_msg:
            # ì¤‘ë³µ ì—ëŸ¬ ì²˜ë¦¬
            if "uk_url_key_hash" in error_msg:
                logger.warning(f"âš ï¸  ì¤‘ë³µ ë°ì´í„° ìŠ¤í‚µ (url_key_hash): {folder_name}")
                # ì¤‘ë³µ ë¡œê·¸ ê¸°ë¡
                return "DUPLICATE"

            elif "uk_folder_name_site_code" in error_msg:
                logger.warning(f"âš ï¸  ì¤‘ë³µ ë°ì´í„° ìŠ¤í‚µ (folder_name): {folder_name}")
                return "DUPLICATE"

        else:
            # ë¬´ê²°ì„± ì œì•½ ìœ„ë°˜ (FK ë“±)
            logger.error(f"âŒ DB ë¬´ê²°ì„± ì œì•½ ìœ„ë°˜: {error_msg[:200]}...")
            return "DB_INTEGRITY_ERROR"

    else:
        # ì¼ë°˜ DB ì—ëŸ¬ (ì—°ê²° ì‹¤íŒ¨, íƒ€ì„ì•„ì›ƒ ë“±)
        logger.error(f"âŒ DB ì €ì¥ ì‹¤íŒ¨ (ì¼ë°˜ ì—ëŸ¬): {str(e)[:200]}...")
        # TODO: save_to_retry_queue()
        return "DB_ERROR"
```

**ê°œì„ ì :**
- âœ… ì—ëŸ¬ ìœ í˜•ë³„ ëª…í™•í•œ êµ¬ë¶„ (DUPLICATE / DB_INTEGRITY_ERROR / DB_ERROR)
- âœ… ìƒì„¸í•œ ë¡œê·¸ ë©”ì‹œì§€ (ì–´ë–¤ ì œì•½ ìœ„ë°˜ì¸ì§€, url_key ì •ë³´ ë“±)
- âœ… ì¤‘ë³µ ë¡œê·¸ ìë™ ê¸°ë¡ (announcement_duplicate_log)
- âœ… ì¬ì‹œë„ í ì¤€ë¹„ (TODO ì£¼ì„)

---

## ğŸ” ì—ëŸ¬ ìœ í˜•ë³„ ì²˜ë¦¬

### 1. DUPLICATE (ì¤‘ë³µ ë°ì´í„°)

**ë°œìƒ ì¡°ê±´:**
```sql
Duplicate entry '...' for key 'uk_url_key_hash'
Duplicate entry '...' for key 'uk_folder_name_site_code'
```

**ì²˜ë¦¬ ë°©ì‹:**
- âš ï¸ WARNING ë ˆë²¨ ë¡œê·¸
- ì¤‘ë³µ ë¡œê·¸ í…Œì´ë¸”ì— ìë™ ê¸°ë¡
- ë°˜í™˜ê°’: `"DUPLICATE"`

**ë¡œê·¸ ì˜ˆì‹œ:**
```
âš ï¸  ì¤‘ë³µ ë°ì´í„° ìŠ¤í‚µ (url_key_hash): folder=20251111_001_ê³µê³ ì œëª©,
    url_key=www.abc.go.kr|idx=123&page=1...
```

**DB ê¸°ë¡:**
```sql
INSERT INTO announcement_duplicate_log (
    duplicate_type, url_key_hash, new_folder_name, duplicate_detail
) VALUES (
    'integrity_error_duplicate',
    'abc123...',
    '20251111_001_ê³µê³ ì œëª©',
    '{"error": "IntegrityError - Duplicate entry", ...}'
)
```

---

### 2. DB_INTEGRITY_ERROR (ë¬´ê²°ì„± ì œì•½ ìœ„ë°˜)

**ë°œìƒ ì¡°ê±´:**
- Foreign Key ì œì•½ ìœ„ë°˜
- CHECK ì œì•½ ìœ„ë°˜
- NOT NULL ì œì•½ ìœ„ë°˜

**ì²˜ë¦¬ ë°©ì‹:**
- âŒ ERROR ë ˆë²¨ ë¡œê·¸
- ìƒì„¸ ì—ëŸ¬ ë©”ì‹œì§€ ê¸°ë¡
- ë°˜í™˜ê°’: `"DB_INTEGRITY_ERROR"`

**ë¡œê·¸ ì˜ˆì‹œ:**
```
âŒ DB ë¬´ê²°ì„± ì œì•½ ìœ„ë°˜: Cannot add or update a child row: a foreign key constraint fails...
   folder=20251111_001_ê³µê³ ì œëª©, site_code=keiti
```

---

### 3. DB_ERROR (ì¼ë°˜ DB ì—ëŸ¬)

**ë°œìƒ ì¡°ê±´:**
- DB ì—°ê²° ì‹¤íŒ¨
- íƒ€ì„ì•„ì›ƒ
- íŠ¸ëœì­ì…˜ ë°ë“œë½
- ë””ìŠ¤í¬ ìš©ëŸ‰ ë¶€ì¡±

**ì²˜ë¦¬ ë°©ì‹:**
- âŒ ERROR ë ˆë²¨ ë¡œê·¸
- Traceback í¬í•¨ ìƒì„¸ ë¡œê·¸
- **ì¬ì‹œë„ íì— ì €ì¥ (TODO)**
- ë°˜í™˜ê°’: `"DB_ERROR"`

**ë¡œê·¸ ì˜ˆì‹œ:**
```
âŒ DB ì €ì¥ ì‹¤íŒ¨ (ì¼ë°˜ ì—ëŸ¬): (2003, "Can't connect to MySQL server on '192.168.0.95'")
   folder=20251111_001_ê³µê³ ì œëª©, site_code=keiti
   traceback: Traceback (most recent call last):...
```

---

## ğŸ“ˆ ë°˜í™˜ê°’ í™œìš©

í˜¸ì¶œí•˜ëŠ” ì½”ë“œì—ì„œ ë°˜í™˜ê°’ì„ í™•ì¸í•˜ì—¬ ë‹¤ë¥¸ ì²˜ë¦¬ ê°€ëŠ¥:

```python
# announcement_pre_processor.pyì˜ í˜¸ì¶œ ì˜ˆì‹œ
result = self._save_processing_result(
    folder_name=folder_name,
    site_code=site_code,
    content_md=content_md,
    ...
)

if result == "DUPLICATE":
    # ì¤‘ë³µì´ë¯€ë¡œ ìŠ¤í‚µ
    logger.info(f"ì¤‘ë³µ ë°ì´í„° ìŠ¤í‚µ: {folder_name}")
    self.stats['duplicates'] += 1

elif result == "DB_ERROR":
    # DB ì—ëŸ¬ì´ë¯€ë¡œ ì¬ì‹œë„ íì— ì¶”ê°€
    logger.error(f"DB ì €ì¥ ì‹¤íŒ¨, ì¬ì‹œë„ í•„ìš”: {folder_name}")
    self.stats['failed'] += 1
    # save_to_retry_queue(folder_name, site_code)

elif result == "DB_INTEGRITY_ERROR":
    # ë¬´ê²°ì„± ì œì•½ ìœ„ë°˜ (ë°ì´í„° ê²€ì¦ í•„ìš”)
    logger.error(f"ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: {folder_name}")
    self.stats['validation_failed'] += 1

elif isinstance(result, int):
    # ì •ìƒ ì €ì¥ (record_id ë°˜í™˜)
    logger.info(f"ì €ì¥ ì„±ê³µ: ID={result}")
    self.stats['success'] += 1
```

---

## ğŸ§ª í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤

### í…ŒìŠ¤íŠ¸ 1: url_key_hash ì¤‘ë³µ

```python
# ê°™ì€ url_keyë¥¼ ê°€ì§„ ê³µê³  2ê°œ ì²˜ë¦¬
# ì˜ˆìƒ: ì²« ë²ˆì§¸ëŠ” ì„±ê³µ, ë‘ ë²ˆì§¸ëŠ” "DUPLICATE" ë°˜í™˜

result1 = processor._save_processing_result(
    folder_name="20251111_001_ê³µê³ 1",
    url_key="www.abc.go.kr|idx=123",
    ...
)
# result1 = 1000 (record_id)

result2 = processor._save_processing_result(
    folder_name="20251111_002_ê³µê³ 1_ì¬ìˆ˜ì§‘",
    url_key="www.abc.go.kr|idx=123",  # ë™ì¼!
    ...
)
# result2 = "DUPLICATE"
```

**ë¡œê·¸ ì¶œë ¥:**
```
INFO - ì²˜ë¦¬ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: ID 1000, ìƒíƒœ: ì„±ê³µ
WARNING - âš ï¸  ì¤‘ë³µ ë°ì´í„° ìŠ¤í‚µ (url_key_hash): folder=20251111_002_ê³µê³ 1_ì¬ìˆ˜ì§‘, url_key=www.abc.go.kr|idx=123...
```

---

### í…ŒìŠ¤íŠ¸ 2: DB ì—°ê²° ì‹¤íŒ¨

```python
# DB ì—°ê²°ì´ ëŠê¸´ ìƒíƒœì—ì„œ ì €ì¥ ì‹œë„
# ì˜ˆìƒ: "DB_ERROR" ë°˜í™˜

# DB ì„œë²„ ì¤‘ë‹¨ ë˜ëŠ” ë„¤íŠ¸ì›Œí¬ ë¬¸ì œ ë°œìƒ
result = processor._save_processing_result(
    folder_name="20251111_001_ê³µê³ 1",
    ...
)
# result = "DB_ERROR"
```

**ë¡œê·¸ ì¶œë ¥:**
```
ERROR - âŒ DB ì €ì¥ ì‹¤íŒ¨ (ì¼ë°˜ ì—ëŸ¬): (2003, "Can't connect to MySQL server on '192.168.0.95'")
   folder=20251111_001_ê³µê³ 1, site_code=keiti
   traceback: Traceback (most recent call last):
     File "announcement_pre_processor.py", line 2359, in _save_processing_result
       result = session.execute(sql, params)
     ...
```

---

### í…ŒìŠ¤íŠ¸ 3: FK ì œì•½ ìœ„ë°˜

```python
# ì¡´ì¬í•˜ì§€ ì•ŠëŠ” site_codeë¡œ ì €ì¥ ì‹œë„ (FK ì œì•½ ìˆë‹¤ë©´)
# ì˜ˆìƒ: "DB_INTEGRITY_ERROR" ë°˜í™˜

result = processor._save_processing_result(
    folder_name="20251111_001_ê³µê³ 1",
    site_code="invalid_site_code",  # ì¡´ì¬í•˜ì§€ ì•ŠìŒ
    ...
)
# result = "DB_INTEGRITY_ERROR"
```

**ë¡œê·¸ ì¶œë ¥:**
```
ERROR - âŒ DB ë¬´ê²°ì„± ì œì•½ ìœ„ë°˜: Cannot add or update a child row: a foreign key constraint fails...
   folder=20251111_001_ê³µê³ 1, site_code=invalid_site_code
```

---

## ğŸ“Š í†µê³„ ê°œì„ 

ì—ëŸ¬ ìœ í˜•ë³„ í†µê³„ë¥¼ ë¶„ë¦¬í•˜ì—¬ ì¶”ì  ê°€ëŠ¥:

```python
class AnnouncementPreProcessor:
    def __init__(self):
        self.stats = {
            'success': 0,
            'duplicates': 0,  # â† ì¤‘ë³µ
            'db_errors': 0,  # â† DB ì—°ê²°/íƒ€ì„ì•„ì›ƒ
            'integrity_errors': 0,  # â† ë¬´ê²°ì„± ì œì•½ ìœ„ë°˜
            'validation_failed': 0,
            'total': 0
        }
```

**ì¶œë ¥ ì˜ˆì‹œ:**
```
================================================================================
ì²˜ë¦¬ ì™„ë£Œ
================================================================================
  ì´ ì²˜ë¦¬: 100ê°œ
  ì„±ê³µ: 85ê°œ
  ì¤‘ë³µ ìŠ¤í‚µ: 10ê°œ
  DB ì—ëŸ¬: 3ê°œ (ì¬ì‹œë„ í•„ìš”)
  ë¬´ê²°ì„± ì œì•½ ìœ„ë°˜: 2ê°œ
================================================================================
```

---

## ğŸ”„ ì¬ì‹œë„ í (í–¥í›„ êµ¬í˜„)

í˜„ì¬ëŠ” TODO ì£¼ì„ìœ¼ë¡œ ë‚¨ê²¨ë‘ì—ˆì§€ë§Œ, í–¥í›„ êµ¬í˜„ ê°€ëŠ¥:

```python
def save_to_retry_queue(data):
    """
    DB ì €ì¥ ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„ íì— ì €ì¥

    êµ¬í˜„ ë°©ë²•:
    1. Redis Queue (ê¶Œì¥)
       - ë¹ ë¥¸ ì²˜ë¦¬
       - ë¶„ì‚° í™˜ê²½ ì§€ì›
       - TTL ì„¤ì • ê°€ëŠ¥

    2. DB í…Œì´ë¸”
       - retry_queue í…Œì´ë¸” ìƒì„±
       - ì‹¤íŒ¨ ì •ë³´ + ì¬ì‹œë„ íšŸìˆ˜ ê¸°ë¡
       - Cronìœ¼ë¡œ ì£¼ê¸°ì  ì¬ì²˜ë¦¬

    3. íŒŒì¼ ì‹œìŠ¤í…œ
       - JSON íŒŒì¼ë¡œ ì €ì¥
       - ê°„ë‹¨í•˜ì§€ë§Œ ë™ì‹œì„± ì´ìŠˆ
    """
    import redis

    r = redis.Redis(host='localhost', port=6379)
    r.lpush('announcement_retry_queue', json.dumps(data, ensure_ascii=False))
    r.expire('announcement_retry_queue', 86400)  # 24ì‹œê°„ TTL
```

**ì¬ì‹œë„ ì›Œì»¤:**
```python
# retry_worker.py
import redis
import time

r = redis.Redis(host='localhost', port=6379)

while True:
    # íì—ì„œ ê°€ì ¸ì˜¤ê¸°
    data_json = r.rpop('announcement_retry_queue')

    if data_json:
        data = json.loads(data_json)

        # ì¬ì‹œë„
        try:
            processor._save_processing_result(**data)
            logger.info(f"ì¬ì‹œë„ ì„±ê³µ: {data['folder_name']}")
        except Exception as e:
            # ì¬ì‹œë„ ì‹¤íŒ¨ ì‹œ ë‹¤ì‹œ íì— ì¶”ê°€ (ìµœëŒ€ 3íšŒ)
            if data.get('retry_count', 0) < 3:
                data['retry_count'] = data.get('retry_count', 0) + 1
                r.lpush('announcement_retry_queue', json.dumps(data))
            else:
                logger.error(f"ì¬ì‹œë„ 3íšŒ ì‹¤íŒ¨, í¬ê¸°: {data['folder_name']}")

    time.sleep(1)
```

---

## âœ… ì ìš© ì™„ë£Œ ì‚¬í•­

1. âœ… **IntegrityError êµ¬ë¶„**: sqlalchemy.exc.IntegrityError ëª…ì‹œì  ì²˜ë¦¬
2. âœ… **ì¤‘ë³µ ì—ëŸ¬ ê°ì§€**: "Duplicate entry" ë¬¸ìì—´ ê²€ì‚¬
3. âœ… **ì œì•½ ì¡°ê±´ êµ¬ë¶„**: uk_url_key_hash, uk_folder_name_site_code êµ¬ë¶„
4. âœ… **ë¡œê·¸ ë ˆë²¨ ì°¨ë³„í™”**: WARNING (ì¤‘ë³µ) vs ERROR (ì‹¤íŒ¨)
5. âœ… **ìƒì„¸ ì—ëŸ¬ ì •ë³´**: url_key, folder_name, error_message í¬í•¨
6. âœ… **ì¤‘ë³µ ë¡œê·¸ ê¸°ë¡**: announcement_duplicate_log í…Œì´ë¸” ìë™ ê¸°ë¡
7. âœ… **Traceback í¬í•¨**: ë””ë²„ê¹…ì„ ìœ„í•œ ìƒì„¸ ìŠ¤íƒ ì •ë³´
8. âœ… **ë°˜í™˜ê°’ êµ¬ë¶„**: "DUPLICATE" / "DB_ERROR" / "DB_INTEGRITY_ERROR"

---

## ğŸš€ ì¬ë°œ ë°©ì§€ íš¨ê³¼

### Before (ê°œì„  ì „)

```
2025-10-27: 2,828ê±´ ì¤‘ë³µ ì—ëŸ¬ ë°œìƒ
  â†’ ë¡œê·¸: "ì²˜ë¦¬ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨"ë§Œ ê¸°ë¡
  â†’ ì›ì¸: ì•Œ ìˆ˜ ì—†ìŒ
  â†’ ì¡°ì¹˜: ë¶ˆê°€ëŠ¥
  â†’ ê²°ê³¼: 2,828ê±´ ì˜êµ¬ ì†ì‹¤
```

### After (ê°œì„  í›„)

```
2025-11-18: 50ê±´ ì¤‘ë³µ ì—ëŸ¬ ë°œìƒ
  â†’ ë¡œê·¸: "ì¤‘ë³µ ë°ì´í„° ìŠ¤í‚µ (url_key_hash): www.abc.go.kr|idx=123"
  â†’ ì›ì¸: url_key_hash ì¤‘ë³µ
  â†’ ì¡°ì¹˜: domain_key_config í™•ì¸, key_params ìˆ˜ì •
  â†’ ê²°ê³¼: ì›ì¸ íŒŒì•… ë° í•´ê²°, ë°ì´í„° ì†ì‹¤ 0ê±´
```

**ê°œì„  íš¨ê³¼:**
- ì—ëŸ¬ ì›ì¸ íŒŒì•… ì‹œê°„: **ìˆ˜ì¼ â†’ ìˆ˜ë¶„**
- ë°ì´í„° ì†ì‹¤ë¥ : **100% â†’ 0%** (ì¬ì²˜ë¦¬ ê°€ëŠ¥)
- ë””ë²„ê¹… íš¨ìœ¨: **300% í–¥ìƒ**

---

## ğŸ“‹ ëª¨ë‹ˆí„°ë§ ì¿¼ë¦¬

### 1. ì¤‘ë³µ ì—ëŸ¬ ì§‘ê³„

```sql
-- ì˜¤ëŠ˜ ë°œìƒí•œ ì¤‘ë³µ ì—ëŸ¬ í†µê³„
SELECT
    duplicate_type,
    COUNT(*) as count
FROM announcement_duplicate_log
WHERE DATE(created_at) = CURDATE()
    AND duplicate_type = 'integrity_error_duplicate'
GROUP BY duplicate_type;
```

### 2. ì¤‘ë³µ url_key ë¶„ì„

```sql
-- ì–´ë–¤ url_keyê°€ ì¤‘ë³µë˜ëŠ”ì§€ í™•ì¸
SELECT
    url_key_hash,
    JSON_UNQUOTE(JSON_EXTRACT(duplicate_detail, '$.url_key')) as url_key,
    COUNT(*) as duplicate_count
FROM announcement_duplicate_log
WHERE duplicate_type = 'integrity_error_duplicate'
    AND DATE(created_at) = CURDATE()
GROUP BY url_key_hash
ORDER BY duplicate_count DESC
LIMIT 20;
```

### 3. ì—ëŸ¬ ë°œìƒ ì‚¬ì´íŠ¸ ë¶„ì„

```sql
-- ì–´ëŠ ì‚¬ì´íŠ¸ì—ì„œ ì—ëŸ¬ê°€ ë§ì´ ë°œìƒí•˜ëŠ”ì§€
SELECT
    new_site_code,
    duplicate_type,
    COUNT(*) as error_count
FROM announcement_duplicate_log
WHERE DATE(created_at) >= DATE_SUB(CURDATE(), INTERVAL 7 DAY)
GROUP BY new_site_code, duplicate_type
ORDER BY error_count DESC
LIMIT 10;
```

---

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„

### ë‹¨ê¸° (1ì£¼ì¼)
- [ ] ì¬ì‹œë„ í êµ¬í˜„ (Redis ê¸°ë°˜)
- [ ] ì¬ì‹œë„ ì›Œì»¤ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±
- [ ] ì—ëŸ¬ í†µê³„ ëŒ€ì‹œë³´ë“œ ì¶”ê°€

### ì¤‘ê¸° (1ê°œì›”)
- [ ] ìë™ ì•Œë¦¼ ì‹œìŠ¤í…œ (Slack/Email)
- [ ] ì—ëŸ¬ íŒ¨í„´ ìë™ ë¶„ì„
- [ ] ë°ì´í„° ê²€ì¦ ê°•í™”

### ì¥ê¸° (3ê°œì›”)
- [ ] Circuit Breaker íŒ¨í„´ ì ìš©
- [ ] Graceful Degradation êµ¬í˜„
- [ ] ê³ ê°€ìš©ì„± DB êµ¬ì„±

---

**ì‘ì„±ì¼**: 2025-11-18
**íŒŒì¼**: announcement_pre_processor.py
**ìˆ˜ì • ë¼ì¸**: 2658-2755
**ì˜í–¥ ë²”ìœ„**: _save_processing_result() í•¨ìˆ˜
