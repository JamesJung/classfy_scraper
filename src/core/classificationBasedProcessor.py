"""
Î∂ÑÎ•ò Í∏∞Î∞ò ÏÑ†Î≥ÑÏ†Å LLM Ï≤òÎ¶¨ ÏãúÏä§ÌÖú
ANNOUNCEMENT_CLASSIFICATION ÌÖåÏù¥Î∏îÏùò Î∂ÑÎ•ò Í≤∞Í≥ºÎ•º Í∏∞Î∞òÏúºÎ°ú
ÌäπÏ†ï Î∂ÑÎ•ò ÌÉÄÏûÖÎßå LLM Ï≤òÎ¶¨ Î∞è DB Ï†ÄÏû•
"""

import logging
import sys
from datetime import datetime
from pathlib import Path
from typing import Any

# ÌîÑÎ°úÏ†ùÌä∏ Î£®Ìä∏ Ï∂îÍ∞Ä
sys.path.append(str(Path(__file__).parent.parent))

logger = logging.getLogger(__name__)


class ClassificationBasedProcessor:
    """Î∂ÑÎ•ò Í∏∞Î∞ò ÏÑ†Î≥ÑÏ†Å Ï≤òÎ¶¨ ÌÅ¥ÎûòÏä§"""

    # LLM Ï≤òÎ¶¨ ÎåÄÏÉÅ Î∂ÑÎ•ò ÌÉÄÏûÖ
    TARGET_CLASSIFICATION_TYPES = {
        "SMALL_BUSINESS": "ÏÜåÏÉÅÍ≥µÏù∏",
        "SME": "Ï§ëÏÜåÍ∏∞ÏóÖ",
        "STARTUP": "Ï∞ΩÏóÖ",
        "SOCIAL_ENTERPRISE": "ÏÇ¨ÌöåÏ†ÅÍ∏∞ÏóÖ",
    }

    def __init__(self):
        self.processed_count = 0
        self.skipped_count = 0
        self.error_count = 0

        # ÏßÄÏó∞ Î°úÎî©ÏùÑ ÏúÑÌïú Ïª¥Ìè¨ÎÑåÌä∏Îì§
        self._llm_parser = None
        self._db_handler = None

        logger.info("Î∂ÑÎ•ò Í∏∞Î∞ò ÏÑ†Î≥ÑÏ†Å Ï≤òÎ¶¨ ÏãúÏä§ÌÖú Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")

    @property
    def llm_parser(self):
        """LLM ÌååÏÑú ÏßÄÏó∞ Î°úÎî©"""
        if self._llm_parser is None:
            try:
                from src.core.llmParserGpt import LLMParserGpt

                self._llm_parser = LLMParserGpt()
                logger.info("LLMParserGpt ÏßÄÏó∞ Î°úÎî© ÏôÑÎ£å")
            except Exception as e:
                logger.error(f"LLMParserGpt Î°úÎî© Ïã§Ìå®: {e}")
        return self._llm_parser

    @property
    def db_handler(self):
        """DB Ìï∏Îì§Îü¨ ÏßÄÏó∞ Î°úÎî©"""
        if self._db_handler is None:
            try:
                from src.core.dbHandler import DBHandler

                self._db_handler = DBHandler()
                logger.info("DBHandler ÏßÄÏó∞ Î°úÎî© ÏôÑÎ£å")
            except Exception as e:
                logger.error(f"DBHandler Î°úÎî© Ïã§Ìå®: {e}")
        return self._db_handler

    def should_process_with_llm(
        self, folder_path: Path, site_code: str
    ) -> tuple[bool, dict]:
        """
        Ìè¥ÎçîÍ∞Ä LLM Ï≤òÎ¶¨ ÎåÄÏÉÅÏù∏ÏßÄ ÌôïÏù∏

        Args:
            folder_path: Í≥µÍ≥† Ìè¥Îçî Í≤ΩÎ°ú
            site_code: ÏÇ¨Ïù¥Ìä∏ ÏΩîÎìú

        Returns:
            (should_process: bool, classification_info: Dict)
        """

        try:
            from sqlalchemy import text

            from src.models.database import SessionLocal

            db = SessionLocal()

            # ANNOUNCEMENT_CLASSIFICATION ÌÖåÏù¥Î∏îÏóêÏÑú Î∂ÑÎ•ò Ï†ïÎ≥¥ Ï°∞Ìöå
            query = text(
                """
            SELECT CLASSIFICATION_TYPE, CONFIDENCE_SCORE, MATCHED_KEYWORDS,
                   INDUSTRY_KEYWORDS, CLASSIFICATION_ID, IS_PROCESSED
            FROM ANNOUNCEMENT_CLASSIFICATION
            WHERE SITE_CODE = :site_code
            AND FOLDER_NAME = :folder_name
            ORDER BY CLASSIFICATION_DATE DESC
            LIMIT 1
            """
            )

            folder_name = folder_path.name
            result = db.execute(
                query, {"site_code": site_code, "folder_name": folder_name}
            )

            row = result.fetchone()
            db.close()

            if not row:
                logger.warning(f"Î∂ÑÎ•ò Ï†ïÎ≥¥Î•º Ï∞æÏùÑ Ïàò ÏóÜÏùå: {folder_path}")
                return False, {}

            classification_type = row[0]
            confidence_score = row[1] or 0
            matched_keywords = row[2] or "[]"
            industry_keywords = row[3] or "[]"
            classification_id = row[4]
            is_processed = row[5] or False

            classification_info = {
                "classification_id": classification_id,
                "classification_type": classification_type,
                "confidence_score": confidence_score,
                "matched_keywords": matched_keywords,
                "industry_keywords": industry_keywords,
                "is_processed": is_processed,
                "classification_name": self.TARGET_CLASSIFICATION_TYPES.get(
                    classification_type, "Í∏∞ÌÉÄ"
                ),
            }

            # ÎåÄÏÉÅ Î∂ÑÎ•ò ÌÉÄÏûÖÏù∏ÏßÄ ÌôïÏù∏
            should_process = classification_type in self.TARGET_CLASSIFICATION_TYPES

            if should_process:
                logger.info(
                    f"‚úÖ LLM Ï≤òÎ¶¨ ÎåÄÏÉÅ: {folder_name} ({self.TARGET_CLASSIFICATION_TYPES[classification_type]})"
                )
            else:
                logger.info(f"‚è≠Ô∏è  LLM Ï≤òÎ¶¨ Ï†úÏô∏: {folder_name} ({classification_type})")

            return should_process, classification_info

        except Exception as e:
            logger.error(f"Î∂ÑÎ•ò Ï†ïÎ≥¥ Ï°∞Ìöå Ïã§Ìå® ({folder_path}): {e}")
            return False, {}

    def process_classified_folder(
        self, folder_path: Path, site_code: str
    ) -> dict[str, Any]:
        """
        Î∂ÑÎ•òÎêú Ìè¥ÎçîÏóê ÎåÄÌïú ÏÑ†Î≥ÑÏ†Å LLM Ï≤òÎ¶¨

        Args:
            folder_path: Í≥µÍ≥† Ìè¥Îçî Í≤ΩÎ°ú
            site_code: ÏÇ¨Ïù¥Ìä∏ ÏΩîÎìú

        Returns:
            Ï≤òÎ¶¨ Í≤∞Í≥º ÎîïÏÖîÎÑàÎ¶¨
        """

        result = {
            "folder_path": str(folder_path),
            "site_code": site_code,
            "processed": False,
            "skipped": False,
            "error": None,
            "classification_info": {},
            "llm_result": None,
            "db_saved": False,
            "sbvt_id": None,
        }

        try:
            # 1. Î∂ÑÎ•ò Ï†ïÎ≥¥ ÌôïÏù∏
            should_process, classification_info = self.should_process_with_llm(
                folder_path, site_code
            )
            result["classification_info"] = classification_info

            if not should_process:
                result["skipped"] = True
                self.skipped_count += 1
                logger.info(f"‚è≠Ô∏è  LLM Ï≤òÎ¶¨ Í±¥ÎÑàÎõ∞Í∏∞: {folder_path.name}")
                return result

            # 2. Ïù¥ÎØ∏ Ï≤òÎ¶¨Îêú Í≤ΩÏö∞ ÌôïÏù∏
            if classification_info.get("is_processed", False):
                result["skipped"] = True
                self.skipped_count += 1
                logger.info(f"‚úÖ Ïù¥ÎØ∏ Ï≤òÎ¶¨Îê®: {folder_path.name}")
                return result

            # 3. LLM Ï≤òÎ¶¨ ÏàòÌñâ
            logger.info(
                f"ü§ñ LLM Ï≤òÎ¶¨ ÏãúÏûë: {folder_path.name} ({classification_info.get('classification_name', 'Ïïå Ïàò ÏóÜÏùå')})"
            )

            llm_result = self._perform_llm_processing(
                folder_path, site_code, classification_info
            )

            if not llm_result:
                raise Exception("LLM Ï≤òÎ¶¨ Ïã§Ìå®")

            result["llm_result"] = llm_result

            # 4. DB Ï†ÄÏû•
            sbvt_id = self._save_to_master_table(
                llm_result, folder_path, site_code, classification_info
            )

            if sbvt_id:
                result["db_saved"] = True
                result["sbvt_id"] = sbvt_id
                result["processed"] = True
                self.processed_count += 1

                # 5. Î∂ÑÎ•ò ÌÖåÏù¥Î∏î Ï≤òÎ¶¨ ÏÉÅÌÉú ÏóÖÎç∞Ïù¥Ìä∏ (Í≤ÄÏ¶ù Í≤∞Í≥º Ìè¨Ìï®)
                validation_result = llm_result.get("validation_result")
                self._update_classification_processed_status(
                    classification_info["classification_id"], sbvt_id, validation_result
                )

                logger.info(
                    f"‚úÖ LLM Ï≤òÎ¶¨ Î∞è DB Ï†ÄÏû• ÏôÑÎ£å: {folder_path.name}, SBVT_ID: {sbvt_id}"
                )
            else:
                raise Exception("DB Ï†ÄÏû• Ïã§Ìå®")

        except Exception as e:
            result["error"] = str(e)
            self.error_count += 1
            logger.error(f"‚ùå Ï≤òÎ¶¨ Ïã§Ìå® ({folder_path.name}): {e}")

        return result

    def _perform_llm_processing(
        self, folder_path: Path, site_code: str, classification_info: dict
    ) -> dict | None:
        """LLM Ï≤òÎ¶¨ ÏàòÌñâ"""

        if not self.llm_parser:
            logger.error("LLM ÌååÏÑúÍ∞Ä Ï¥àÍ∏∞ÌôîÎêòÏßÄ ÏïäÏùå")
            return None

        try:
            # Ìè¥Îçî ÎÇ¥ ÌååÏùºÎì§ÏùÑ Ï≤òÎ¶¨ÌïòÏó¨ ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú
            extracted_text = self._extract_text_from_folder(folder_path)

            if not extracted_text:
                logger.warning(f"Ï∂îÏ∂úÌï† ÌÖçÏä§Ìä∏Í∞Ä ÏóÜÏùå: {folder_path}")
                return None

            # Î∂ÑÎ•ò Ï†ïÎ≥¥Î•º Ïª®ÌÖçÏä§Ìä∏Î°ú Ï†úÍ≥µÌïòÏó¨ LLM Ï≤òÎ¶¨ ÌíàÏßà Ìñ•ÏÉÅ
            enhanced_prompt = self._create_enhanced_prompt(
                extracted_text, classification_info
            )

            # LLM ÌååÏã± ÏàòÌñâ
            llm_result = self.llm_parser.parse_with_llm(
                enhanced_prompt,
                ".md",  # file_type - dot prefix required
                str(folder_path),  # file_path
            )

            # LLM Í≤∞Í≥ºÍ∞Ä ÎîïÏÖîÎÑàÎ¶¨Ïù¥Í≥† ÎπÑÏñ¥ÏûàÏßÄ ÏïäÏúºÎ©¥ ÏÑ±Í≥µÏúºÎ°ú Í∞ÑÏ£º
            # (LLM ÌååÏÑúÎäî parsed_data ÌÇ§Í∞Ä ÏïÑÎãå ÎîïÏÖîÎÑàÎ¶¨ ÏûêÏ≤¥Î•º Î∞òÌôò)
            if llm_result and isinstance(llm_result, dict) and len(llm_result) > 0:
                # Î∂ÑÎ•ò Í≤ÄÏ¶ù Î∞è Î∂àÏùºÏπò Í∞êÏßÄ ÏàòÌñâ
                validation_result = self._validate_classification_consistency(
                    classification_info, llm_result
                )
                
                # Î∂ÑÎ•ò Ï†ïÎ≥¥Î•º LLM Í≤∞Í≥ºÏóê Ï∂îÍ∞Ä
                llm_result["classification_info"] = classification_info
                llm_result["processed_timestamp"] = datetime.now().isoformat()

                # DB Ï†ÄÏû•ÏùÑ ÏúÑÌï¥ parsed_data ÎûòÌïë
                final_result = {
                    "parsed_data": llm_result.copy(),
                    "quality_score": llm_result.get("quality_score", 0),
                    "field_completion_rate": llm_result.get("field_completion_rate", 0),
                    "classification_info": classification_info,
                    "validation_result": validation_result,
                    "processed_timestamp": datetime.now().isoformat(),
                }

                # Î∂ÑÎ•ò Î∂àÏùºÏπò Î°úÍπÖ
                if validation_result.get("is_mismatch", False):
                    logger.warning(
                        f"‚ö†Ô∏è Î∂ÑÎ•ò Î∂àÏùºÏπò Í∞êÏßÄ: {folder_path.name} - "
                        f"ÌÇ§ÏõåÎìú: {validation_result.get('keyword_primary')}, "
                        f"LLM: {validation_result.get('llm_primary')}"
                    )
                else:
                    logger.info(
                        f"‚úÖ Î∂ÑÎ•ò ÏùºÏπò ÌôïÏù∏: {folder_path.name} - "
                        f"Î∂ÑÎ•ò: {validation_result.get('keyword_primary')}"
                    )

                logger.info(
                    f"‚úÖ LLM Ï≤òÎ¶¨ ÏÑ±Í≥µ: {folder_path.name} - ÌíàÏßà: {llm_result.get('quality_score', 0):.1f}%"
                )
                return final_result
            else:
                logger.error(
                    f"‚ùå LLM ÌååÏã± Ïã§Ìå®: {folder_path.name} - Í≤∞Í≥º: {type(llm_result)}, ÌÇ§ Í∞úÏàò: {len(llm_result) if llm_result and isinstance(llm_result, dict) else 0}"
                )
                return None

        except Exception as e:
            logger.error(f"LLM Ï≤òÎ¶¨ Ï§ë Ïò§Î•ò ({folder_path}): {e}")
            return None

    def _extract_text_from_folder(self, folder_path: Path) -> str:
        """Ìè¥ÎçîÏóêÏÑú ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú"""

        try:
            from src.utils.announcementClassifier import AnnouncementClassifier

            classifier = AnnouncementClassifier()
            extracted_texts = classifier.extract_text_from_files(folder_path)

            if not extracted_texts:
                return ""

            # ÌíàÏßà Ïö∞ÏÑ†ÏàúÏúÑÎ°ú ÌÖçÏä§Ìä∏ Í≤∞Ìï©
            combined_text = ""
            for filename, text in extracted_texts.items():
                if filename.endswith((".pdf", ".hwp", ".hwpx")):
                    combined_text = text + "\n" + combined_text  # ÎÜíÏùÄ ÌíàÏßà ÌååÏùºÏùÑ ÏïûÏóê
                else:
                    combined_text += "\n" + text

            return combined_text.strip()

        except Exception as e:
            logger.error(f"ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú Ïã§Ìå® ({folder_path}): {e}")
            return ""

    def _create_enhanced_prompt(self, text: str, classification_info: dict) -> str:
        """Î∂ÑÎ•ò Ï†ïÎ≥¥Î•º ÌôúÏö©Ìïú Ìñ•ÏÉÅÎêú ÌîÑÎ°¨ÌîÑÌä∏ ÏÉùÏÑ±"""

        classification_type = classification_info.get("classification_type", "UNKNOWN")
        classification_name = classification_info.get(
            "classification_name", "Ïïå Ïàò ÏóÜÏùå"
        )
        confidence_score = classification_info.get("confidence_score", 0)

        enhanced_prompt = f"""
[Î∂ÑÎ•ò Ï†ïÎ≥¥]
- ÎåÄÏÉÅ Î∂ÑÎ•ò: {classification_name} ({classification_type})
- Ïã†Î¢∞ÎèÑ: {confidence_score}Ï†ê
- Î∂ÑÎ•ò Í∑ºÍ±∞: Ïù¥ Í≥µÍ≥†Îäî {classification_name} ÎåÄÏÉÅÏúºÎ°ú Î∂ÑÎ•òÎêòÏóàÏäµÎãàÎã§.

[ÏõêÎ≥∏ ÌÖçÏä§Ìä∏]
{text}

[Ï≤òÎ¶¨ ÏßÄÏπ®]
ÏúÑ Î∂ÑÎ•ò Ï†ïÎ≥¥Î•º Ï∞∏Í≥†ÌïòÏó¨ {classification_name} Í¥ÄÎ†® ÌïÑÎìúÎì§ÏùÑ Ï§ëÏ†êÏ†ÅÏúºÎ°ú Ï∂îÏ∂úÌïòÍ≥† Î∂ÑÏÑùÌï¥Ï£ºÏÑ∏Ïöî.
ÌäπÌûà ÏßÄÏõê ÎåÄÏÉÅ, ÏßÄÏõê ÏûêÍ≤©, ÏßÄÏõê ÎÇ¥Ïö© Îì±ÏùÑ ÏÉÅÏÑ∏Ìûà ÌååÏïÖÌï¥Ï£ºÏÑ∏Ïöî.
"""

        return enhanced_prompt

    def _save_to_master_table(
        self,
        llm_result: dict,
        folder_path: Path,
        site_code: str,
        classification_info: dict,
    ) -> int | None:
        """SubventionMasterTableÏóê Ï†ÄÏû•"""

        if not self.db_handler:
            logger.error("DB Ìï∏Îì§Îü¨Í∞Ä Ï¥àÍ∏∞ÌôîÎêòÏßÄ ÏïäÏùå")
            return None

        try:
            # PRV ÏÇ¨Ïù¥Ìä∏ Ï†ÑÏö© Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨ (ÏÑ†ÌÉùÏÇ¨Ìï≠ - Î™®ÎìàÏù¥ ÏûàÏúºÎ©¥ ÏÇ¨Ïö©)
            if site_code.lower() == "prv":
                try:
                    from src.utils.prvDataExtractor import process_prv_data

                    llm_result = process_prv_data(folder_path, llm_result)
                    logger.info(f"PRV ÏÇ¨Ïù¥Ìä∏ Ï†ÑÏö© Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨ ÏôÑÎ£å: {folder_path.name}")
                except ImportError:
                    logger.debug(
                        f"PRV Ï†ÑÏö© Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨ Î™®ÎìàÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏùå - Í∏∞Î≥∏ Ï≤òÎ¶¨Î°ú ÏßÑÌñâ: {folder_path.name}"
                    )
                except Exception as e:
                    logger.warning(f"PRV Ï†ÑÏö© Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨ Ïã§Ìå®, Í∏∞Î≥∏ Ï≤òÎ¶¨Î°ú ÏßÑÌñâ: {e}")

            # LLM Í≤∞Í≥º Îç∞Ïù¥ÌÑ∞Ïóê Î∂ÑÎ•ò Ï†ïÎ≥¥ Ï∂îÍ∞Ä
            final_data = llm_result.get("parsed_data", {}).copy()

            # Î∂ÑÎ•ò Í¥ÄÎ†® ÌïÑÎìú Ï∂îÍ∞Ä
            final_data.update(
                {
                    "CLASSIFICATION_TYPE": classification_info.get(
                        "classification_type"
                    ),
                    "CLASSIFICATION_CONFIDENCE": classification_info.get(
                        "confidence_score", 0
                    ),
                    "CLASSIFICATION_PROCESSED": True,
                    "CLASSIFICATION_TIMESTAMP": datetime.now(),
                    "MATCHED_KEYWORDS": classification_info.get(
                        "matched_keywords", "[]"
                    ),
                    "INDUSTRY_KEYWORDS": classification_info.get(
                        "industry_keywords", "[]"
                    ),
                    "FOLDER_NAME": folder_path.name,
                    "SITE_CODE": site_code,
                }
            )

            # ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú
            extracted_text = self._extract_text_from_folder(folder_path)

            # Ï≤òÎ¶¨Îêú ÌååÏùº Ï†ïÎ≥¥ Íµ¨ÏÑ± (UNIQUE Ï†úÏïΩÏ°∞Í±¥ Ïò§Î•ò Î∞©ÏßÄÎ•º ÏúÑÌïú folder_name Ï∂îÍ∞Ä)
            processed_files = {
                "best_file": {"exists": True, "path": str(folder_path)},
                "announcement_folder": str(folder_path),
                "classification_based_processing": True,
                "output_json": str(
                    folder_path / "processed_data.json"
                ),  # DB Ìï∏Îì§Îü¨ÏóêÏÑú ÌïÑÏöîÌïú ÌÇ§
                "pdf": [],
                "hwp": [],
                "folder_name": folder_path.name  # Ïã§Ï†ú Í≥µÍ≥† Ìè¥ÎçîÎ™Ö Î™ÖÏãúÏ†Å Ï∂îÍ∞Ä
            }

            # DB Ï†ÄÏû•
            sbvt_id = self.db_handler.save_to_db(
                final_data, folder_path, processed_files, site_code, extracted_text
            )

            logger.debug(f"DB Ï†ÄÏû• Í≤∞Í≥º: SBVT_ID={sbvt_id} (ÌÉÄÏûÖ: {type(sbvt_id)})")

            return sbvt_id

        except Exception as e:
            logger.error(f"ÎßàÏä§ÌÑ∞ ÌÖåÏù¥Î∏î Ï†ÄÏû• Ïã§Ìå® ({folder_path}): {e}")
            import traceback

            traceback.print_exc()
            return None

    def _update_classification_processed_status(
        self, classification_id: int, sbvt_id: int, validation_result: dict = None
    ):
        """Î∂ÑÎ•ò ÌÖåÏù¥Î∏îÏùò Ï≤òÎ¶¨ ÏÉÅÌÉú Î∞è LLM Î∂ÑÎ•ò Ï†ïÎ≥¥ ÏóÖÎç∞Ïù¥Ìä∏"""

        try:
            from sqlalchemy import text
            import json

            from src.models.database import SessionLocal

            db = SessionLocal()
            
            # LLM Î∂ÑÎ•ò Ï†ïÎ≥¥ Ï∂îÏ∂ú
            llm_classification = None
            llm_classification_types = None
            llm_classification_scores = None
            llm_detected_keywords = None
            classification_validation_status = "LLM_COMPLETED"
            is_classification_mismatch = False
            mismatch_details = None
            
            if validation_result:
                llm_classification = validation_result.get("llm_primary")
                llm_types = validation_result.get("llm_types", [])
                llm_keywords = validation_result.get("llm_detected_keywords", [])
                llm_confidence = validation_result.get("llm_confidence", 0)
                
                if llm_types:
                    llm_classification_types = json.dumps(llm_types, ensure_ascii=False)
                if llm_keywords:
                    llm_detected_keywords = json.dumps(llm_keywords, ensure_ascii=False)
                if llm_confidence > 0:
                    llm_classification_scores = json.dumps({llm_classification: llm_confidence}, ensure_ascii=False)
                
                is_classification_mismatch = validation_result.get("is_mismatch", False)
                if is_classification_mismatch:
                    classification_validation_status = "MISMATCH"
                    mismatch_details = json.dumps(validation_result.get("mismatch_details", {}), ensure_ascii=False)
                else:
                    classification_validation_status = "VALIDATED"

            update_query = text(
                """
            UPDATE ANNOUNCEMENT_CLASSIFICATION
            SET IS_PROCESSED = TRUE,
                SBVT_ID = :sbvt_id,
                LLM_CLASSIFICATION_TYPE = :llm_classification_type,
                LLM_CLASSIFICATION_TYPES = :llm_classification_types,
                LLM_CLASSIFICATION_SCORES = :llm_classification_scores,
                LLM_DETECTED_KEYWORDS = :llm_detected_keywords,
                CLASSIFICATION_VALIDATION_STATUS = :classification_validation_status,
                IS_CLASSIFICATION_MISMATCH = :is_classification_mismatch,
                MISMATCH_DETAILS = :mismatch_details,
                LAST_UPDATED = NOW()
            WHERE CLASSIFICATION_ID = :classification_id
            """
            )

            db.execute(
                update_query,
                {
                    "classification_id": classification_id,
                    "sbvt_id": sbvt_id,
                    "llm_classification_type": llm_classification,
                    "llm_classification_types": llm_classification_types,
                    "llm_classification_scores": llm_classification_scores,
                    "llm_detected_keywords": llm_detected_keywords,
                    "classification_validation_status": classification_validation_status,
                    "is_classification_mismatch": is_classification_mismatch,
                    "mismatch_details": mismatch_details,
                },
            )

            db.commit()
            db.close()

            status_msg = "ÏùºÏπò" if not is_classification_mismatch else "Î∂àÏùºÏπò"
            logger.info(
                f"Î∂ÑÎ•ò ÌÖåÏù¥Î∏î ÏóÖÎç∞Ïù¥Ìä∏ ÏôÑÎ£å: {classification_id} -> SBVT_ID: {sbvt_id}, "
                f"LLM Î∂ÑÎ•ò: {llm_classification}, Í≤ÄÏ¶ù: {status_msg}"
            )

        except Exception as e:
            logger.error(f"Î∂ÑÎ•ò ÌÖåÏù¥Î∏î ÏÉÅÌÉú ÏóÖÎç∞Ïù¥Ìä∏ Ïã§Ìå®: {e}")

    def process_site_folders(
        self, data_origin_path: Path, site_codes: list[str] | None = None
    ) -> dict[str, Any]:
        """ÏÇ¨Ïù¥Ìä∏Î≥Ñ Ìè¥Îçî ÏùºÍ¥Ñ Ï≤òÎ¶¨"""

        results = {
            "total_folders": 0,
            "processed_folders": 0,
            "skipped_folders": 0,
            "error_folders": 0,
            "by_site": {},
            "by_classification_type": {},
            "processing_time": 0,
        }

        start_time = datetime.now()

        try:
            logger.info(f"üöÄ Î∂ÑÎ•ò Í∏∞Î∞ò ÏÑ†Î≥ÑÏ†Å Ï≤òÎ¶¨ ÏãúÏûë: {data_origin_path}")

            # ÏÇ¨Ïù¥Ìä∏Î≥Ñ Ìè¥Îçî ÏàúÌöå
            for site_dir in data_origin_path.iterdir():
                if not site_dir.is_dir():
                    continue

                site_code = site_dir.name

                # ÌäπÏ†ï ÏÇ¨Ïù¥Ìä∏Îßå Ï≤òÎ¶¨ÌïòÎäî Í≤ΩÏö∞
                if site_codes and site_code not in site_codes:
                    continue

                logger.info(f"üìÅ ÏÇ¨Ïù¥Ìä∏ Ï≤òÎ¶¨ Ï§ë: {site_code}")

                site_results = {
                    "total": 0,
                    "processed": 0,
                    "skipped": 0,
                    "errors": 0,
                    "by_type": {},
                }

                # prv ÏÇ¨Ïù¥Ìä∏ ÌäπÏàò Ï≤òÎ¶¨ ([ÏãúÎèÑ]/[ÏãúÍµ∞Íµ¨]/Í≥µÍ≥† Íµ¨Ï°∞)
                if site_code.lower() == "prv":
                    from src.utils.folderUtil import get_prv_announcement_folders

                    prv_folders = get_prv_announcement_folders(site_dir)

                    for (
                        announcement_dir,
                        sido_name,
                        sigungu_name,
                        announcement_name,
                    ) in prv_folders:
                        results["total_folders"] += 1
                        site_results["total"] += 1

                        # prv ÏÇ¨Ïù¥Ìä∏ Ï†ÑÏö© Ï≤òÎ¶¨ ÏàòÌñâ
                        folder_result = self.process_classified_folder(
                            announcement_dir, site_code
                        )

                        # Í≤∞Í≥º ÏßëÍ≥Ñ
                        self._aggregate_classification_results(
                            folder_result, results, site_results
                        )

                        logger.debug(
                            f"prv Í≥µÍ≥† Ï≤òÎ¶¨ ÏôÑÎ£å: {sido_name}/{sigungu_name}/{announcement_name}"
                        )
                else:
                    # ÏùºÎ∞ò ÏÇ¨Ïù¥Ìä∏ Ï≤òÎ¶¨ (Í∏∞Ï°¥ Î°úÏßÅ)
                    for announcement_dir in site_dir.iterdir():
                        if not announcement_dir.is_dir():
                            continue

                        # Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ìè¥Îçî Ï†úÏô∏
                        if (
                            announcement_dir.name.startswith("processed_")
                            or announcement_dir.name.startswith("backup_")
                            or announcement_dir.name.startswith("temp_")
                        ):
                            continue

                        results["total_folders"] += 1
                        site_results["total"] += 1

                        # Í∞úÎ≥Ñ Ìè¥Îçî Ï≤òÎ¶¨
                        folder_result = self.process_classified_folder(
                            announcement_dir, site_code
                        )

                        # Í≤∞Í≥º ÏßëÍ≥Ñ
                        self._aggregate_classification_results(
                            folder_result, results, site_results
                        )

                results["by_site"][site_code] = site_results
                logger.info(
                    f"‚úÖ ÏÇ¨Ïù¥Ìä∏ {site_code} ÏôÑÎ£å: Ï≤òÎ¶¨ {site_results['processed']}, Í±¥ÎÑàÎúÄ {site_results['skipped']}, Ïò§Î•ò {site_results['errors']}"
                )

            end_time = datetime.now()
            results["processing_time"] = (end_time - start_time).total_seconds()

            logger.info("üéâ Î∂ÑÎ•ò Í∏∞Î∞ò ÏÑ†Î≥ÑÏ†Å Ï≤òÎ¶¨ ÏôÑÎ£å:")
            logger.info(f"  - Ï¥ù Ìè¥Îçî: {results['total_folders']}Í∞ú")
            logger.info(f"  - Ï≤òÎ¶¨Îê®: {results['processed_folders']}Í∞ú")
            logger.info(f"  - Í±¥ÎÑàÎúÄ: {results['skipped_folders']}Í∞ú")
            logger.info(f"  - Ïò§Î•ò: {results['error_folders']}Í∞ú")
            logger.info(f"  - Ï≤òÎ¶¨ ÏãúÍ∞Ñ: {results['processing_time']:.1f}Ï¥à")

        except Exception as e:
            logger.error(f"ÏùºÍ¥Ñ Ï≤òÎ¶¨ Ï§ë Ïò§Î•ò Î∞úÏÉù: {e}")
            import traceback

            traceback.print_exc()

        return results

    def _aggregate_classification_results(
        self, folder_result: dict, results: dict, site_results: dict
    ):
        """Î∂ÑÎ•ò Ï≤òÎ¶¨ Í≤∞Í≥ºÎ•º ÏßëÍ≥ÑÌï©ÎãàÎã§."""

        if folder_result["processed"]:
            results["processed_folders"] += 1
            site_results["processed"] += 1

            # Î∂ÑÎ•ò ÌÉÄÏûÖÎ≥Ñ ÏßëÍ≥Ñ
            classification_type = folder_result["classification_info"].get(
                "classification_type", "UNKNOWN"
            )
            results["by_classification_type"][classification_type] = (
                results["by_classification_type"].get(classification_type, 0) + 1
            )
            site_results["by_type"][classification_type] = (
                site_results["by_type"].get(classification_type, 0) + 1
            )

        elif folder_result["skipped"]:
            results["skipped_folders"] += 1
            site_results["skipped"] += 1
        else:
            results["error_folders"] += 1
            site_results["errors"] += 1

    def get_processing_statistics(self) -> dict[str, Any]:
        """Ï≤òÎ¶¨ ÌÜµÍ≥Ñ Î∞òÌôò"""

        return {
            "target_classification_types": self.TARGET_CLASSIFICATION_TYPES,
            "processed_count": self.processed_count,
            "skipped_count": self.skipped_count,
            "error_count": self.error_count,
            "total_count": self.processed_count + self.skipped_count + self.error_count,
            "success_rate": (
                self.processed_count / max(1, self.processed_count + self.error_count)
            )
            * 100,
        }

    def _validate_classification_consistency(
        self, classification_info: dict, llm_result: dict
    ) -> dict:
        """
        ÌÇ§ÏõåÎìú Í∏∞Î∞ò Î∂ÑÎ•òÏôÄ LLM Í∏∞Î∞ò Î∂ÑÎ•òÏùò ÏùºÏπòÏÑ± Í≤ÄÏ¶ù
        
        Args:
            classification_info: ÌÇ§ÏõåÎìú Í∏∞Î∞ò Î∂ÑÎ•ò Ï†ïÎ≥¥
            llm_result: LLM Ï≤òÎ¶¨ Í≤∞Í≥º (CLASSIFICATION_INFO Ìè¨Ìï®)
            
        Returns:
            dict: Í≤ÄÏ¶ù Í≤∞Í≥º Ï†ïÎ≥¥
        """
        try:
            # ÌÇ§ÏõåÎìú Í∏∞Î∞ò Î∂ÑÎ•ò Ï†ïÎ≥¥ Ï∂îÏ∂ú
            keyword_primary = classification_info.get("classification_type", "")
            keyword_name = self.TARGET_CLASSIFICATION_TYPES.get(keyword_primary, "")
            keyword_confidence = classification_info.get("confidence_score", 0)
            
            # LLM Í∏∞Î∞ò Î∂ÑÎ•ò Ï†ïÎ≥¥ Ï∂îÏ∂ú
            llm_classification = llm_result.get("CLASSIFICATION_INFO", {})
            llm_primary = llm_classification.get("primary_classification", "")
            llm_types = llm_classification.get("classification_types", [])
            llm_confidence = llm_classification.get("confidence_score", 0)
            llm_keywords = llm_classification.get("detected_keywords", [])
            llm_reasoning = llm_classification.get("reasoning", "")
            
            # Î∂ÑÎ•ò Îß§Ìïë (ÌïúÍ∏Ä -> ÏòÅÏñ¥ ÏΩîÎìú)
            classification_mapping = {v: k for k, v in self.TARGET_CLASSIFICATION_TYPES.items()}
            llm_primary_code = classification_mapping.get(llm_primary, "")
            
            # Î∂àÏùºÏπò Ïó¨Î∂Ä ÌåêÎã®
            is_mismatch = False
            mismatch_details = {}
            
            if keyword_primary != llm_primary_code and llm_primary_code:
                is_mismatch = True
                confidence_diff = abs(keyword_confidence - llm_confidence)
                
                mismatch_details = {
                    "keyword_primary": keyword_name,
                    "llm_primary": llm_primary,
                    "keyword_code": keyword_primary,
                    "llm_primary_code": llm_primary_code,
                    "confidence_diff": confidence_diff,
                    "keyword_confidence": keyword_confidence,
                    "llm_confidence": llm_confidence,
                    "review_needed": confidence_diff > 20,  # 20Ï†ê Ïù¥ÏÉÅ Ï∞®Ïù¥Î©¥ Î¶¨Î∑∞ ÌïÑÏöî
                    "llm_reasoning": llm_reasoning
                }
                
                logger.info(f"Î∂ÑÎ•ò Î∂àÏùºÏπò Í∞êÏßÄ: {keyword_name} -> {llm_primary} (Ïã†Î¢∞ÎèÑ Ï∞®Ïù¥: {confidence_diff}Ï†ê)")
            else:
                logger.debug(f"Î∂ÑÎ•ò ÏùºÏπò ÌôïÏù∏: {keyword_name}")
            
            # Í≤ÄÏ¶ù Í≤∞Í≥º Íµ¨ÏÑ±
            validation_result = {
                "is_mismatch": is_mismatch,
                "keyword_primary": keyword_name,
                "keyword_code": keyword_primary,
                "llm_primary": llm_primary,
                "llm_primary_code": llm_primary_code,
                "llm_types": llm_types,
                "llm_detected_keywords": llm_keywords,
                "llm_confidence": llm_confidence,
                "keyword_confidence": keyword_confidence,
                "mismatch_details": mismatch_details,
                "validation_status": "MISMATCH" if is_mismatch else "VALIDATED",
                "validation_timestamp": datetime.now().isoformat()
            }
            
            return validation_result
            
        except Exception as e:
            logger.error(f"Î∂ÑÎ•ò Í≤ÄÏ¶ù Ï§ë Ïò§Î•ò Î∞úÏÉù: {e}")
            return {
                "is_mismatch": False,
                "validation_status": "ERROR",
                "error_message": str(e),
                "validation_timestamp": datetime.now().isoformat()
            }
