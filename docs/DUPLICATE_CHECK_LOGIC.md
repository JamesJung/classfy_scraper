# ì¤‘ë³µ ê³µê³  ì²´í¬ ë¡œì§ ë¶„ì„ ë³´ê³ ì„œ

## ğŸ“‹ ì‹¤í–‰ ìš”ì•½

**ëª©ì **: `announcement_pre_processor.py` ì‹¤í–‰ ì‹œ url_key/url_key_hash ê¸°ë°˜ ì¤‘ë³µ ì²´í¬ ë¡œì§ì´ ì˜ë„ëŒ€ë¡œ ë™ì‘í•˜ëŠ”ì§€ ê²€ì¦

**ë¶„ì„ ì¼ì‹œ**: 2025-10-30

**ë¶„ì„ ëŒ€ìƒ**:
- `announcement_pre_processor.py` (line 1622-1994)
- ì¤‘ë³µ ì²´í¬ í•µì‹¬ ë¡œì§ (`_save_to_database_simple` ë©”ì„œë“œ)

---

## ğŸ” ì¤‘ë³µ ì²´í¬ ë¡œì§ ë¶„ì„

### 1. í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜

#### 1.1 UNIQUE ì œì•½ ì¡°ê±´
```sql
UNIQUE KEY uk_url_key_hash (url_key_hash)
```

**announcement_pre_processing í…Œì´ë¸”**ì— `url_key_hash` ì»¬ëŸ¼ì— UNIQUE ì œì•½ ì¡°ê±´ì´ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

- **url_key**: ì •ê·œí™”ëœ URL (VARCHAR 500)
- **url_key_hash**: url_keyì˜ MD5 í•´ì‹œ (CHAR 32)

#### 1.2 UPSERT ì¿¼ë¦¬
```python
INSERT INTO announcement_pre_processing (...)
VALUES (...)
ON DUPLICATE KEY UPDATE
    site_type = IF(ì¡°ê±´, VALUES(site_type), site_type),
    ...
```

**ë™ì‘ ë°©ì‹**:
1. **INSERT ì‹œë„**: url_key_hashê°€ ì¤‘ë³µë˜ì§€ ì•Šìœ¼ë©´ ìƒˆ ë ˆì½”ë“œ ì‚½ì…
   - `affected_rows = 1` ë°˜í™˜

2. **DUPLICATE KEY ê°ì§€**: url_key_hashê°€ ì´ë¯¸ ì¡´ì¬í•˜ë©´
   - `affected_rows = 2` ë°˜í™˜
   - ON DUPLICATE KEY UPDATE ì ˆ ì‹¤í–‰
   - ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ì—…ë°ì´íŠ¸ ë˜ëŠ” ê¸°ì¡´ ë°ì´í„° ìœ ì§€

---

### 2. ì¤‘ë³µ ì²´í¬ ì„¸ë¶€ ë¡œì§

#### 2.1 url_key ìƒì„± (announcement_pre_processor.py:565-591)

```python
# 3.5. origin_urlì—ì„œ url_key ì¶”ì¶œ (URL ì •ê·œí™”)
url_key = None
if origin_url:
    try:
        # 1ìˆœìœ„: domain_key_configì—ì„œ ë„ë©”ì¸ ì„¤ì • ì¡°íšŒ
        url_key = self.url_key_extractor.extract_url_key(origin_url, site_code)
        if url_key:
            logger.debug(f"âœ“ URL ì •ê·œí™” ì™„ë£Œ (domain_key_config ì‚¬ìš©)")
        else:
            # 2ìˆœìœ„: í´ë°± ì •ê·œí™” (ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì •ë ¬)
            logger.warning(f"âš ï¸  ë„ë©”ì¸ ì„¤ì • ì—†ìŒ, í´ë°± ì •ê·œí™” ìˆ˜í–‰")
            url_key = self._fallback_normalize_url(origin_url)
    except Exception as e:
        # ì˜ˆì™¸ ë°œìƒ ì‹œì—ë„ í´ë°± ì‹œë„
        url_key = self._fallback_normalize_url(origin_url)
```

**url_key ìƒì„± ìš°ì„ ìˆœìœ„**:
1. **domain_key_config ì‚¬ìš©** (DomainKeyExtractor)
   - domain_key_config í…Œì´ë¸”ì— ë„ë©”ì¸ë³„ ì„¤ì • ì¡°íšŒ
   - key_paramsì— ì •ì˜ëœ íŒŒë¼ë¯¸í„°ë§Œ ì¶”ì¶œ
   - ì•ŒíŒŒë²³ ìˆœ ì •ë ¬í•˜ì—¬ ì¼ê´€ëœ í‚¤ ìƒì„±

2. **í´ë°± ì •ê·œí™”** (_fallback_normalize_url)
   - domain_key_config ì—†ëŠ” ê²½ìš°
   - ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°ë¥¼ ì•ŒíŒŒë²³ ìˆœ ì •ë ¬
   - page, pageIndex ë“± í˜ì´ì§€ë„¤ì´ì…˜ íŒŒë¼ë¯¸í„° ì œì™¸

#### 2.2 ì¤‘ë³µ ì²´í¬ ë¶„ê¸° (announcement_pre_processor.py:1853-1970)

**ì¼€ì´ìŠ¤ 1: domain_key_config ì—†ëŠ” ê²½ìš°** (line 1854-1879)
```python
if not domain_has_config:
    logger.info(f"domain_key_config ì—†ìŒ, ì¤‘ë³µ ì²´í¬ ì œì™¸")

    if affected_rows == 1:
        processing_status = 'new_inserted'
    elif affected_rows == 2:
        processing_status = 'duplicate_skipped'
        duplicate_reason = {
            "reason": "domain_key_config ì—†ìŒ, ì¤‘ë³µ ê°ì§€í–ˆìœ¼ë‚˜ ì œì™¸ë¨",
            "domain": domain,
            "fallback_used": True
        }
```

**íŠ¹ì§•**:
- ì¤‘ë³µ ê°ì§€ëŠ” í•˜ì§€ë§Œ **ìš°ì„ ìˆœìœ„ ë¹„êµ ì•ˆ í•¨**
- `duplicate_skipped` ìƒíƒœë¡œ ë¡œê·¸ë§Œ ê¸°ë¡
- ë°ì´í„°ëŠ” ì—…ë°ì´íŠ¸ë˜ì§€ ì•ŠìŒ (ê¸°ì¡´ ë°ì´í„° ìœ ì§€)

**ì¼€ì´ìŠ¤ 2: domain_key_config ìˆëŠ” ê²½ìš°** (line 1881-1946)
```python
elif affected_rows == 1:
    # ìƒˆë¡œ INSERTë¨
    processing_status = 'new_inserted'

elif affected_rows == 2:
    # UPDATEë¨ (ì¤‘ë³µ ê°ì§€)
    # UPSERT ì „ì— ì¡°íšŒí•œ ê¸°ì¡´ ë ˆì½”ë“œë¡œ ìš°ì„ ìˆœìœ„ ë¹„êµ
    if existing_record_before_upsert:
        existing_site_type = existing_record_before_upsert.site_type
        current_priority = self._get_priority(self.site_type)
        existing_priority = self._get_priority(existing_site_type)

        if current_priority > existing_priority:
            processing_status = 'duplicate_updated'  # ì—…ë°ì´íŠ¸ë¨
        elif current_priority == existing_priority:
            processing_status = 'duplicate_updated'  # ìµœì‹  ë°ì´í„° ìš°ì„ 
        else:
            processing_status = 'duplicate_preserved'  # ê¸°ì¡´ ìœ ì§€
```

**ìš°ì„ ìˆœìœ„ ì •ì±…** (_get_priority ë©”ì„œë“œ):
- Eminwon: 3
- Homepage: 2
- Scraper: 1
- API (kStartUp, bizInfo, smes24): 0

**ì¼€ì´ìŠ¤ 3: url_key ì—†ëŠ” ê²½ìš°** (line 1823-1834)
```python
if not url_key:
    self._log_api_url_processing(
        processing_status='no_url_key',
        error_message="URL ì •ê·œí™” ì‹¤íŒ¨ (url_key ì—†ìŒ)"
    )
```

---

### 3. ì²˜ë¦¬ ìƒíƒœ (processing_status)

| ìƒíƒœ | ì˜ë¯¸ | ë°œìƒ ì¡°ê±´ |
|------|------|----------|
| `new_inserted` | ìƒˆë¡œ ì‚½ì…ë¨ | affected_rows=1 (ì¤‘ë³µ ì•„ë‹˜) |
| `duplicate_updated` | ì¤‘ë³µì´ì§€ë§Œ ì—…ë°ì´íŠ¸ë¨ | affected_rows=2 + ìš°ì„ ìˆœìœ„ ë†’ìŒ/ë™ì¼ |
| `duplicate_preserved` | ì¤‘ë³µì´ë¼ ê¸°ì¡´ ìœ ì§€ | affected_rows=2 + ìš°ì„ ìˆœìœ„ ë‚®ìŒ |
| `duplicate_skipped` | ì¤‘ë³µ ê°ì§€í–ˆìœ¼ë‚˜ ì œì™¸ë¨ | affected_rows=2 + domain_key_config ì—†ìŒ |
| `no_url_key` | URL ì •ê·œí™” ì‹¤íŒ¨ | url_keyê°€ None |
| `failed` | ì²˜ë¦¬ ì‹¤íŒ¨ | ì˜ˆìƒì¹˜ ëª»í•œ affected_rows |

---

### 4. api_url_processing_log ê¸°ë¡

ëª¨ë“  ì²˜ë¦¬ ê²°ê³¼ëŠ” `api_url_processing_log` í…Œì´ë¸”ì— ê¸°ë¡ë©ë‹ˆë‹¤:

```python
self._log_api_url_processing(
    session=session,
    site_code=db_site_code,
    url_key=url_key,
    url_key_hash=url_key_hash,
    processing_status=processing_status,
    preprocessing_id=record_id,
    existing_preprocessing_id=existing_preprocessing_id,
    existing_site_type=existing_site_type,
    existing_site_code=existing_site_code,
    duplicate_reason=duplicate_reason,
    title=title,
    folder_name=folder_name
)
```

**ê¸°ë¡ ë‚´ìš©**:
- url_key, url_key_hash
- processing_status
- duplicate_reason (JSON)
- ê¸°ì¡´ ë ˆì½”ë“œ ì •ë³´ (existing_preprocessing_id, existing_site_type ë“±)

---

## âœ… ì˜ë„ëŒ€ë¡œ ë™ì‘í•˜ëŠ”ì§€ ê²€ì¦

### 1. ì¤‘ë³µ ì²´í¬ ë™ì‘ ì—¬ë¶€

**ì˜ˆìƒ ë™ì‘**:
1. âœ… url_key_hash UNIQUE ì œì•½ìœ¼ë¡œ ì¤‘ë³µ ê°ì§€
2. âœ… affected_rows=2 ì‹œ UPSERT ì‹¤í–‰
3. âœ… domain_key_config ìœ ë¬´ì— ë”°ë¼ ë¶„ê¸° ì²˜ë¦¬

**ê²€ì¦ ë°©ë²•**:
```sql
-- 1. ì¤‘ë³µ ì²˜ë¦¬ ë¡œê·¸ ì¡°íšŒ
SELECT processing_status, COUNT(*) as cnt
FROM api_url_processing_log
GROUP BY processing_status;

-- ì˜ˆìƒ ê²°ê³¼:
-- new_inserted: ì‹ ê·œ ì‚½ì… ê±´ìˆ˜
-- duplicate_updated: ì¤‘ë³µì´ì§€ë§Œ ì—…ë°ì´íŠ¸ëœ ê±´ìˆ˜
-- duplicate_skipped: í´ë°±ìœ¼ë¡œ ì¤‘ë³µ ì œì™¸ëœ ê±´ìˆ˜
-- duplicate_preserved: ìš°ì„ ìˆœìœ„ ë‚®ì•„ ìœ ì§€ëœ ê±´ìˆ˜
-- no_url_key: URL ì •ê·œí™” ì‹¤íŒ¨ ê±´ìˆ˜
```

```sql
-- 2. url_key_hash ì¤‘ë³µ í™•ì¸
SELECT url_key_hash, COUNT(*) as cnt
FROM announcement_pre_processing
WHERE url_key_hash IS NOT NULL
GROUP BY url_key_hash
HAVING COUNT(*) > 1;

-- ì˜ˆìƒ ê²°ê³¼: 0ê±´ (UNIQUE ì œì•½ìœ¼ë¡œ ì¤‘ë³µ ë¶ˆê°€)
```

### 2. domain_key_config ìœ ë¬´ì— ë”°ë¥¸ ë¶„ê¸°

**domain_key_config ìˆëŠ” ê²½ìš°**:
- âœ… ì¤‘ë³µ ì²´í¬ í™œì„±í™”
- âœ… ìš°ì„ ìˆœìœ„ ë¹„êµ ìˆ˜í–‰
- âœ… duplicate_updated / duplicate_preserved ìƒíƒœ

**domain_key_config ì—†ëŠ” ê²½ìš°**:
- âœ… ì¤‘ë³µ ê°ì§€ëŠ” í•˜ì§€ë§Œ ìš°ì„ ìˆœìœ„ ë¹„êµ ì•ˆ í•¨
- âœ… duplicate_skipped ìƒíƒœ
- âœ… duplicate_reasonì— `"fallback_used": true` í‘œì‹œ

**ê²€ì¦ ë°©ë²•**:
```sql
-- í´ë°± ì‚¬ìš© ë¡œê·¸ ì¡°íšŒ
SELECT *
FROM api_url_processing_log
WHERE processing_status = 'duplicate_skipped'
AND duplicate_reason LIKE '%fallback_used%'
LIMIT 10;
```

### 3. ìš°ì„ ìˆœìœ„ ì •ì±…

**ìš°ì„ ìˆœìœ„**: Eminwon (3) > Homepage (2) > Scraper (1) > API (0)

**ì˜ˆìƒ ë™ì‘**:
- API ê³µê³  í›„ Eminwon ê³µê³  ìˆ˜ì§‘ â†’ **ì—…ë°ì´íŠ¸** (duplicate_updated)
- Eminwon ê³µê³  í›„ API ê³µê³  ìˆ˜ì§‘ â†’ **ê¸°ì¡´ ìœ ì§€** (duplicate_preserved)

**ê²€ì¦ ë°©ë²•**:
```sql
-- ìš°ì„ ìˆœìœ„ì— ë”°ë¥¸ ì²˜ë¦¬ ë¡œê·¸
SELECT
    processing_status,
    existing_site_type,
    site_code,
    duplicate_reason,
    COUNT(*) as cnt
FROM api_url_processing_log
WHERE processing_status IN ('duplicate_updated', 'duplicate_preserved')
GROUP BY processing_status, existing_site_type, site_code;
```

---

## âš ï¸ ê³ ë ¤ì‚¬í•­ ë° ì ì¬ì  ì´ìŠˆ

### 1. domain_key_config ì—†ëŠ” ë„ë©”ì¸ì˜ ì‹¤ì œ ì¤‘ë³µ

**í˜„ìƒ**:
- domain_key_config ì—†ëŠ” ë„ë©”ì¸ì—ì„œ ë™ì¼ ê³µê³ ê°€ ì—¬ëŸ¬ ë²ˆ ìˆ˜ì§‘ë˜ì–´ë„
- url_key_hash UNIQUE ì œì•½ìœ¼ë¡œ INSERT ì‹¤íŒ¨
- affected_rows=2 (UPDATE ì‹¤í–‰)
- `duplicate_skipped` ìƒíƒœë¡œ ì²˜ë¦¬
- **ë¡œê·¸ë§Œ ê¸°ë¡, ë°ì´í„°ëŠ” ì—…ë°ì´íŠ¸ ì•ˆ ë¨**

**ì˜í–¥**:
- ì‹¤ì œ ì¤‘ë³µ ê³µê³ ê°€ ìˆì–´ë„ ë°ì´í„°ê°€ ê°±ì‹ ë˜ì§€ ì•ŠìŒ
- ê³µê³  ë‚´ìš©ì´ ë³€ê²½ë˜ì–´ë„ ë°˜ì˜ ì•ˆ ë  ìˆ˜ ìˆìŒ

**ì˜ˆì‹œ**:
```
1ì°¨ ìˆ˜ì§‘: www.example.com/notice?id=123 â†’ url_key_hash=abc123 â†’ INSERT
2ì°¨ ìˆ˜ì§‘: www.example.com/notice?id=123 (ë‚´ìš© ë³€ê²½ë¨)
  â†’ url_key_hash=abc123 (ë™ì¼)
  â†’ affected_rows=2
  â†’ domain_key_config ì—†ìŒ
  â†’ duplicate_skipped
  â†’ ê¸°ì¡´ ë°ì´í„° ìœ ì§€ (ë³€ê²½ ë‚´ìš© ë°˜ì˜ ì•ˆ ë¨)
```

**ê°œì„  ë°©ì•ˆ**:
1. domain_key_config ì—†ëŠ” ë„ë©”ì¸ë„ ìš°ì„ ìˆœìœ„ ë¹„êµ ìˆ˜í–‰
2. ë˜ëŠ” ìµœì‹  ë°ì´í„°ë¡œ í•­ìƒ ì—…ë°ì´íŠ¸ (updated_at ê¸°ì¤€)
3. ë˜ëŠ” ì¤‘ë³µ ì²´í¬ ì™„ì „ ì œì™¸ (url_key_hash UNIQUE ì œê±°)

### 2. url_key_hash UNIQUE ì œì•½ê³¼ í´ë°±ì˜ ì¶©ëŒ

**ë¬¸ì œ**:
- `url_key_hash UNIQUE` ì œì•½ì€ DB ë ˆë²¨ì—ì„œ í•­ìƒ ê°•ì œ
- í•˜ì§€ë§Œ ì½”ë“œ ë ˆë²¨ì—ì„œëŠ” domain_key_config ì—†ìœ¼ë©´ ì¤‘ë³µ ì²´í¬ ì œì™¸
- **ëª¨ìˆœ**: DBëŠ” ì¤‘ë³µ í—ˆìš© ì•ˆ í•˜ì§€ë§Œ, ì½”ë“œëŠ” ì¤‘ë³µ ì²´í¬ ì•ˆ í•¨

**ê²°ê³¼**:
- affected_rows=2 ë°˜í™˜ë˜ì§€ë§Œ ìš°ì„ ìˆœìœ„ ë¹„êµ ì•ˆ í•¨
- ë¡œê·¸ì—ë§Œ ê¸°ë¡, ì‹¤ì œ ë°ì´í„°ëŠ” ì¡°ê±´ë¶€ ì—…ë°ì´íŠ¸

**ê°œì„  ë°©ì•ˆ**:
1. domain_key_config ì—†ëŠ” ë„ë©”ì¸ì€ url_key_hashë¥¼ NULLë¡œ ì„¤ì •
2. ë˜ëŠ” url_key_hash ëŒ€ì‹  url_keyì—ë§Œ INDEX ì„¤ì • (UNIQUE ì œê±°)
3. ë˜ëŠ” domain_key_config ì—†ëŠ” ë„ë©”ì¸ë„ ì •ìƒ ì¤‘ë³µ ì²´í¬ ìˆ˜í–‰

### 3. UPSERT ì „ ê¸°ì¡´ ë ˆì½”ë“œ ì¡°íšŒ

**í˜„ì¬ ë¡œì§** (line 1637-1647):
```python
if force and url_key:
    existing_record_before_upsert = session.execute(
        text("SELECT id, site_type, site_code FROM announcement_pre_processing WHERE url_key = :url_key"),
        {"url_key": url_key}
    ).fetchone()
```

**ë¬¸ì œ**:
- `force=True`ì¼ ë•Œë§Œ ê¸°ì¡´ ë ˆì½”ë“œ ì¡°íšŒ
- `force=False`ì´ë©´ ì¡°íšŒ ì•ˆ í•¨ â†’ ìš°ì„ ìˆœìœ„ ë¹„êµ ë¶ˆê°€
- í•˜ì§€ë§Œ UPSERTëŠ” force ê´€ê³„ì—†ì´ í•­ìƒ ì‹¤í–‰

**ì˜í–¥**:
- force=False ì‹œ existing_site_type ì •ë³´ ì—†ìŒ
- line 1942-1945: "UPSERT ì „ ê¸°ì¡´ ë ˆì½”ë“œ ì¡°íšŒ ì‹¤íŒ¨" ì²˜ë¦¬
- duplicate_updatedë¡œ ê°„ì£¼ (ìš°ì„ ìˆœìœ„ ë¹„êµ ì—†ì´)

**ê°œì„  ë°©ì•ˆ**:
```python
# force ê´€ê³„ì—†ì´ í•­ìƒ ì¡°íšŒ
if url_key:
    existing_record_before_upsert = session.execute(...)
```

### 4. folder_name ì¤‘ë³µ ì²´í¬ì™€ url_key ì¤‘ë³µ ì²´í¬ì˜ ì´ì¤‘í™”

**í˜„ì¬**:
1. **folder_name ì¤‘ë³µ ì²´í¬** (line 446-448)
   - force=False ì‹œ folder_name ì¤‘ë³µë˜ë©´ ê±´ë„ˆëœ€
   - UNIQUE KEY on folder_name

2. **url_key_hash ì¤‘ë³µ ì²´í¬** (line 1853-1970)
   - UPSERT ì‹œ url_key_hash ì¤‘ë³µë˜ë©´ ìš°ì„ ìˆœìœ„ ë¹„êµ
   - UNIQUE KEY on url_key_hash

**ë¬¸ì œ**:
- ê°™ì€ ê³µê³ ë¥¼ ë‹¤ë¥¸ í´ë”ëª…ìœ¼ë¡œ ë‘ ë²ˆ ìˆ˜ì§‘í•˜ë©´?
  - folder_name ë‹¤ë¦„ â†’ ì²« ë²ˆì§¸ ì²´í¬ í†µê³¼
  - url_key_hash ê°™ìŒ â†’ UPSERT ì‹¤í–‰
  - ì¤‘ë³µ ì²˜ë¦¬ë¨ (ì •ìƒ)

- í•˜ì§€ë§Œ folder_name UNIQUEë„ ìˆì–´ì„œ ê°™ì€ í´ë” ë‘ ë²ˆ ì²˜ë¦¬ ë¶ˆê°€

**ê°œì„  ë°©ì•ˆ**:
- folder_nameì€ ì²˜ë¦¬ ì´ë ¥ ê´€ë¦¬ìš©
- url_key_hashëŠ” ì‹¤ì œ ì¤‘ë³µ ê°ì§€ìš©
- **í˜„ì¬ ì„¤ê³„ëŠ” ì •ìƒì ìœ¼ë¡œ ë³´ì„**

### 5. api_url_registry ì—…ë°ì´íŠ¸ íƒ€ì´ë°

**í˜„ì¬** (line 1973-1984):
```python
# API ì‚¬ì´íŠ¸ì¸ ê²½ìš° api_url_registry í…Œì´ë¸” ì—…ë°ì´íŠ¸ (commit ì „ì— ì‹¤í–‰)
if origin_url:
    api_registry_updated = self._update_api_url_registry(
        session, origin_url, record_id, db_site_code, scraping_url
    )
```

**ë¬¸ì œ**:
- api_url_registry ì—…ë°ì´íŠ¸ëŠ” ì¤‘ë³µ ì—¬ë¶€ì™€ ë¬´ê´€í•˜ê²Œ ì‹¤í–‰
- ì¤‘ë³µìœ¼ë¡œ ê¸°ì¡´ ë°ì´í„° ìœ ì§€ë˜ì–´ë„ api_url_registryëŠ” ì—…ë°ì´íŠ¸

**ì˜í–¥**:
- ì¼ê´€ì„± ë¬¸ì œëŠ” ì—†ìœ¼ë‚˜, ë¶ˆí•„ìš”í•œ ì—…ë°ì´íŠ¸ ë°œìƒ ê°€ëŠ¥

**ê°œì„  ë°©ì•ˆ**:
```python
# duplicate_updatedì¼ ë•Œë§Œ api_url_registry ì—…ë°ì´íŠ¸
if processing_status == 'duplicate_updated' and origin_url:
    api_registry_updated = self._update_api_url_registry(...)
```

### 6. í˜ì´ì§€ë„¤ì´ì…˜ íŒŒë¼ë¯¸í„° ì œì™¸

**í´ë°± ì •ê·œí™”** (_fallback_normalize_url, line 1207-1250):
```python
# í˜ì´ì§€ë„¤ì´ì…˜/ê²€ìƒ‰ íŒŒë¼ë¯¸í„° ì œì™¸
exclude_params = {
    'page', 'pageIndex', 'pageNo', 'pageSize', 'pageNum',
    'currentPage', 'searchCnd', 'searchWrd', 'srchWrd'
}
```

**ë¬¸ì œ**:
- ëª¨ë“  ë„ë©”ì¸ì— ë™ì¼í•œ ì œì™¸ íŒŒë¼ë¯¸í„° ì ìš©
- ì¼ë¶€ ì‚¬ì´íŠ¸ëŠ” page íŒŒë¼ë¯¸í„°ê°€ ê²Œì‹œê¸€ IDì¼ ìˆ˜ ìˆìŒ
- ì˜ëª»ëœ ì œì™¸ë¡œ ë‹¤ë¥¸ ê³µê³ ê°€ ì¤‘ë³µìœ¼ë¡œ íŒì •ë  ìˆ˜ ìˆìŒ

**ì˜ˆì‹œ**:
```
www.example.com/notice?page=123  # page=ê²Œì‹œê¸€ID
www.example.com/notice?page=456  # page=ê²Œì‹œê¸€ID

â†’ ë‘ ê³µê³  ëª¨ë‘ url_key = "www.example.com|/notice|" (page ì œì™¸ë¨)
â†’ ì¤‘ë³µìœ¼ë¡œ íŒì •ë¨ (ì‹¤ì œë¡œëŠ” ë‹¤ë¥¸ ê³µê³ )
```

**ê°œì„  ë°©ì•ˆ**:
- domain_key_configì— exclude_params ì„¤ì • ì¶”ê°€
- ë˜ëŠ” í´ë°± ë¡œì§ì—ì„œ ë” ì •êµí•œ íŒŒë¼ë¯¸í„° ë¶„ì„

---

## ğŸ“Š í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

ì‘ì„±ëœ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸: `test_duplicate_check_url_key.py`

### í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤

1. **domain_key_config ì¡´ì¬ - ì¤‘ë³µ ì²´í¬ í™œì„±í™”**
   - k-startup.go.kr ë“± API ì‚¬ì´íŠ¸
   - ì¤‘ë³µ ì²˜ë¦¬ ë¡œê·¸ í™•ì¸
   - ìš°ì„ ìˆœìœ„ ë¹„êµ ë™ì‘ í™•ì¸

2. **domain_key_config ì—†ìŒ - ì¤‘ë³µ ì²´í¬ ì œì™¸ (í´ë°±)**
   - í´ë°± ë¡œì§ ì‚¬ìš© í™•ì¸
   - duplicate_skipped ìƒíƒœ í™•ì¸
   - duplicate_reasonì— fallback_used í‘œì‹œ í™•ì¸

3. **url_key ì—†ìŒ - no_url_key ìƒíƒœ**
   - url_key NULL ë ˆì½”ë“œ ì¡°íšŒ
   - no_url_key ì²˜ë¦¬ ë¡œê·¸ í™•ì¸

4. **ì „ì²´ í†µê³„**
   - processing_statusë³„ ê±´ìˆ˜
   - ì¤‘ë³µ ì²˜ë¦¬ ë¹„ìœ¨
   - url_key ì¡´ì¬ ì—¬ë¶€ í†µê³„

### ì‹¤í–‰ ë°©ë²•

```bash
# MySQL ì„œë²„ ì‹œì‘ í•„ìš”
python3 test_duplicate_check_url_key.py
```

**ê²°ê³¼ íŒŒì¼**: `test_duplicate_check_results.json`

---

## ğŸ¯ ê²°ë¡ 

### ì˜ë„ëŒ€ë¡œ ë™ì‘í•˜ëŠ” ë¶€ë¶„

1. âœ… **url_key_hash UNIQUE ì œì•½**ìœ¼ë¡œ ì¤‘ë³µ ê°ì§€ ì •ìƒ ë™ì‘
2. âœ… **UPSERT ë¡œì§**ìœ¼ë¡œ affected_rows=2 ì‹œ ì¤‘ë³µ ì²˜ë¦¬
3. âœ… **domain_key_config ìœ ë¬´**ì— ë”°ë¥¸ ë¶„ê¸° ì²˜ë¦¬
4. âœ… **ìš°ì„ ìˆœìœ„ ì •ì±…** ì ìš© (Eminwon > Homepage > Scraper > API)
5. âœ… **api_url_processing_log** ëª¨ë“  ì²˜ë¦¬ ê¸°ë¡

### ì ì¬ì  ë¬¸ì œì 

1. âš ï¸ **domain_key_config ì—†ëŠ” ë„ë©”ì¸ì˜ ì‹¤ì œ ì¤‘ë³µ**
   - ì¤‘ë³µ ê°ì§€ëŠ” í•˜ì§€ë§Œ ë°ì´í„° ì—…ë°ì´íŠ¸ ì•ˆ ë¨
   - ê³µê³  ë³€ê²½ ì‚¬í•­ ë°˜ì˜ ì•ˆ ë  ìˆ˜ ìˆìŒ

2. âš ï¸ **force=False ì‹œ ìš°ì„ ìˆœìœ„ ë¹„êµ ë¶ˆê°€**
   - existing_record ì¡°íšŒ ì•ˆ í•¨
   - duplicate_updatedë¡œ ê°„ì£¼

3. âš ï¸ **í˜ì´ì§€ë„¤ì´ì…˜ íŒŒë¼ë¯¸í„° ì œì™¸**
   - ì¼ë¶€ ì‚¬ì´íŠ¸ì—ì„œ ì˜¤íƒ ê°€ëŠ¥ì„±

### ê¶Œì¥ ì‚¬í•­

1. **domain_key_config ì—†ëŠ” ë„ë©”ì¸ ì²˜ë¦¬ ê°œì„ **
   ```python
   # ì˜µì…˜ 1: ìµœì‹  ë°ì´í„°ë¡œ í•­ìƒ ì—…ë°ì´íŠ¸
   if not domain_has_config and affected_rows == 2:
       processing_status = 'duplicate_updated'

   # ì˜µì…˜ 2: url_key_hashë¥¼ NULLë¡œ ì„¤ì • (ì¤‘ë³µ ì²´í¬ ì™„ì „ ì œì™¸)
   if not domain_has_config:
       url_key_hash = None
   ```

2. **force ê´€ê³„ì—†ì´ ê¸°ì¡´ ë ˆì½”ë“œ ì¡°íšŒ**
   ```python
   # force=Falseì—¬ë„ ìš°ì„ ìˆœìœ„ ë¹„êµë¥¼ ìœ„í•´ ì¡°íšŒ
   if url_key and affected_rows == 2:
       existing_record_before_upsert = session.execute(...)
   ```

3. **ë„ë©”ì¸ë³„ exclude_params ì„¤ì •**
   - domain_key_configì— exclude_params ì»¬ëŸ¼ ì¶”ê°€
   - ë˜ëŠ” í´ë°± ë¡œì§ì—ì„œ ë” ì •êµí•œ íŒŒë¼ë¯¸í„° ë¶„ì„

---

## ğŸ“ ì¶”ê°€ ê²€ì¦ í•„ìš” ì‚¬í•­

DB ì„œë²„ ì ‘ì† í›„ ë‹¤ìŒ ì¿¼ë¦¬ë¡œ ì‹¤ì œ ë™ì‘ í™•ì¸:

```sql
-- 1. ì²˜ë¦¬ ìƒíƒœë³„ í†µê³„
SELECT processing_status, COUNT(*) as cnt
FROM api_url_processing_log
GROUP BY processing_status
ORDER BY cnt DESC;

-- 2. ì¤‘ë³µ ì²˜ë¦¬ ìƒì„¸ (ìƒ˜í”Œ 10ê±´)
SELECT
    url_key_hash,
    site_code,
    processing_status,
    existing_site_type,
    duplicate_reason,
    created_at
FROM api_url_processing_log
WHERE processing_status LIKE 'duplicate%'
ORDER BY created_at DESC
LIMIT 10;

-- 3. í´ë°± ì‚¬ìš© ë¡œê·¸
SELECT COUNT(*) as cnt
FROM api_url_processing_log
WHERE duplicate_reason LIKE '%fallback_used%';

-- 4. url_key_hash ì¤‘ë³µ í™•ì¸ (0ê±´ì´ì–´ì•¼ ì •ìƒ)
SELECT url_key_hash, COUNT(*) as cnt
FROM announcement_pre_processing
WHERE url_key_hash IS NOT NULL
GROUP BY url_key_hash
HAVING COUNT(*) > 1;

-- 5. url_key ì—†ëŠ” ë ˆì½”ë“œ
SELECT COUNT(*) as cnt
FROM announcement_pre_processing
WHERE url_key IS NULL;
```

---

**ì‘ì„±ì¼**: 2025-10-30
**ì‘ì„±ì**: Claude Code Assistant
**ë¶„ì„ ëŒ€ìƒ íŒŒì¼**: announcement_pre_processor.py (line 1622-1994)
