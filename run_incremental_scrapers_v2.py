#!/usr/bin/env python3
import os
import sys
import subprocess
import pymysql
from dotenv import load_dotenv
from datetime import datetime
from pathlib import Path

load_dotenv()

DB_HOST = os.getenv('DB_HOST')
DB_PORT = int(os.getenv('DB_PORT', 3306))
DB_USER = os.getenv('DB_USER')
DB_PASSWORD = os.getenv('DB_PASSWORD')
DB_NAME = os.getenv('DB_NAME')

NODE_DIR = Path(__file__).parent / "node"
SCRAPER_DIR = NODE_DIR / "scraper"
BASE_OUTPUT_DIR = Path(__file__).parent / "scraped_incremental_v2"

def get_db_connection():
    return pymysql.connect(
        host=DB_HOST,
        port=DB_PORT,
        user=DB_USER,
        password=DB_PASSWORD,
        database=DB_NAME,
        charset='utf8mb4',
        cursorclass=pymysql.cursors.DictCursor
    )

def get_sites_to_scrape():
    conn = get_db_connection()
    try:
        with conn.cursor() as cursor:
            cursor.execute("""
                SELECT 
                    site_code,
                    latest_announcement_date
                FROM homepage_site_announcement_date
                ORDER BY site_code
            """)
            return cursor.fetchall()
    finally:
        conn.close()

def get_latest_date_from_scraped_files(site_code, output_dir):
    """ìŠ¤í¬ëž˜í•‘ëœ íŒŒì¼ì—ì„œ ìµœì‹  ë‚ ì§œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    from pathlib import Path
    import re
    
    output_path = Path(output_dir)
    if not output_path.exists():
        return None
    
    # 001_ë¡œ ì‹œìž‘í•˜ëŠ” ì²« ë²ˆì§¸ í´ë” ì°¾ê¸°
    first_dir = None
    for item_dir in sorted(output_path.iterdir()):
        if item_dir.is_dir() and item_dir.name.startswith('001_'):
            first_dir = item_dir
            break
    
    if not first_dir:
        # 001_ë¡œ ì‹œìž‘í•˜ëŠ” í´ë”ê°€ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ë””ë ‰í† ë¦¬ ì‚¬ìš©
        dirs = [d for d in output_path.iterdir() if d.is_dir()]
        if dirs:
            first_dir = sorted(dirs)[0]
        else:
            return None
    
    # content.md íŒŒì¼ ì½ê¸°
    content_md_path = first_dir / "content.md"
    if not content_md_path.exists():
        print(f"  âš ï¸ content.md íŒŒì¼ ì—†ìŒ: {first_dir.name}")
        return None
    
    try:
        with open(content_md_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # ë‚ ì§œ ì¶”ì¶œ íŒ¨í„´
        date_patterns = [
            r'\*\*ìž‘ì„±ì¼\*\*[:\s]*(.+?)(?:\n|$)',
            r'ìž‘ì„±ì¼[:\s]*(.+?)(?:\n|$)',
            r'\*\*ë“±ë¡ì¼\*\*[:\s]*(.+?)(?:\n|$)',
            r'ë“±ë¡ì¼[:\s]*(.+?)(?:\n|$)',
            r'\*\*ê³µê³ ì¼\*\*[:\s]*(.+?)(?:\n|$)',
            r'ê³µê³ ì¼[:\s]*(.+?)(?:\n|$)'
        ]
        
        announcement_date = None
        for pattern in date_patterns:
            date_match = re.search(pattern, content, re.IGNORECASE)
            if date_match:
                announcement_date = date_match.group(1).strip()
                break
        
        if announcement_date:
            print(f"  ðŸ“„ íŒŒì¼ì—ì„œ ì¶”ì¶œí•œ ìµœì‹  ë‚ ì§œ: {announcement_date} (from {first_dir.name})")
            return announcement_date
        else:
            print(f"  âš ï¸ ë‚ ì§œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {first_dir.name}")
            return None
            
    except Exception as e:
        print(f"  âŒ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
        return None

def update_latest_announcement_date(site_code, output_dir=None):
    """ìŠ¤í¬ëž˜í•‘ ì™„ë£Œ í›„ í•´ë‹¹ ì‚¬ì´íŠ¸ì˜ ìµœì‹  ê³µê³  ë‚ ì§œë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
    
    # ìŠ¤í¬ëž˜í•‘ëœ íŒŒì¼ì—ì„œ ìµœì‹  ë‚ ì§œ ê°€ì ¸ì˜¤ê¸°
    latest_date_str = None
    if output_dir:
        latest_date_str = get_latest_date_from_scraped_files(site_code, output_dir)
    
    if not latest_date_str:
        print(f"  âš ï¸ ë‚ ì§œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ DB ì—…ë°ì´íŠ¸ ìŠ¤í‚µ")
        return False
    
    # ë‚ ì§œ í˜•ì‹ ë³€í™˜ (YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ)
    from datetime import datetime
    import re
    
    try:
        # ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ ì²˜ë¦¬
        date_obj = None
        
        # YYYY-MM-DD í˜•ì‹
        if re.match(r'^\d{4}-\d{2}-\d{2}$', latest_date_str):
            date_obj = datetime.strptime(latest_date_str, '%Y-%m-%d')
        # YYYY.MM.DD í˜•ì‹
        elif re.match(r'^\d{4}\.\d{2}\.\d{2}$', latest_date_str):
            date_obj = datetime.strptime(latest_date_str, '%Y.%m.%d')
        # YYYYMMDD í˜•ì‹
        elif re.match(r'^\d{8}$', latest_date_str):
            date_obj = datetime.strptime(latest_date_str, '%Y%m%d')
        # YYYYë…„ MMì›” DDì¼ í˜•ì‹
        elif re.match(r'^\d{4}ë…„\s*\d{1,2}ì›”\s*\d{1,2}ì¼$', latest_date_str):
            date_obj = datetime.strptime(re.sub(r'[ë…„ì›”ì¼\s]', '-', latest_date_str).rstrip('-'), '%Y-%m-%d')
        else:
            print(f"  âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ë‚ ì§œ í˜•ì‹: {latest_date_str}")
            return False
        
        # YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        formatted_date = date_obj.strftime('%Y-%m-%d')
        
        # DB ì—…ë°ì´íŠ¸
        conn = get_db_connection()
        try:
            with conn.cursor() as cursor:
                cursor.execute("""
                    UPDATE homepage_site_announcement_date
                    SET latest_announcement_date = %s,
                        updated_at = NOW()
                    WHERE site_code = %s
                """, (formatted_date, site_code))
                
                conn.commit()
                print(f"  ðŸ“… DB ì—…ë°ì´íŠ¸ ì„±ê³µ: {site_code} â†’ {formatted_date}")
                return True
        finally:
            conn.close()
                
    except Exception as e:
        print(f"  âŒ DB ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {site_code} - {e}")
        return False
    finally:
        conn.close()

def check_scraper_exists(site_code):
    scraper_path = SCRAPER_DIR / f"{site_code}_scraper.js"
    return scraper_path.exists(), scraper_path

def run_scraper(site_code, from_date):
    exists, scraper_path = check_scraper_exists(site_code)
    
    if not exists:
        return {
            'site_code': site_code,
            'status': 'skipped',
            'reason': f'ìŠ¤í¬ëž˜í¼ íŒŒì¼ ì—†ìŒ: {scraper_path}'
        }
    
    target_year = from_date.year
    from_date_str = from_date.strftime("%Y-%m-%d")
    today_str = datetime.now().strftime("%Y-%m-%d")
    # ìŠ¤í¬ëž˜í¼ê°€ ë‚´ë¶€ì ìœ¼ë¡œ site_codeë¥¼ ì¶”ê°€í•˜ë¯€ë¡œ, ì—¬ê¸°ì„œëŠ” ë‚ ì§œ ë””ë ‰í† ë¦¬ê¹Œì§€ë§Œ ìƒì„±
    base_dir_for_date = BASE_OUTPUT_DIR / today_str
    base_dir_for_date.mkdir(parents=True, exist_ok=True)
    
    # ì‹¤ì œ output_dirëŠ” ìŠ¤í¬ëž˜í¼ê°€ ìƒì„±í•  ê²ƒì´ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” base_dirë§Œ ì „ë‹¬
    expected_output_dir = base_dir_for_date / site_code  # ì˜ˆìƒ ì¶œë ¥ ë””ë ‰í† ë¦¬ (ë¡œê¹…ìš©)
    
    try:
        # ìŠ¤í¬ëž˜í¼ì— ì „ë‹¬í•  arguments (named arguments í˜•ì‹)
        cmd = [
            "node",
            str(scraper_path),
            "--output", str(base_dir_for_date),     # ë‚ ì§œ ë””ë ‰í† ë¦¬ê¹Œì§€ë§Œ ì „ë‹¬
            "--date", from_date_str,          # ì‹œìž‘ ë‚ ì§œ
            "--site", site_code,              # ì‚¬ì´íŠ¸ ì½”ë“œ
            "--force"                         # ê¸°ì¡´ í´ë” ë®ì–´ì“°ê¸°
        ]
        
        print(f"\n[{site_code}] ìŠ¤í¬ëž˜í¼ ì‹¤í–‰")
        print(f"  ìŠ¤í¬ëž˜í¼ íŒŒì¼: {scraper_path}")
        print(f"  ì‹œìž‘ì¼: {from_date_str}")
        print(f"  ì¢…ë£Œì¼: {today_str}")
        print(f"  ê¸°ë³¸ ì¶œë ¥ ë””ë ‰í† ë¦¬: {base_dir_for_date}")
        print(f"  ì˜ˆìƒ ìµœì¢… ë””ë ‰í† ë¦¬: {expected_output_dir}")
        print(f"  ìž‘ì—… ë””ë ‰í† ë¦¬: {NODE_DIR}")
        print(f"  ëª…ë ¹: {' '.join(cmd)}")
        print(f"  Arguments: --output {base_dir_for_date} --date {from_date_str} --site {site_code} --force")
        
        # í™˜ê²½ë³€ìˆ˜ ì„¤ì • (í•„ìš”ì‹œ)
        env = os.environ.copy()
        env['NODE_ENV'] = 'production'
        
        result = subprocess.run(
            cmd,
            cwd=str(NODE_DIR),
            capture_output=True,
            text=True,
            timeout=300,
            env=env
        )
        
        # stdout, stderr ì¶œë ¥ (ë””ë²„ê¹…ìš©)
        if result.stdout:
            print(f"  [STDOUT]: {result.stdout[:500]}")
        if result.stderr:
            print(f"  [STDERR]: {result.stderr[:500]}")
        
        if result.returncode == 0:
            # stdoutì´ ì—†ì–´ë„ ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìžˆë„ë¡ ì²´í¬
            scraped_count = 0
            if "scraped" in result.stdout.lower():
                # stdoutì—ì„œ ìŠ¤í¬ëž˜í•‘ ê°œìˆ˜ ì¶”ì¶œ ì‹œë„
                import re
                match = re.search(r'(\d+)\s*(?:items?|announcements?|ê³µê³ )', result.stdout)
                if match:
                    scraped_count = int(match.group(1))
            
            return {
                'site_code': site_code,
                'status': 'success',
                'output_dir': str(expected_output_dir),
                'scraped_count': scraped_count,
                'stdout': result.stdout[-500:] if len(result.stdout) > 500 else result.stdout
            }
        else:
            return {
                'site_code': site_code,
                'status': 'failed',
                'returncode': result.returncode,
                'error': result.stderr if result.stderr else result.stdout,
                'stdout': result.stdout[-500:] if len(result.stdout) > 500 else result.stdout
            }
    
    except subprocess.TimeoutExpired:
        return {
            'site_code': site_code,
            'status': 'timeout',
            'error': '5ë¶„ íƒ€ìž„ì•„ì›ƒ ì´ˆê³¼'
        }
    except Exception as e:
        return {
            'site_code': site_code,
            'status': 'error',
            'error': str(e)
        }

def main():
    print("=" * 80)
    print("í™ˆíŽ˜ì´ì§€ ê³ ì‹œ/ê³µê³  ì ì§„ì  ìŠ¤í¬ëž˜í•‘ v2")
    print("=" * 80)
    
    sites = get_sites_to_scrape()
    print(f"\nì´ {len(sites)}ê°œ ì‚¬ì´íŠ¸ ëŒ€ìƒ")
    
    BASE_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    
    results = {
        'success': [],
        'failed': [],
        'skipped': [],
        'timeout': [],
        'error': []
    }
    
    for idx, site in enumerate(sites, 1):
        site_code = site['site_code']
        from_date = site['latest_announcement_date']
        
        print(f"\n{'='*80}")
        print(f"[{idx}/{len(sites)}] {site_code}")
        print(f"{'='*80}")
        
        result = run_scraper(site_code, from_date)
        status = result['status']
        results[status].append(result)
        
        if status == 'success':
            print(f"  âœ“ ì„±ê³µ: {result['output_dir']}")
            # ìŠ¤í¬ëž˜í•‘ ì„±ê³µ ì‹œ DB ì—…ë°ì´íŠ¸
            update_latest_announcement_date(site_code, result['output_dir'])
        elif status == 'skipped':
            print(f"  âŠ˜ ìŠ¤í‚µ: {result['reason']}")
        elif status == 'failed':
            print(f"  âœ— ì‹¤íŒ¨: {result['error'][:200]}")
        elif status == 'timeout':
            print(f"  â± íƒ€ìž„ì•„ì›ƒ: {result['error']}")
        elif status == 'error':
            print(f"  âš  ì˜¤ë¥˜: {result['error'][:200]}")
    
    print("\n" + "=" * 80)
    print("ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½")
    print("=" * 80)
    print(f"ì„±ê³µ: {len(results['success'])}ê°œ")
    print(f"ì‹¤íŒ¨: {len(results['failed'])}ê°œ")
    print(f"ìŠ¤í‚µ: {len(results['skipped'])}ê°œ")
    print(f"íƒ€ìž„ì•„ì›ƒ: {len(results['timeout'])}ê°œ")
    print(f"ì˜¤ë¥˜: {len(results['error'])}ê°œ")
    
    if results['skipped']:
        print(f"\nìŠ¤í‚µëœ ì‚¬ì´íŠ¸ ({len(results['skipped'])}ê°œ):")
        for r in results['skipped'][:10]:
            print(f"  - {r['site_code']}")
        if len(results['skipped']) > 10:
            print(f"  ... ì™¸ {len(results['skipped']) - 10}ê°œ")
    
    print("\n" + "=" * 80)
    print("ì™„ë£Œ!")
    print("=" * 80)

if __name__ == "__main__":
    main()