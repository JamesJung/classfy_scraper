#!/usr/bin/env python3
"""
api_url_registryì—ì„œ 2025-10-30 ì´í›„ ë°ì´í„°ë¥¼ announcement_pre_processingì— ë“±ë¡

ì „ì œì¡°ê±´:
1. announcement_pre_processingì—ì„œ êµ¬ api_scrap ë°ì´í„° ì‚­ì œ ì™„ë£Œ
2. /home/zium/moabojo/incremental/api/{site_code}/{folder_id}/content.md íŒŒì¼ ì¡´ì¬

ê¸°ëŠ¥:
1. api_url_registryì—ì„œ post_date >= '2025-10-30' ë°ì´í„° ì¡°íšŒ
2. ê° ë ˆì½”ë“œì˜ folder_nameì—ì„œ í´ë” ê²½ë¡œ ì¶”ì¶œ
3. content.md íŒŒì¼ ì½ê¸°
4. announcement_pre_processing í…Œì´ë¸”ì— INSERT
5. api_url_registryì˜ preprocessing_id ì—…ë°ì´íŠ¸
"""

import mysql.connector
from pathlib import Path
from datetime import datetime
import sys
import re
import json
from typing import Dict, List, Optional, Tuple
import hashlib

# ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„¤ì •
DB_CONFIG = {
    'host': '192.168.0.95',
    'port': 3309,
    'user': 'root',
    'password': 'b3UvSDS232GbdZ42',
    'database': 'subvention'
}

# ê¸°ì¤€ ë‚ ì§œ
CUTOFF_DATE = '2025-10-30'

# í”„ë¡œë•ì…˜ ë°ì´í„° ê²½ë¡œ
BASE_DATA_PATH = Path("/home/zium/moabojo/incremental")


class ApiDataRegistrar:
    def __init__(self, dry_run: bool = True):
        """
        Args:
            dry_run: Trueë©´ ë¯¸ë¦¬ë³´ê¸°ë§Œ, Falseë©´ ì‹¤ì œ ë“±ë¡
        """
        self.dry_run = dry_run
        self.stats = {
            'total': 0,
            'content_md_found': 0,
            'content_md_not_found': 0,
            'registered': 0,
            'skipped': 0,
            'errors': 0
        }
        self.errors_list = []

    def extract_folder_info(self, folder_name: str) -> Tuple[str, str]:
        """
        folder_nameì—ì„œ site_codeì™€ folder_id ì¶”ì¶œ

        Args:
            folder_name: "output/data/kStartUp/175424" í˜•ì‹

        Returns:
            (site_code, folder_id) íŠœí”Œ
        """
        # output/data/{site_code}/{folder_id} íŒ¨í„´
        match = re.search(r'output/data/([^/]+)/(.+)$', folder_name)
        if match:
            return match.group(1), match.group(2)

        # ì˜ˆì™¸ ì²˜ë¦¬: ë‹¤ë¥¸ íŒ¨í„´ì´ ìˆì„ ìˆ˜ ìˆìŒ
        parts = folder_name.split('/')
        if len(parts) >= 2:
            return parts[-2], parts[-1]

        return None, None

    def get_content_md_path(self, site_code: str, folder_id: str) -> Path:
        """
        content.md íŒŒì¼ ê²½ë¡œ ìƒì„±

        Args:
            site_code: bizInfo, kStartUp, smes24
            folder_id: í´ë” ID

        Returns:
            ì „ì²´ ê²½ë¡œ
        """
        return BASE_DATA_PATH / "api" / site_code / folder_id / "content.md"

    def extract_title_from_content(self, content: str) -> Optional[str]:
        """content.mdì—ì„œ ì œëª© ì¶”ì¶œ"""
        lines = content.split('\n')
        for line in lines:
            if line.startswith('# '):
                return line[2:].strip()
            if line.startswith('## '):
                return line[3:].strip()
        return None

    def extract_url_from_content(self, content: str, marker: str) -> Optional[str]:
        """content.mdì—ì„œ URL ì¶”ì¶œ"""
        pattern = rf'{marker}:\s*(.+)'
        match = re.search(pattern, content)
        if match:
            return match.group(1).strip()
        return None

    def extract_date_from_content(self, content: str) -> Optional[str]:
        """content.mdì—ì„œ ë‚ ì§œ ì¶”ì¶œ"""
        # ê³µê³ ì¼ì, ê²Œì‹œì¼, ë“±ë¡ì¼ ë“± ë‹¤ì–‘í•œ í˜•ì‹
        patterns = [
            r'ê³µê³ ì¼ì:\s*(.+)',
            r'ê²Œì‹œì¼:\s*(.+)',
            r'ë“±ë¡ì¼:\s*(.+)',
            r'ë‚ ì§œ:\s*(.+)',
        ]
        for pattern in patterns:
            match = re.search(pattern, content)
            if match:
                return match.group(1).strip()
        return None

    def convert_date_to_yyyymmdd(self, date_str: str) -> Optional[str]:
        """ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ì„ YYYYMMDDë¡œ ë³€í™˜"""
        if not date_str:
            return None

        # ì´ë¯¸ YYYYMMDD í˜•ì‹
        if len(date_str) == 8 and date_str.isdigit():
            return date_str

        # YYYY.MM.DD, YYYY-MM-DD ë“±
        date_str = date_str.replace('.', '-').replace('/', '-')
        try:
            from datetime import datetime
            dt = datetime.strptime(date_str.split()[0], '%Y-%m-%d')
            return dt.strftime('%Y%m%d')
        except:
            return None

    def get_new_api_data(self) -> List[Dict]:
        """
        api_url_registryì—ì„œ 2025-10-30 ì´í›„ ë°ì´í„° ì¡°íšŒ

        Returns:
            ë ˆì½”ë“œ ëª©ë¡
        """
        try:
            conn = mysql.connector.connect(**DB_CONFIG)
            cursor = conn.cursor(dictionary=True)

            query = f"""
                SELECT
                    id,
                    site_code,
                    site_name,
                    announcement_url,
                    announcement_id,
                    title,
                    post_date,
                    status,
                    folder_name,
                    scrap_url,
                    url_key,
                    url_key_hash
                FROM api_url_registry
                WHERE post_date >= '{CUTOFF_DATE}'
                ORDER BY site_code, post_date
            """

            cursor.execute(query)
            records = cursor.fetchall()

            cursor.close()
            conn.close()

            print(f"âœ… ë°ì´í„°ë² ì´ìŠ¤ ì¡°íšŒ ì™„ë£Œ: {len(records)}ê±´")
            return records

        except mysql.connector.Error as err:
            print(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì˜¤ë¥˜: {err}")
            sys.exit(1)

    def register_to_preprocessing(self, record: Dict, content_md: str) -> Optional[int]:
        """
        announcement_pre_processing í…Œì´ë¸”ì— ë ˆì½”ë“œ ë“±ë¡

        Args:
            record: api_url_registry ë ˆì½”ë“œ
            content_md: content.md ë‚´ìš©

        Returns:
            ìƒì„±ëœ ë ˆì½”ë“œ ID ë˜ëŠ” None
        """
        try:
            conn = mysql.connector.connect(**DB_CONFIG)
            cursor = conn.cursor()

            # content.mdì—ì„œ ì •ë³´ ì¶”ì¶œ
            title = record['title'] or self.extract_title_from_content(content_md)
            origin_url = record['announcement_url']
            scraping_url = record['scrap_url']
            url_key = record['url_key']

            # ë‚ ì§œ ë³€í™˜
            announcement_date = None
            if record['post_date']:
                announcement_date = record['post_date'].strftime('%Y%m%d')

            # folder_name ì¶”ì¶œ
            _, folder_id = self.extract_folder_info(record['folder_name'])

            # INSERT ì¿¼ë¦¬
            sql = """
                INSERT INTO announcement_pre_processing (
                    folder_name, site_type, site_code, content_md, combined_content,
                    attachment_filenames, attachment_files_list, exclusion_keyword, exclusion_reason,
                    title, origin_url, url_key, scraping_url, announcement_date,
                    processing_status, error_message, created_at, updated_at
                ) VALUES (
                    %s, %s, %s, %s, %s,
                    %s, %s, %s, %s,
                    %s, %s, %s, %s, %s,
                    %s, %s, NOW(), NOW()
                )
            """

            params = (
                folder_id,  # folder_name
                'api_scrap',  # site_type
                record['site_code'],  # site_code
                content_md,  # content_md
                '',  # combined_content (ì²¨ë¶€íŒŒì¼ ì²˜ë¦¬ëŠ” ë³„ë„)
                None,  # attachment_filenames
                None,  # attachment_files_list
                None,  # exclusion_keyword
                None,  # exclusion_reason
                title,  # title
                origin_url,  # origin_url
                url_key,  # url_key
                scraping_url,  # scraping_url
                announcement_date,  # announcement_date
                'success',  # processing_status
                None,  # error_message
            )

            cursor.execute(sql, params)
            record_id = cursor.lastrowid

            # api_url_registry ì—…ë°ì´íŠ¸
            update_sql = """
                UPDATE api_url_registry
                SET preprocessing_id = %s,
                    update_at = NOW()
                WHERE id = %s
            """
            cursor.execute(update_sql, (record_id, record['id']))

            conn.commit()
            cursor.close()
            conn.close()

            return record_id

        except mysql.connector.Error as err:
            print(f"  âŒ DB ì˜¤ë¥˜: {err}")
            self.errors_list.append({
                'folder_name': record['folder_name'],
                'error': str(err)
            })
            return None

    def process_records(self, records: List[Dict]):
        """
        ë ˆì½”ë“œ ì²˜ë¦¬ ë©”ì¸ ë¡œì§
        """
        print("\n" + "=" * 80)
        print(f"ì²˜ë¦¬ ì‹œì‘: {len(records)}ê±´")
        print("=" * 80)

        site_stats = {}

        for i, record in enumerate(records, 1):
            self.stats['total'] += 1

            site_code = record['site_code']
            folder_name = record['folder_name']
            post_date = record['post_date']

            # ì‚¬ì´íŠ¸ë³„ í†µê³„ ì´ˆê¸°í™”
            if site_code not in site_stats:
                site_stats[site_code] = {
                    'total': 0,
                    'content_md_found': 0,
                    'content_md_not_found': 0,
                    'registered': 0,
                    'errors': 0
                }

            site_stats[site_code]['total'] += 1

            # folder_nameì—ì„œ site_codeì™€ folder_id ì¶”ì¶œ
            _, folder_id = self.extract_folder_info(folder_name)

            if not folder_id:
                print(f"âš ï¸  [{i}/{len(records)}] folder_name íŒŒì‹± ì‹¤íŒ¨: {folder_name}")
                self.stats['errors'] += 1
                site_stats[site_code]['errors'] += 1
                self.errors_list.append({
                    'folder_name': folder_name,
                    'error': 'Cannot parse folder_name'
                })
                continue

            # content.md ê²½ë¡œ ìƒì„±
            content_md_path = self.get_content_md_path(site_code, folder_id)

            # ì§„í–‰ ìƒí™© ì¶œë ¥ (10ê±´ë§ˆë‹¤)
            if i % 10 == 0 or i <= 10:
                print(f"\n[{i}/{len(records)}] ì²˜ë¦¬ ì¤‘...")

            # content.md ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            if not content_md_path.exists():
                if i <= 10:
                    print(f"  â­ï¸  ê±´ë„ˆëœ€ (content.md ì—†ìŒ): {content_md_path.relative_to(BASE_DATA_PATH)}")
                self.stats['content_md_not_found'] += 1
                site_stats[site_code]['content_md_not_found'] += 1
                continue

            # content.md ì½ê¸°
            try:
                with open(content_md_path, 'r', encoding='utf-8') as f:
                    content_md = f.read()

                # DO_NOT_PROCESS í”Œë˜ê·¸ í™•ì¸
                if "DO_NOT_PROCESS" in content_md:
                    if i <= 10:
                        print(f"  â­ï¸  ê±´ë„ˆëœ€ (ARCHIVED): {folder_id}")
                    self.stats['skipped'] += 1
                    continue

                self.stats['content_md_found'] += 1
                site_stats[site_code]['content_md_found'] += 1

                if i <= 10:
                    print(f"  ğŸ“ ë“±ë¡: {folder_id} - {record['title'][:50]}")

                # DRY RUN ëª¨ë“œê°€ ì•„ë‹ˆë©´ ì‹¤ì œ ë“±ë¡
                if not self.dry_run:
                    record_id = self.register_to_preprocessing(record, content_md)
                    if record_id:
                        self.stats['registered'] += 1
                        site_stats[site_code]['registered'] += 1
                        if i <= 10:
                            print(f"    âœ… ë“±ë¡ ì™„ë£Œ (ID: {record_id})")
                    else:
                        self.stats['errors'] += 1
                        site_stats[site_code]['errors'] += 1
                else:
                    self.stats['registered'] += 1
                    site_stats[site_code]['registered'] += 1

            except Exception as e:
                print(f"  âŒ ì˜¤ë¥˜: {content_md_path} - {e}")
                self.stats['errors'] += 1
                site_stats[site_code]['errors'] += 1
                self.errors_list.append({
                    'path': str(content_md_path),
                    'error': str(e)
                })

        # ì‚¬ì´íŠ¸ë³„ í†µê³„ ì¶œë ¥
        print("\n" + "=" * 80)
        print("ì‚¬ì´íŠ¸ë³„ í†µê³„")
        print("=" * 80)
        for site_code, stats in site_stats.items():
            print(f"\n[{site_code}]")
            print(f"  ì´ ë ˆì½”ë“œ: {stats['total']}ê±´")
            print(f"  content.md ë°œê²¬: {stats['content_md_found']}ê±´")
            print(f"  content.md ì—†ìŒ: {stats['content_md_not_found']}ê±´")
            print(f"  ë“±ë¡ ì™„ë£Œ: {stats['registered']}ê±´")
            print(f"  ì—ëŸ¬: {stats['errors']}ê±´")

    def print_summary(self):
        """
        ìµœì¢… í†µê³„ ì¶œë ¥
        """
        print("\n" + "=" * 80)
        print("ìµœì¢… í†µê³„")
        print("=" * 80)
        print(f"ì´ ë ˆì½”ë“œ: {self.stats['total']}ê±´")
        print(f"content.md ë°œê²¬: {self.stats['content_md_found']}ê±´")
        print(f"content.md ì—†ìŒ: {self.stats['content_md_not_found']}ê±´")
        print(f"ê±´ë„ˆëœ€ (ARCHIVED): {self.stats['skipped']}ê±´")
        print(f"ë“±ë¡ ì™„ë£Œ: {self.stats['registered']}ê±´")
        print(f"ì—ëŸ¬: {self.stats['errors']}ê±´")

        if self.errors_list:
            print(f"\nì—ëŸ¬ ëª©ë¡ (ìµœëŒ€ 10ê±´):")
            for error in self.errors_list[:10]:
                print(f"  - {error}")

        print("\n" + "=" * 80)
        if self.dry_run:
            print("DRY RUN ëª¨ë“œ ì™„ë£Œ")
            print("ì‹¤ì œ ë“±ë¡ì„ í•˜ë ¤ë©´ ë‹¤ìŒ ëª…ë ¹ì„ ì‹¤í–‰í•˜ì„¸ìš”:")
            print("  python register_new_api_data_to_preprocessing.py --execute")
        else:
            print("ì‹¤í–‰ ì™„ë£Œ")
            print(f"ì´ {self.stats['registered']}ê±´ì´ announcement_pre_processingì— ë“±ë¡ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("=" * 80)


def main():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
    """
    if len(sys.argv) > 1 and sys.argv[1] == '--execute':
        dry_run = False
    else:
        dry_run = True

    print("=" * 80)
    print("API ì‹ ê·œ ë°ì´í„° announcement_pre_processing ë“±ë¡ ìŠ¤í¬ë¦½íŠ¸")
    print("=" * 80)
    print(f"ê¸°ì¤€ ë‚ ì§œ: {CUTOFF_DATE} ì´í›„ ë°ì´í„°")
    print(f"ë°ì´í„° ê²½ë¡œ: {BASE_DATA_PATH}/api/")
    print(f"ëª¨ë“œ: {'DRY RUN (ë¯¸ë¦¬ë³´ê¸°)' if dry_run else 'EXECUTE (ì‹¤ì œ ë“±ë¡)'}")
    print("=" * 80)

    # í™•ì¸
    if not dry_run:
        print("\nâš ï¸  WARNING: announcement_pre_processing í…Œì´ë¸”ì— ì‹¤ì œë¡œ ë°ì´í„°ë¥¼ ë“±ë¡í•©ë‹ˆë‹¤!")
        print("âš ï¸  WARNING: ì´ì „ì— êµ¬ ë°ì´í„°ë¥¼ ì‚­ì œí–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”!")
        confirm = input("ê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (yes/no): ")
        if confirm.lower() != 'yes':
            print("ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            sys.exit(0)

    # ì²˜ë¦¬ ì‹œì‘
    registrar = ApiDataRegistrar(dry_run)

    # ë°ì´í„° ì¡°íšŒ
    records = registrar.get_new_api_data()

    if not records:
        print("ì²˜ë¦¬í•  ë ˆì½”ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit(0)

    # ë ˆì½”ë“œ ì²˜ë¦¬
    registrar.process_records(records)

    # í†µê³„ ì¶œë ¥
    registrar.print_summary()


if __name__ == '__main__':
    main()
