#!/usr/bin/env python3
"""
Ollama ì„œë²„ ë™ì‹œ ì²˜ë¦¬ ì„±ëŠ¥ ì¢…í•© í…ŒìŠ¤íŠ¸ í”„ë¡œê·¸ë¨

ë™ì‹œ ìš”ì²­ ê°œìˆ˜ë¥¼ ì ì§„ì ìœ¼ë¡œ ì¦ê°€ì‹œí‚¤ë©° ì„±ëŠ¥ê³¼ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ì„ ì¸¡ì •í•©ë‹ˆë‹¤.
"""

import asyncio
import time
import json
import csv
from datetime import datetime
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
from pathlib import Path
import sys
import argparse

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python pathì— ì¶”ê°€
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.config.logConfig import setup_logging
from ollama_performance_tester import OllamaPerformanceTester, PerformanceMetric, BatchMetric
from resource_monitor import RemoteResourceMonitor, MonitoringSession

logger = setup_logging(__name__)


@dataclass
class ExperimentResult:
    """ì‹¤í—˜ ê²°ê³¼"""
    experiment_id: str
    concurrent_requests: int
    baseline_avg_response_time: float
    baseline_success_rate: float
    
    # ë°°ì¹˜ ì„±ëŠ¥
    batch_total_time: float
    batch_success_rate: float
    batch_average_response_time: float
    batch_throughput_rps: float
    batch_first_response_time: float
    batch_last_response_time: float
    
    # ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰
    avg_cpu_percent: float
    max_cpu_percent: float
    avg_memory_percent: float
    max_memory_percent: float
    avg_gpu_utilization: float
    max_gpu_utilization: float
    avg_gpu_memory_percent: float
    max_gpu_memory_percent: float
    
    # ì„±ëŠ¥ ë¹„êµ
    speedup_ratio: float  # ê¸°ì¤€ì„  ëŒ€ë¹„ ì„±ëŠ¥ í–¥ìƒ ë¹„ìœ¨
    efficiency: float     # ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ì„± (throughput / cpu_usage)
    
    # í’ˆì§ˆ ì ìˆ˜
    avg_quality_score: float
    quality_degradation: float  # ê¸°ì¤€ì„  ëŒ€ë¹„ í’ˆì§ˆ ì €í•˜


class OllamaConcurrentTester:
    """Ollama ë™ì‹œ ì²˜ë¦¬ ì¢…í•© í…ŒìŠ¤í„°"""
    
    def __init__(self, 
                 ollama_url: str = "http://192.168.0.40:11434/api",
                 model: str = "gpt-oss:20b",
                 server_host: str = "192.168.0.40"):
        """
        Args:
            ollama_url: Ollama API URL
            model: ì‚¬ìš©í•  ëª¨ë¸ëª…  
            server_host: ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§í•  ì„œë²„ í˜¸ìŠ¤íŠ¸
        """
        self.ollama_url = ollama_url
        self.model = model
        self.server_host = server_host
        
        self.performance_tester = None
        self.resource_monitor = None
        self.results: List[ExperimentResult] = []
    
    async def __aenter__(self):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì‹œì‘"""
        self.performance_tester = await OllamaPerformanceTester(
            ollama_url=self.ollama_url, 
            model=self.model
        ).__aenter__()
        
        self.resource_monitor = RemoteResourceMonitor(server_host=self.server_host)
        
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        if self.performance_tester:
            await self.performance_tester.__aexit__(exc_type, exc_val, exc_tb)
    
    def _get_sample_content_batch(self, batch_size: int) -> List[str]:
        """í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ì½˜í…ì¸  ë°°ì¹˜ ìƒì„±"""
        contents = []
        sample_types = ["short", "medium", "long"]
        
        for i in range(batch_size):
            sample_type = sample_types[i % len(sample_types)]
            content = self.performance_tester.sample_texts[sample_type]
            contents.append(content)
        
        return contents
    
    async def _measure_baseline_performance(self, iterations: int = 5) -> Tuple[float, float, float]:
        """ê¸°ì¤€ì„  ì„±ëŠ¥ ì¸¡ì • (ìˆœì°¨ ì²˜ë¦¬)"""
        logger.info(f"ê¸°ì¤€ì„  ì„±ëŠ¥ ì¸¡ì • ì‹œì‘: {iterations}íšŒ")
        
        baseline_metrics = await self.performance_tester.baseline_performance(iterations)
        successful_metrics = [m for m in baseline_metrics if m.success]
        
        if not successful_metrics:
            return 0.0, 0.0, 0.0
        
        avg_response_time = sum(m.response_time for m in successful_metrics) / len(successful_metrics)
        success_rate = len(successful_metrics) / len(baseline_metrics)
        avg_quality_score = sum(m.response_quality_score for m in successful_metrics) / len(successful_metrics)
        
        logger.info(f"ê¸°ì¤€ì„  ì„±ëŠ¥: í‰ê· ì‘ë‹µ {avg_response_time:.2f}ì´ˆ, ì„±ê³µë¥  {success_rate:.2%}, í’ˆì§ˆ {avg_quality_score:.3f}")
        return avg_response_time, success_rate, avg_quality_score
    
    async def _run_concurrent_experiment(self, 
                                       concurrent_count: int,
                                       baseline_avg_time: float,
                                       baseline_quality: float) -> ExperimentResult:
        """ê°œë³„ ë™ì‹œ ì²˜ë¦¬ ì‹¤í—˜ ì‹¤í–‰"""
        experiment_id = f"concurrent_{concurrent_count}_{int(time.time())}"
        logger.info(f"ì‹¤í—˜ ì‹œì‘: {concurrent_count}ê°œ ë™ì‹œ ìš”ì²­")
        
        # í…ŒìŠ¤íŠ¸ ì½˜í…ì¸  ì¤€ë¹„
        contents = self._get_sample_content_batch(concurrent_count)
        
        # ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œì‘
        self.resource_monitor.start_monitoring()
        
        try:
            # ë™ì‹œ ìš”ì²­ ì‹¤í–‰
            batch_result = await self.performance_tester.concurrent_requests(
                contents, batch_id=experiment_id
            )
            
            # ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€
            monitoring_session = self.resource_monitor.stop_monitoring()
            
            # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
            speedup_ratio = baseline_avg_time / batch_result.average_response_time if batch_result.average_response_time > 0 else 0.0
            theoretical_max_speedup = concurrent_count
            efficiency = speedup_ratio / theoretical_max_speedup if theoretical_max_speedup > 0 else 0.0
            
            # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
            successful_metrics = [m for m in batch_result.individual_metrics if m.success]
            avg_quality_score = sum(m.response_quality_score for m in successful_metrics) / len(successful_metrics) if successful_metrics else 0.0
            quality_degradation = baseline_quality - avg_quality_score
            
            # CPU íš¨ìœ¨ì„± ê³„ì‚° (throughput per CPU usage)
            cpu_efficiency = batch_result.throughput_rps / monitoring_session.avg_cpu_percent if monitoring_session and monitoring_session.avg_cpu_percent > 0 else 0.0
            
            result = ExperimentResult(
                experiment_id=experiment_id,
                concurrent_requests=concurrent_count,
                baseline_avg_response_time=baseline_avg_time,
                baseline_success_rate=1.0,  # ê¸°ì¤€ì„ ì€ í•­ìƒ ì„±ê³µìœ¼ë¡œ ê°€ì •
                
                batch_total_time=batch_result.total_time,
                batch_success_rate=batch_result.success_count / batch_result.concurrent_requests,
                batch_average_response_time=batch_result.average_response_time,
                batch_throughput_rps=batch_result.throughput_rps,
                batch_first_response_time=batch_result.first_response_time,
                batch_last_response_time=batch_result.last_response_time,
                
                avg_cpu_percent=monitoring_session.avg_cpu_percent if monitoring_session else 0.0,
                max_cpu_percent=monitoring_session.max_cpu_percent if monitoring_session else 0.0,
                avg_memory_percent=monitoring_session.avg_memory_percent if monitoring_session else 0.0,
                max_memory_percent=monitoring_session.max_memory_percent if monitoring_session else 0.0,
                avg_gpu_utilization=monitoring_session.avg_gpu_utilization if monitoring_session else 0.0,
                max_gpu_utilization=monitoring_session.max_gpu_utilization if monitoring_session else 0.0,
                avg_gpu_memory_percent=monitoring_session.avg_gpu_memory_percent if monitoring_session else 0.0,
                max_gpu_memory_percent=monitoring_session.max_gpu_memory_percent if monitoring_session else 0.0,
                
                speedup_ratio=speedup_ratio,
                efficiency=cpu_efficiency,
                
                avg_quality_score=avg_quality_score,
                quality_degradation=quality_degradation
            )
            
            logger.info(f"ì‹¤í—˜ ì™„ë£Œ: {concurrent_count}ê°œ ë™ì‹œìš”ì²­, ì„±ëŠ¥í–¥ìƒ {speedup_ratio:.2f}ë°°")
            return result
            
        except Exception as e:
            logger.error(f"ì‹¤í—˜ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ({concurrent_count}ê°œ): {e}")
            # ëª¨ë‹ˆí„°ë§ ì¤‘ì§€
            try:
                self.resource_monitor.stop_monitoring()
            except:
                pass
            raise
    
    async def run_full_experiment(self, 
                                max_concurrent: int = 8,
                                baseline_iterations: int = 5,
                                step_size: int = 2) -> List[ExperimentResult]:
        """ì „ì²´ ì‹¤í—˜ ì‹¤í–‰"""
        logger.info(f"Ollama ë™ì‹œ ì²˜ë¦¬ ì„±ëŠ¥ ì‹¤í—˜ ì‹œì‘ (ìµœëŒ€ {max_concurrent}ê°œ)")
        
        print("ğŸ§ª Ollama ì„œë²„ ë™ì‹œ ì²˜ë¦¬ ì„±ëŠ¥ ì‹¤í—˜")
        print(f"{'='*60}")
        print(f"ì„œë²„: {self.server_host}")
        print(f"ëª¨ë¸: {self.model}")
        print(f"ìµœëŒ€ ë™ì‹œ ìš”ì²­: {max_concurrent}ê°œ")
        print(f"{'='*60}")
        
        # Phase 1: ê¸°ì¤€ì„  ì„±ëŠ¥ ì¸¡ì •
        print(f"\n1ï¸âƒ£ ê¸°ì¤€ì„  ì„±ëŠ¥ ì¸¡ì • ({baseline_iterations}íšŒ ìˆœì°¨ ì²˜ë¦¬)")
        baseline_avg_time, baseline_success_rate, baseline_quality = await self._measure_baseline_performance(baseline_iterations)
        
        if baseline_avg_time == 0:
            print("âŒ ê¸°ì¤€ì„  ì„±ëŠ¥ ì¸¡ì • ì‹¤íŒ¨")
            return []
        
        print(f"âœ… ê¸°ì¤€ì„  ì„±ëŠ¥: í‰ê·  {baseline_avg_time:.2f}ì´ˆ, ì„±ê³µë¥  {baseline_success_rate:.1%}")
        
        # Phase 2: ë™ì‹œ ì²˜ë¦¬ ì‹¤í—˜
        print(f"\n2ï¸âƒ£ ë™ì‹œ ì²˜ë¦¬ ì„±ëŠ¥ ì‹¤í—˜")
        concurrent_counts = list(range(2, max_concurrent + 1, step_size))
        if max_concurrent not in concurrent_counts:
            concurrent_counts.append(max_concurrent)
        
        results = []
        
        for i, concurrent_count in enumerate(concurrent_counts, 1):
            print(f"\n[{i}/{len(concurrent_counts)}] {concurrent_count}ê°œ ë™ì‹œ ìš”ì²­ í…ŒìŠ¤íŠ¸...")
            
            try:
                result = await self._run_concurrent_experiment(
                    concurrent_count, baseline_avg_time, baseline_quality
                )
                results.append(result)
                
                # ì‹¤ì‹œê°„ ê²°ê³¼ ì¶œë ¥
                print(f"  âœ… ì™„ë£Œ: {result.batch_success_rate:.1%} ì„±ê³µ")
                print(f"     ì´ ì†Œìš”: {result.batch_total_time:.1f}ì´ˆ")
                print(f"     ì²˜ë¦¬ëŸ‰: {result.batch_throughput_rps:.2f} RPS")
                print(f"     ì„±ëŠ¥í–¥ìƒ: {result.speedup_ratio:.2f}ë°°")
                print(f"     CPU ì‚¬ìš©: {result.avg_cpu_percent:.1f}%")
                
                # ì„±ëŠ¥ ì €í•˜ ê°ì§€
                if result.speedup_ratio < 1.0:
                    print(f"  âš ï¸ ì„±ëŠ¥ ì €í•˜ ê°ì§€: {result.speedup_ratio:.2f}ë°°")
                
                # ì‹¤íŒ¨ìœ¨ ë†’ìŒ ê°ì§€
                if result.batch_success_rate < 0.8:
                    print(f"  âš ï¸ ë†’ì€ ì‹¤íŒ¨ìœ¨: {result.batch_success_rate:.1%}")
                
            except Exception as e:
                print(f"  âŒ ì‹¤í—˜ ì‹¤íŒ¨: {e}")
                logger.error(f"ë™ì‹œ ìš”ì²­ {concurrent_count}ê°œ ì‹¤í—˜ ì‹¤íŒ¨: {e}")
        
        self.results = results
        return results
    
    def analyze_results(self) -> Dict[str, Any]:
        """ì‹¤í—˜ ê²°ê³¼ ë¶„ì„"""
        if not self.results:
            return {}
        
        # ìµœì  ì„±ëŠ¥ ì§€ì  ì°¾ê¸°
        best_speedup = max(self.results, key=lambda r: r.speedup_ratio)
        best_throughput = max(self.results, key=lambda r: r.batch_throughput_rps)
        best_efficiency = max(self.results, key=lambda r: r.efficiency)
        
        # ì•ˆì •ì„± ê¸°ì¤€ (ì„±ê³µë¥  95% ì´ìƒ)
        stable_results = [r for r in self.results if r.batch_success_rate >= 0.95]
        best_stable = max(stable_results, key=lambda r: r.speedup_ratio) if stable_results else None
        
        analysis = {
            "total_experiments": len(self.results),
            "best_speedup": {
                "concurrent_count": best_speedup.concurrent_requests,
                "speedup_ratio": best_speedup.speedup_ratio,
                "success_rate": best_speedup.batch_success_rate
            },
            "best_throughput": {
                "concurrent_count": best_throughput.concurrent_requests,
                "throughput_rps": best_throughput.batch_throughput_rps,
                "success_rate": best_throughput.batch_success_rate
            },
            "best_efficiency": {
                "concurrent_count": best_efficiency.concurrent_requests,
                "efficiency": best_efficiency.efficiency,
                "cpu_usage": best_efficiency.avg_cpu_percent
            },
            "recommended_stable": {
                "concurrent_count": best_stable.concurrent_requests if best_stable else None,
                "speedup_ratio": best_stable.speedup_ratio if best_stable else None,
                "success_rate": best_stable.batch_success_rate if best_stable else None
            } if best_stable else None
        }
        
        return analysis
    
    def print_summary_report(self):
        """ì‹¤í—˜ ê²°ê³¼ ìš”ì•½ ë³´ê³ ì„œ ì¶œë ¥"""
        if not self.results:
            print("âŒ ì‹¤í—˜ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")
            return
        
        analysis = self.analyze_results()
        
        print(f"\nğŸ“Š Ollama ë™ì‹œ ì²˜ë¦¬ ì„±ëŠ¥ ì‹¤í—˜ ê²°ê³¼ ìš”ì•½")
        print(f"{'='*60}")
        print(f"ì´ ì‹¤í—˜ íšŸìˆ˜: {analysis['total_experiments']}ê°œ")
        
        # ìµœê³  ì„±ëŠ¥ ì§€ì 
        print(f"\nğŸ† ìµœê³  ì„±ëŠ¥ í–¥ìƒ:")
        best = analysis['best_speedup']
        print(f"   ë™ì‹œ ìš”ì²­ ìˆ˜: {best['concurrent_count']}ê°œ")
        print(f"   ì„±ëŠ¥ í–¥ìƒ: {best['speedup_ratio']:.2f}ë°°")
        print(f"   ì„±ê³µë¥ : {best['success_rate']:.1%}")
        
        # ìµœê³  ì²˜ë¦¬ëŸ‰
        print(f"\nâš¡ ìµœê³  ì²˜ë¦¬ëŸ‰:")
        throughput = analysis['best_throughput']
        print(f"   ë™ì‹œ ìš”ì²­ ìˆ˜: {throughput['concurrent_count']}ê°œ")
        print(f"   ì²˜ë¦¬ëŸ‰: {throughput['throughput_rps']:.2f} RPS")
        print(f"   ì„±ê³µë¥ : {throughput['success_rate']:.1%}")
        
        # ê¶Œì¥ ì„¤ì •
        recommended = analysis.get('recommended_stable')
        if recommended:
            print(f"\nâœ… ê¶Œì¥ ì„¤ì • (ì•ˆì •ì„± ìš°ì„ ):")
            print(f"   ë™ì‹œ ìš”ì²­ ìˆ˜: {recommended['concurrent_count']}ê°œ")
            print(f"   ì„±ëŠ¥ í–¥ìƒ: {recommended['speedup_ratio']:.2f}ë°°")
            print(f"   ì„±ê³µë¥ : {recommended['success_rate']:.1%}")
        else:
            print(f"\nâš ï¸ ì•ˆì •ì ì¸ ì„¤ì •ì„ ì°¾ì§€ ëª»í•¨ (ì„±ê³µë¥  95% ì´ìƒ ì—†ìŒ)")
        
        # ìƒì„¸ ê²°ê³¼ í…Œì´ë¸”
        print(f"\nğŸ“‹ ìƒì„¸ ì‹¤í—˜ ê²°ê³¼:")
        print(f"{'ë™ì‹œìš”ì²­':<8} {'ì„±ëŠ¥í–¥ìƒ':<8} {'ì²˜ë¦¬ëŸ‰':<8} {'ì„±ê³µë¥ ':<8} {'CPU':<8} {'GPU':<8}")
        print(f"{'-'*50}")
        
        for result in sorted(self.results, key=lambda r: r.concurrent_requests):
            print(f"{result.concurrent_requests:<8} "
                  f"{result.speedup_ratio:<8.2f} "
                  f"{result.batch_throughput_rps:<8.2f} "
                  f"{result.batch_success_rate:<8.1%} "
                  f"{result.avg_cpu_percent:<8.1f} "
                  f"{result.avg_gpu_utilization:<8.1f}")
    
    def save_detailed_results(self, filepath: str = None):
        """ìƒì„¸ ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥"""
        if not self.results:
            return
        
        if filepath is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filepath = f"ollama_concurrent_test_results_{timestamp}.csv"
        
        try:
            with open(filepath, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=list(asdict(self.results[0]).keys()))
                writer.writeheader()
                
                for result in self.results:
                    writer.writerow(asdict(result))
            
            print(f"ğŸ’¾ ìƒì„¸ ê²°ê³¼ ì €ì¥ë¨: {filepath}")
            logger.info(f"ì‹¤í—˜ ê²°ê³¼ ì €ì¥: {filepath}")
            
        except Exception as e:
            print(f"âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            logger.error(f"ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='Ollama ë™ì‹œ ì²˜ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸')
    parser.add_argument('--max-concurrent', type=int, default=8, help='ìµœëŒ€ ë™ì‹œ ìš”ì²­ ìˆ˜')
    parser.add_argument('--baseline-iterations', type=int, default=5, help='ê¸°ì¤€ì„  ì¸¡ì • ë°˜ë³µ íšŸìˆ˜')
    parser.add_argument('--step-size', type=int, default=2, help='ë™ì‹œ ìš”ì²­ ìˆ˜ ì¦ê°€ ë‹¨ìœ„')
    parser.add_argument('--quick', action='store_true', help='ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (2, 4ê°œë§Œ)')
    parser.add_argument('--output', type=str, help='ê²°ê³¼ ì €ì¥ íŒŒì¼ëª…')
    
    args = parser.parse_args()
    
    if args.quick:
        args.max_concurrent = 4
        args.baseline_iterations = 3
    
    async with OllamaConcurrentTester() as tester:
        results = await tester.run_full_experiment(
            max_concurrent=args.max_concurrent,
            baseline_iterations=args.baseline_iterations,
            step_size=args.step_size
        )
        
        # ê²°ê³¼ ë¶„ì„ ë° ì¶œë ¥
        tester.print_summary_report()
        
        # ê²°ê³¼ ì €ì¥
        tester.save_detailed_results(args.output)
        
        print(f"\nğŸ‰ ì‹¤í—˜ ì™„ë£Œ! ì´ {len(results)}ê°œ ì‹¤í—˜ ìˆ˜í–‰ë¨")


if __name__ == "__main__":
    asyncio.run(main())