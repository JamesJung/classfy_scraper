#!/usr/bin/env python3
"""
Ollama ì„œë²„ ê°œë³„ ì„±ëŠ¥ ì¸¡ì • ëª¨ë“ˆ

ë‹¨ì¼ ìš”ì²­ ë° ë™ì‹œ ìš”ì²­ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ê³  ì‘ë‹µ í’ˆì§ˆì„ ê²€ì¦í•©ë‹ˆë‹¤.
"""

import asyncio
import aiohttp
import time
import json
import statistics
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
from pathlib import Path
import sys

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python pathì— ì¶”ê°€
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.config.logConfig import setup_logging

logger = setup_logging(__name__)


@dataclass
class PerformanceMetric:
    """ê°œë³„ ìš”ì²­ ì„±ëŠ¥ ë©”íŠ¸ë¦­"""
    request_id: str
    start_time: float
    end_time: float
    response_time: float
    success: bool
    error_message: str = ""
    response_size: int = 0
    http_status: int = 0
    content_length: int = 0
    response_quality_score: float = 0.0


@dataclass
class BatchMetric:
    """ë°°ì¹˜ ì²˜ë¦¬ ì„±ëŠ¥ ë©”íŠ¸ë¦­"""
    batch_id: str
    concurrent_requests: int
    start_time: float
    end_time: float
    total_time: float
    first_response_time: float  # TTFR (Time To First Response)
    last_response_time: float   # TTL (Time To Last Response)
    individual_metrics: List[PerformanceMetric]
    success_count: int
    failure_count: int
    average_response_time: float
    median_response_time: float
    std_dev_response_time: float
    throughput_rps: float  # Requests Per Second


class OllamaPerformanceTester:
    """Ollama ì„œë²„ ì„±ëŠ¥ í…ŒìŠ¤í„°"""
    
    def __init__(self, 
                 ollama_url: str = "http://192.168.0.40:11434/api",
                 model: str = "gpt-oss:20b",
                 timeout: int = 120):
        """
        Args:
            ollama_url: Ollama API URL
            model: ì‚¬ìš©í•  ëª¨ë¸ëª…
            timeout: ìš”ì²­ íƒ€ì„ì•„ì›ƒ (ì´ˆ)
        """
        self.ollama_url = ollama_url
        self.model = model
        self.timeout = timeout
        self.session = None
        
        # í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ í…ìŠ¤íŠ¸ë“¤ (ë‹¤ì–‘í•œ ê¸¸ì´)
        self.sample_texts = {
            "short": """2025ë…„ ì¤‘ì†Œê¸°ì—… ì°½ì—… ì§€ì›ì‚¬ì—… ê³µê³ 
ì§€ì›ëŒ€ìƒ: ì°½ì—… 3ë…„ ì´ë‚´ ì¤‘ì†Œê¸°ì—…
ì§€ì›ê¸ˆì•¡: ìµœëŒ€ 5ì²œë§Œì›""",
            
            "medium": """2025ë…„ ì¤‘ì†Œê¸°ì—… ì°½ì—… ì§€ì›ì‚¬ì—… ê³µê³ 

## ì‚¬ì—…ê°œìš”
- ì‚¬ì—…ê¸°ê°„: 2025.1.1 ~ 2025.12.31
- ì§€ì›ëŒ€ìƒ: ì°½ì—… 3ë…„ ì´ë‚´ ì¤‘ì†Œê¸°ì—…
- ì§€ì›ë‚´ìš©: ì‚¬ì—…ë¹„ì˜ 70% ì§€ì› (ìµœëŒ€ 5ì²œë§Œì›)

## ì‹ ì²­ë°©ë²•
- ì ‘ìˆ˜ê¸°ê°„: 2025.1.15 ~ 2025.2.15
- ì œì¶œì²˜: ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ë¶€

## ë¬¸ì˜ì²˜
- ë‹´ë‹¹ì: í™ê¸¸ë™ (02-1234-5678)""",
            
            "long": """2025ë…„ ì¤‘ì†Œê¸°ì—… ì°½ì—… ì§€ì›ì‚¬ì—… ê³µê³ 

## 1. ì‚¬ì—…ê°œìš”
- ì‚¬ì—…ëª…: 2025ë…„ ì¤‘ì†Œê¸°ì—… ì°½ì—… ì§€ì›ì‚¬ì—…
- ì‚¬ì—…ê¸°ê°„: 2025.1.1 ~ 2025.12.31
- ì‚¬ì—…ì˜ˆì‚°: ì´ 100ì–µì›
- ì§€ì›ëŒ€ìƒ: ì°½ì—… 3ë…„ ì´ë‚´ ì¤‘ì†Œê¸°ì—… (ì œì¡°ì—…, ITì„œë¹„ìŠ¤ì—… ìš°ì„ )
- ì§€ì›ë‚´ìš©: ì‚¬ì—…ë¹„ì˜ 70% ì§€ì› (ìµœëŒ€ 5ì²œë§Œì›)

## 2. ì§€ì› ë¶„ì•¼
### 2.1 ì œì¡°ì—… ë¶„ì•¼
- ìŠ¤ë§ˆíŠ¸ ì œì¡° ê¸°ìˆ 
- ì¹œí™˜ê²½ ê¸°ìˆ 
- ë°”ì´ì˜¤ ê¸°ìˆ 
- ì‹ ì†Œì¬ ê¸°ìˆ 

### 2.2 ITì„œë¹„ìŠ¤ì—… ë¶„ì•¼  
- ì¸ê³µì§€ëŠ¥/ë¹…ë°ì´í„°
- ì‚¬ë¬¼ì¸í„°ë„·(IoT)
- í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤
- í•€í…Œí¬

## 3. ì‹ ì²­ìê²©
- ì‚¬ì—…ìë“±ë¡ì¦ì„ ë³´ìœ í•œ ì¤‘ì†Œê¸°ì—…
- ì°½ì—…ì¼ë¡œë¶€í„° 3ë…„ ì´ë‚´ ê¸°ì—…
- ìµœê·¼ 3ë…„ê°„ êµ­ì„¸ ë° ì§€ë°©ì„¸ ì™„ë‚© ê¸°ì—…
- ê³ ìš©ë³´í—˜ ê°€ì… ê¸°ì—…

## 4. ì‹ ì²­ë°©ë²•
- ì ‘ìˆ˜ê¸°ê°„: 2025.1.15 ~ 2025.2.15 (18:00ê¹Œì§€)
- ì ‘ìˆ˜ë°©ë²•: ì˜¨ë¼ì¸ ì ‘ìˆ˜ (www.smes.go.kr)
- ì œì¶œì„œë¥˜: ì‚¬ì—…ê³„íšì„œ, ì¬ë¬´ì œí‘œ, ì‚¬ì—…ìë“±ë¡ì¦ ë“±

## 5. ì„ ì •ì ˆì°¨
1ì°¨: ì„œë¥˜ì‹¬ì‚¬ (ì •ëŸ‰í‰ê°€)
2ì°¨: ë°œí‘œì‹¬ì‚¬ (ì •ì„±í‰ê°€)
3ì°¨: í˜„ì¥ì‹¤ì‚¬

## 6. ë¬¸ì˜ì²˜
- ë‹´ë‹¹ë¶€ì„œ: ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ë¶€ ì°½ì—…ì§€ì›ê³¼
- ë‹´ë‹¹ì: í™ê¸¸ë™ ê³¼ì¥
- ì—°ë½ì²˜: 02-1234-5678
- ì´ë©”ì¼: startup@smes.go.kr"""
        }
        
    async def __aenter__(self):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì‹œì‘"""
        connector = aiohttp.TCPConnector(limit=50, limit_per_host=20)
        timeout = aiohttp.ClientTimeout(total=self.timeout)
        self.session = aiohttp.ClientSession(
            connector=connector, 
            timeout=timeout,
            headers={'Content-Type': 'application/json'}
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        if self.session:
            await self.session.close()
    
    def _create_ollama_payload(self, content: str) -> Dict[str, Any]:
        """Ollama API ìš”ì²­ í˜ì´ë¡œë“œ ìƒì„±"""
        system_prompt = """ë‹¹ì‹ ì€ ì •ë¶€ ë° ê³µê³µê¸°ê´€ì˜ ê³µê³ ë¬¸ì„ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ ê³µê³  ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ì •ë³´ë¥¼ ì •í™•íˆ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

ì¶”ì¶œí•  ì •ë³´:
1. ì§€ì›ëŒ€ìƒ: ëˆ„ê°€ ì§€ì›í•  ìˆ˜ ìˆëŠ”ì§€
2. ì§€ì›ê¸ˆì•¡: ì§€ì›ë°›ì„ ìˆ˜ ìˆëŠ” ê¸ˆì•¡
3. ì œëª©: ê³µê³ ì˜ ì œëª©
4. ì§€ì›ì‚¬ì—… ì—¬ë¶€: ì¬ì •ì  ì§€ì›ì„ ì œê³µí•˜ëŠ”ì§€ ì—¬ë¶€

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”."""

        user_prompt = f"""ë‹¤ìŒ ê³µê³  ë‚´ìš©ì„ ë¶„ì„í•´ì£¼ì„¸ìš”:

{content}

=== ê³µê³  ë‚´ìš© ë ===

ìœ„ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•´ì£¼ì„¸ìš”."""

        return {
            "model": self.model,
            "prompt": user_prompt,
            "system": system_prompt,
            "stream": False,
            "options": {
                "top_p": 0.9,
                "num_predict": 2049,
            }
        }
    
    def _evaluate_response_quality(self, response_data: Dict[str, Any]) -> float:
        """ì‘ë‹µ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (0.0 - 1.0)"""
        if not response_data or 'response' not in response_data:
            return 0.0
        
        response_text = response_data.get('response', '')
        score = 0.0
        
        # 1. ì‘ë‹µ ê¸¸ì´ ì ì ˆì„± (0.2ì )
        if 50 <= len(response_text) <= 2000:
            score += 0.2
        
        # 2. JSON í˜•ì‹ ì—¬ë¶€ (0.3ì )
        try:
            parsed = json.loads(response_text)
            if isinstance(parsed, dict):
                score += 0.3
        except:
            pass
        
        # 3. í•„ìˆ˜ í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ (0.3ì )
        required_keywords = ['ì§€ì›', 'ëŒ€ìƒ', 'ê¸ˆì•¡', 'ì œëª©']
        keyword_count = sum(1 for keyword in required_keywords if keyword in response_text)
        score += (keyword_count / len(required_keywords)) * 0.3
        
        # 4. ì‘ë‹µ ì™„ë£Œì„± (0.2ì )
        if not response_data.get('done', True):
            score -= 0.2
        else:
            score += 0.2
            
        return min(1.0, max(0.0, score))
    
    async def single_request(self, 
                           content: str, 
                           request_id: str = "single") -> PerformanceMetric:
        """ë‹¨ì¼ ìš”ì²­ ì„±ëŠ¥ ì¸¡ì •"""
        start_time = time.time()
        
        try:
            payload = self._create_ollama_payload(content)
            
            async with self.session.post(
                f"{self.ollama_url}/generate",
                json=payload
            ) as response:
                end_time = time.time()
                response_time = end_time - start_time
                
                if response.status == 200:
                    response_data = await response.json()
                    quality_score = self._evaluate_response_quality(response_data)
                    
                    return PerformanceMetric(
                        request_id=request_id,
                        start_time=start_time,
                        end_time=end_time,
                        response_time=response_time,
                        success=True,
                        response_size=len(str(response_data)),
                        http_status=response.status,
                        content_length=len(content),
                        response_quality_score=quality_score
                    )
                else:
                    error_text = await response.text()
                    return PerformanceMetric(
                        request_id=request_id,
                        start_time=start_time,
                        end_time=end_time,
                        response_time=response_time,
                        success=False,
                        error_message=f"HTTP {response.status}: {error_text}",
                        http_status=response.status,
                        content_length=len(content)
                    )
                    
        except asyncio.TimeoutError:
            end_time = time.time()
            return PerformanceMetric(
                request_id=request_id,
                start_time=start_time,
                end_time=end_time,
                response_time=end_time - start_time,
                success=False,
                error_message="Timeout",
                content_length=len(content)
            )
        except Exception as e:
            end_time = time.time()
            return PerformanceMetric(
                request_id=request_id,
                start_time=start_time,
                end_time=end_time,
                response_time=end_time - start_time,
                success=False,
                error_message=str(e),
                content_length=len(content)
            )
    
    async def concurrent_requests(self, 
                                contents: List[str], 
                                batch_id: str = "batch") -> BatchMetric:
        """ë™ì‹œ ìš”ì²­ ì„±ëŠ¥ ì¸¡ì •"""
        batch_start_time = time.time()
        
        # ë¹„ë™ê¸° íƒœìŠ¤í¬ ìƒì„±
        tasks = []
        for i, content in enumerate(contents):
            request_id = f"{batch_id}_req_{i+1}"
            task = asyncio.create_task(self.single_request(content, request_id))
            tasks.append(task)
        
        # ëª¨ë“  ìš”ì²­ ì‹¤í–‰ ë° ì™„ë£Œ ëŒ€ê¸°
        results = await asyncio.gather(*tasks, return_exceptions=True)
        batch_end_time = time.time()
        
        # ê²°ê³¼ ì²˜ë¦¬
        individual_metrics = []
        first_response_time = None
        last_response_time = None
        
        for result in results:
            if isinstance(result, Exception):
                # ì˜ˆì™¸ ë°œìƒí•œ ìš”ì²­ ì²˜ë¦¬
                metric = PerformanceMetric(
                    request_id="error",
                    start_time=batch_start_time,
                    end_time=batch_end_time,
                    response_time=batch_end_time - batch_start_time,
                    success=False,
                    error_message=str(result)
                )
            else:
                metric = result
            
            individual_metrics.append(metric)
            
            # ì²« ë²ˆì§¸/ë§ˆì§€ë§‰ ì‘ë‹µ ì‹œê°„ ì¶”ì 
            if metric.success:
                if first_response_time is None or metric.end_time < first_response_time:
                    first_response_time = metric.end_time
                if last_response_time is None or metric.end_time > last_response_time:
                    last_response_time = metric.end_time
        
        # í†µê³„ ê³„ì‚°
        success_metrics = [m for m in individual_metrics if m.success]
        success_count = len(success_metrics)
        failure_count = len(individual_metrics) - success_count
        
        if success_metrics:
            response_times = [m.response_time for m in success_metrics]
            average_response_time = statistics.mean(response_times)
            median_response_time = statistics.median(response_times)
            std_dev_response_time = statistics.stdev(response_times) if len(response_times) > 1 else 0.0
        else:
            average_response_time = 0.0
            median_response_time = 0.0
            std_dev_response_time = 0.0
        
        total_time = batch_end_time - batch_start_time
        throughput_rps = len(contents) / total_time if total_time > 0 else 0.0
        
        return BatchMetric(
            batch_id=batch_id,
            concurrent_requests=len(contents),
            start_time=batch_start_time,
            end_time=batch_end_time,
            total_time=total_time,
            first_response_time=(first_response_time - batch_start_time) if first_response_time else 0.0,
            last_response_time=(last_response_time - batch_start_time) if last_response_time else 0.0,
            individual_metrics=individual_metrics,
            success_count=success_count,
            failure_count=failure_count,
            average_response_time=average_response_time,
            median_response_time=median_response_time,
            std_dev_response_time=std_dev_response_time,
            throughput_rps=throughput_rps
        )
    
    async def baseline_performance(self, iterations: int = 10) -> List[PerformanceMetric]:
        """ê¸°ì¤€ì„  ì„±ëŠ¥ ì¸¡ì • (ë‹¨ì¼ ìš”ì²­ ë°˜ë³µ)"""
        logger.info(f"ê¸°ì¤€ì„  ì„±ëŠ¥ ì¸¡ì • ì‹œì‘: {iterations}íšŒ ë°˜ë³µ")
        
        results = []
        for i in range(iterations):
            content = self.sample_texts["medium"]  # ì¤‘ê°„ ê¸¸ì´ í…ìŠ¤íŠ¸ ì‚¬ìš©
            metric = await self.single_request(content, f"baseline_{i+1}")
            results.append(metric)
            
            # ì§„í–‰ìƒí™© ì¶œë ¥
            if (i + 1) % 5 == 0:
                success_count = sum(1 for r in results if r.success)
                avg_time = statistics.mean([r.response_time for r in results if r.success])
                logger.info(f"ì§„í–‰: {i+1}/{iterations}, ì„±ê³µ: {success_count}, í‰ê· ì‘ë‹µ: {avg_time:.2f}ì´ˆ")
        
        return results
    
    def print_baseline_summary(self, results: List[PerformanceMetric]):
        """ê¸°ì¤€ì„  ì„±ëŠ¥ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        success_results = [r for r in results if r.success]
        
        if not success_results:
            print("âŒ ëª¨ë“  ìš”ì²­ ì‹¤íŒ¨")
            return
        
        response_times = [r.response_time for r in success_results]
        quality_scores = [r.response_quality_score for r in success_results]
        
        print(f"\nğŸ“Š ê¸°ì¤€ì„  ì„±ëŠ¥ ì¸¡ì • ê²°ê³¼ ({len(results)}íšŒ ìš”ì²­)")
        print(f"{'='*50}")
        print(f"ì„±ê³µë¥ : {len(success_results)}/{len(results)} ({len(success_results)/len(results)*100:.1f}%)")
        print(f"í‰ê·  ì‘ë‹µì‹œê°„: {statistics.mean(response_times):.2f}ì´ˆ")
        print(f"ì¤‘ê°„ê°’ ì‘ë‹µì‹œê°„: {statistics.median(response_times):.2f}ì´ˆ")
        print(f"í‘œì¤€í¸ì°¨: {statistics.stdev(response_times):.2f}ì´ˆ" if len(response_times) > 1 else "í‘œì¤€í¸ì°¨: N/A")
        print(f"ìµœì†Œ ì‘ë‹µì‹œê°„: {min(response_times):.2f}ì´ˆ")
        print(f"ìµœëŒ€ ì‘ë‹µì‹œê°„: {max(response_times):.2f}ì´ˆ")
        print(f"í‰ê·  í’ˆì§ˆì ìˆ˜: {statistics.mean(quality_scores):.3f}")
        
        # ì‹¤íŒ¨í•œ ìš”ì²­ ì •ë³´
        failed_results = [r for r in results if not r.success]
        if failed_results:
            print(f"\nâŒ ì‹¤íŒ¨í•œ ìš”ì²­ ({len(failed_results)}ê°œ):")
            for i, r in enumerate(failed_results[:3], 1):  # ì²˜ìŒ 3ê°œë§Œ í‘œì‹œ
                print(f"  {i}. {r.error_message}")
    
    def print_batch_summary(self, result: BatchMetric):
        """ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print(f"\nğŸ“Š ë™ì‹œ ìš”ì²­ ì„±ëŠ¥ ì¸¡ì • ê²°ê³¼")
        print(f"{'='*50}")
        print(f"ë™ì‹œ ìš”ì²­ ìˆ˜: {result.concurrent_requests}ê°œ")
        print(f"ì´ ì†Œìš”ì‹œê°„: {result.total_time:.2f}ì´ˆ")
        print(f"ì„±ê³µë¥ : {result.success_count}/{result.concurrent_requests} ({result.success_count/result.concurrent_requests*100:.1f}%)")
        print(f"ì²˜ë¦¬ëŸ‰: {result.throughput_rps:.2f} RPS")
        print(f"ì²« ë²ˆì§¸ ì‘ë‹µ: {result.first_response_time:.2f}ì´ˆ")
        print(f"ë§ˆì§€ë§‰ ì‘ë‹µ: {result.last_response_time:.2f}ì´ˆ")
        
        if result.success_count > 0:
            print(f"í‰ê·  ì‘ë‹µì‹œê°„: {result.average_response_time:.2f}ì´ˆ")
            print(f"ì¤‘ê°„ê°’ ì‘ë‹µì‹œê°„: {result.median_response_time:.2f}ì´ˆ")
            print(f"í‘œì¤€í¸ì°¨: {result.std_dev_response_time:.2f}ì´ˆ")
        
        # ì‹¤íŒ¨í•œ ìš”ì²­ ì •ë³´
        if result.failure_count > 0:
            print(f"\nâŒ ì‹¤íŒ¨í•œ ìš”ì²­ ({result.failure_count}ê°œ):")
            failed_metrics = [m for m in result.individual_metrics if not m.success]
            for i, m in enumerate(failed_metrics[:3], 1):  # ì²˜ìŒ 3ê°œë§Œ í‘œì‹œ
                print(f"  {i}. {m.request_id}: {m.error_message}")


async def main():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì˜ˆì œ"""
    async with OllamaPerformanceTester() as tester:
        print("ğŸ§ª Ollama ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        # 1. ê¸°ì¤€ì„  ì„±ëŠ¥ ì¸¡ì •
        print("\n1ï¸âƒ£ ê¸°ì¤€ì„  ì„±ëŠ¥ ì¸¡ì • (ë‹¨ì¼ ìš”ì²­ 5íšŒ)")
        baseline_results = await tester.baseline_performance(5)
        tester.print_baseline_summary(baseline_results)
        
        # 2. ë™ì‹œ ìš”ì²­ í…ŒìŠ¤íŠ¸
        print("\n2ï¸âƒ£ ë™ì‹œ ìš”ì²­ í…ŒìŠ¤íŠ¸ (2ê°œ)")
        contents = [tester.sample_texts["medium"], tester.sample_texts["short"]]
        batch_result = await tester.concurrent_requests(contents, "concurrent_2")
        tester.print_batch_summary(batch_result)


if __name__ == "__main__":
    asyncio.run(main())