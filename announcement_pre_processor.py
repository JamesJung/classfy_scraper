#!/usr/bin/env python3
"""
ê³µê³  ì‚¬ì „ ì²˜ë¦¬ í”„ë¡œê·¸ë¨

ì‚¬ìš©ë²•:
    python announcement_pre_processor.py -d [ë””ë ‰í† ë¦¬ëª…] --site-code [ì‚¬ì´íŠ¸ì½”ë“œ]

ì˜ˆì‹œ:
    python announcement_pre_processor.py -d scraped_data --site-code site001
    python announcement_pre_processor.py -d eminwon_data --site-code emw001
"""

import sys
import argparse
import json
import os
import re
import sys
import time
import unicodedata
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python pathì— ì¶”ê°€
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.config.config import ConfigManager

from src.config.logConfig import setup_logging

from src.models.announcementPrvDatabase import AnnouncementPrvDatabaseManager
from src.utils.domainKeyExtractor import DomainKeyExtractor

logger = setup_logging(__name__)

config = ConfigManager().get_config()


class AnnouncementPreProcessor:
    """ê³µê³  ì‚¬ì „ ì²˜ë¦¬ ë©”ì¸ í´ë˜ìŠ¤"""

    def __init__(
        self,
        site_type: str,
        attach_force: bool = False,
        site_code: str = None,
        lazy_init: bool = False,
    ):
        # lazy_init ì˜µì…˜ì´ Trueë©´ AttachmentProcessorë¥¼ ë‚˜ì¤‘ì— ì´ˆê¸°í™”
        self._lazy_init = lazy_init
        self._attachment_processor = None

        if not lazy_init:
            # AttachmentProcessorë¥¼ ì§€ì—° import
            from src.utils.attachmentProcessor import AttachmentProcessor

            self._attachment_processor = AttachmentProcessor()

        self.db_manager = AnnouncementPrvDatabaseManager()
        self.attach_force = attach_force
        self.site_type = site_type
        self.site_code = site_code  # site_codeë¥¼ ì¸ìŠ¤í„´ìŠ¤ ë³€ìˆ˜ë¡œ ì €ì¥

        # URL ì •ê·œí™”ë¥¼ ìœ„í•œ DomainKeyExtractor ì´ˆê¸°í™”
        # SQLAlchemy engineì—ì„œ DB ì—°ê²° ì •ë³´ ì¶”ì¶œ
        db_url = self.db_manager.engine.url
        db_config = {
            'host': db_url.host,
            'user': db_url.username,
            'password': db_url.password,
            'database': db_url.database,
            'port': db_url.port or 3306,
            'charset': 'utf8mb4'
        }
        self.url_key_extractor = DomainKeyExtractor(db_config=db_config)

        # ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸” ìƒì„± (ì—†ëŠ” ê²½ìš°)
        self._ensure_database_tables()

        # ì œì™¸ í‚¤ì›Œë“œ ë¡œë“œ
        self.exclusion_keywords = self._load_exclusion_keywords()

    @property
    def attachment_processor(self):
        """ì§€ì—° ì´ˆê¸°í™”ë¥¼ ìœ„í•œ property"""
        if self._lazy_init and self._attachment_processor is None:
            logger.info("ì§€ì—° ì´ˆê¸°í™”: AttachmentProcessor ìƒì„±")
            try:
                # ì§€ì—° import
                from src.utils.attachmentProcessor import AttachmentProcessor

                self._attachment_processor = AttachmentProcessor()
            except Exception as e:
                logger.error(f"AttachmentProcessor ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                # ì‹¤íŒ¨ ì‹œ None ë°˜í™˜í•˜ì—¬ í˜¸ì¶œìê°€ ì²˜ë¦¬í•˜ë„ë¡ í•¨
                return None
        return self._attachment_processor

    def _ensure_database_tables(self):
        """ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸”ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³  ìƒì„±í•©ë‹ˆë‹¤."""
        try:
            if self.db_manager.test_connection():
                # announcement_pre_processing í…Œì´ë¸” ìƒì„±
                from sqlalchemy import text

                with self.db_manager.SessionLocal() as session:
                    session.execute(
                        text(
                            """
                        CREATE TABLE IF NOT EXISTS announcement_pre_processing (
                            id INT PRIMARY KEY AUTO_INCREMENT,
                            folder_name VARCHAR(255) UNIQUE,
                            site_type VARCHAR(50),
                            site_code VARCHAR(50),
                            content_md LONGTEXT,
                            combined_content LONGTEXT,
                            attachment_filenames TEXT,
                            attachment_files_list JSON,
                            exclusion_keyword TEXT,
                            exclusion_reason TEXT,
                            title VARCHAR(500),
                            origin_url VARCHAR(1000),
                            scraping_url VARCHAR(1000),
                            announcement_date VARCHAR(50),
                            processing_status VARCHAR(50),
                            error_message TEXT,
                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
                            INDEX idx_site_code (site_code),
                            INDEX idx_processing_status (processing_status),
                            INDEX idx_origin_url (origin_url),
                            INDEX idx_scraping_url (scraping_url)
                        ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci
                    """
                        )
                    )
                    session.commit()
                logger.info("ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸” í™•ì¸/ìƒì„± ì™„ë£Œ")
            else:
                logger.warning(
                    "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨ - ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤ (DB ì €ì¥ ë¶ˆê°€)"
                )
        except Exception as e:
            logger.warning(
                f"ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e} - ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤ (DB ì €ì¥ ë¶ˆê°€)"
            )

    def _load_exclusion_keywords(self) -> List[Dict[str, Any]]:
        """ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì œì™¸ í‚¤ì›Œë“œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            from sqlalchemy import text

            with self.db_manager.SessionLocal() as session:
                result = session.execute(
                    text(
                        """
                    SELECT EXCLUSION_ID, KEYWORD, DESCRIPTION
                    FROM EXCLUSION_KEYWORDS
                    WHERE IS_ACTIVE = TRUE
                    ORDER BY EXCLUSION_ID
                """
                    )
                )

                keywords = []
                for row in result:
                    keywords.append(
                        {"id": row[0], "keyword": row[1], "description": row[2]}
                    )

                logger.info(f"ì œì™¸ í‚¤ì›Œë“œ ë¡œë“œ ì™„ë£Œ: {len(keywords)}ê°œ")
                return keywords

        except Exception as e:
            logger.warning(f"ì œì™¸ í‚¤ì›Œë“œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return []

    def process_directory(self, directory_path: Path, site_code: str) -> bool:
        """
        ë‹¨ì¼ ë””ë ‰í† ë¦¬ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.

        Args:
            directory_path: ì²˜ë¦¬í•  ë””ë ‰í† ë¦¬ ê²½ë¡œ
            site_code: ì‚¬ì´íŠ¸ ì½”ë“œ

        Returns:
            ì²˜ë¦¬ ì„±ê³µ ì—¬ë¶€
        """
        folder_name = directory_path.name
        return self.process_directory_with_custom_name(
            directory_path, site_code, folder_name
        )

    def _find_target_directories(
        self, base_dir: Path, site_code: str, force: bool = False
    ) -> List[Path]:
        """
        ì²˜ë¦¬í•  ëŒ€ìƒ ë””ë ‰í† ë¦¬ë“¤ì„ ì°¾ìŠµë‹ˆë‹¤.

        Args:
            base_dir: ê¸°ë³¸ ë””ë ‰í† ë¦¬
            site_code: ì‚¬ì´íŠ¸ ì½”ë“œ
            force: ì´ë¯¸ ì²˜ë¦¬ëœ í•­ëª©ë„ ë‹¤ì‹œ ì²˜ë¦¬í• ì§€ ì—¬ë¶€

        Returns:
            ì²˜ë¦¬ ëŒ€ìƒ ë””ë ‰í† ë¦¬ ëª©ë¡
        """
        site_dir = base_dir / site_code

        if not site_dir.exists():
            logger.error(f"ì‚¬ì´íŠ¸ ë””ë ‰í† ë¦¬ê°€ ì—†ìŒ: {site_dir}")
            return []

        target_directories = []

        # ëª¨ë“  í•˜ìœ„ ë””ë ‰í† ë¦¬ì—ì„œ content.md, JSON íŒŒì¼ ë˜ëŠ” attachments í´ë”ê°€ ìˆëŠ” ë””ë ‰í† ë¦¬ ì°¾ê¸°
        logger.info(f"ë””ë ‰í† ë¦¬ ê²€ìƒ‰ ì‹œì‘: {site_dir}")

        # bizInfo, smes24, kStartUpì€ í”Œë« êµ¬ì¡° (ì§ì ‘ í•˜ìœ„ ë””ë ‰í† ë¦¬ë§Œ ê²€ìƒ‰)
        if site_code in ["bizInfo", "smes24", "kStartUp"]:
            # ì§ì ‘ í•˜ìœ„ ë””ë ‰í† ë¦¬ë§Œ ê²€ìƒ‰ (ë” ë¹ ë¦„)
            # ëª¨ë“  API ì‚¬ì´íŠ¸ëŠ” content.mdê°€ ë°˜ë“œì‹œ ìˆì–´ì•¼ í•¨
            for root_path in site_dir.iterdir():
                if root_path.is_dir():
                    has_content_md = (root_path / "content.md").exists()

                    if has_content_md:
                        # content.mdê°€ ìˆëŠ” ë””ë ‰í† ë¦¬ë§Œ ì²˜ë¦¬
                        target_directories.append(root_path)
                        logger.debug(
                            f"ëŒ€ìƒ ë””ë ‰í† ë¦¬ ë°œê²¬: {root_path.relative_to(site_dir)}"
                        )
                    else:
                        # content.mdê°€ ì—†ëŠ” ë””ë ‰í† ë¦¬ëŠ” ê±´ë„ˆë›°ê¸°
                        logger.debug(
                            f"{site_code} ë””ë ‰í† ë¦¬ ê±´ë„ˆë›°ê¸° (content.md ì—†ìŒ): {root_path.relative_to(site_dir)}"
                        )
        else:
            # ë‹¤ë¥¸ ì‚¬ì´íŠ¸ëŠ” ì¬ê·€ì ìœ¼ë¡œ ê²€ìƒ‰ (ì¤‘ì²© êµ¬ì¡° ê°€ëŠ¥)
            for root_path in site_dir.rglob("*"):
                if root_path.is_dir():
                    # content.md íŒŒì¼ì´ ìˆê±°ë‚˜ attachments í´ë”ê°€ ìˆê±°ë‚˜ JSON íŒŒì¼ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ë§Œ ëŒ€ìƒìœ¼ë¡œ í•¨
                    has_content_md = (root_path / "content.md").exists()
                    has_json = bool(list(root_path.glob("*.json")))
                    # attachments í´ë” í™•ì¸ ìµœì í™”
                    attachments_dir = root_path / "attachments"
                    has_attachments = False
                    if attachments_dir.exists():
                        # ì²« ë²ˆì§¸ íŒŒì¼ë§Œ í™•ì¸ (ì „ì²´ ë””ë ‰í† ë¦¬ ìˆœíšŒ ë°©ì§€)
                        try:
                            next(attachments_dir.iterdir())
                            has_attachments = True
                        except StopIteration:
                            has_attachments = False

                    if has_content_md or has_attachments or has_json:
                        target_directories.append(root_path)
                        logger.debug(
                            f"ëŒ€ìƒ ë””ë ‰í† ë¦¬ ë°œê²¬: {root_path.relative_to(site_dir)}"
                        )

        # í´ë”ëª…ìœ¼ë¡œ ì •ë ¬
        target_directories = sorted(target_directories, key=self._natural_sort_key)

        logger.info(f"ë°œê²¬ëœ ì „ì²´ ë””ë ‰í† ë¦¬: {len(target_directories)}ê°œ")

        # ì²˜ìŒ ëª‡ ê°œ í´ë”ëª… ë¡œê¹…
        if target_directories:
            logger.info(f"ì²« 5ê°œ í´ë”: {[d.name for d in target_directories[:5]]}")

        # force ì˜µì…˜ì´ ì—†ì„ ë•Œë§Œ ì´ë¯¸ ì²˜ë¦¬ëœ í´ë” ì œì™¸
        if not force:
            processed_folders = set(self._get_processed_folders(site_code))

            filtered_directories = []
            for directory in target_directories:
                # ì‚¬ì´íŠ¸ ë””ë ‰í† ë¦¬ë¡œë¶€í„°ì˜ ìƒëŒ€ ê²½ë¡œë¥¼ í´ë”ëª…ìœ¼ë¡œ ì‚¬ìš©
                relative_path = directory.relative_to(site_dir)
                folder_name = self._normalize_korean_text(
                    str(relative_path).replace("/", "_")
                )

                if folder_name not in processed_folders:
                    filtered_directories.append(directory)
                else:
                    logger.debug(f"ì´ë¯¸ ì²˜ë¦¬ëœ í´ë” ê±´ë„ˆëœ€: {folder_name}")

            logger.info(f"ì „ì²´ ë°œê²¬ëœ ë””ë ‰í† ë¦¬: {len(target_directories)}ê°œ")
            logger.info(f"ì²˜ë¦¬ ëŒ€ìƒ ë””ë ‰í† ë¦¬: {len(filtered_directories)}ê°œ")
            logger.info(f"ì´ë¯¸ ì²˜ë¦¬ëœ í´ë”: {len(processed_folders)}ê°œ")

            return filtered_directories
        else:
            # force ì˜µì…˜ì´ ìˆìœ¼ë©´ ëª¨ë“  ë””ë ‰í† ë¦¬ ë°˜í™˜
            logger.info(
                f"--force ì˜µì…˜: ëª¨ë“  ë””ë ‰í† ë¦¬ ì²˜ë¦¬ ({len(target_directories)}ê°œ)"
            )
            return target_directories

    def _get_processed_folders(self, site_code: str) -> List[str]:
        """ì´ë¯¸ ì²˜ë¦¬ëœ í´ë” ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        try:
            from sqlalchemy import text

            with self.db_manager.SessionLocal() as session:
                result = session.execute(
                    text(
                        """
                    SELECT folder_name 
                    FROM announcement_pre_processing 
                    WHERE site_code = :site_code
                """
                    ),
                    {"site_code": site_code},
                )

                return [row[0] for row in result]

        except Exception as e:
            logger.error(f"ì²˜ë¦¬ëœ í´ë” ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []

    def process_site_directories(
        self, base_dir: Path, site_code: str, force: bool = False
    ) -> Dict[str, int]:
        """
        íŠ¹ì • ì‚¬ì´íŠ¸ì˜ ëª¨ë“  ë””ë ‰í† ë¦¬ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.

        Args:
            base_dir: ê¸°ë³¸ ë””ë ‰í† ë¦¬
            site_code: ì‚¬ì´íŠ¸ ì½”ë“œ
            force: ì´ë¯¸ ì²˜ë¦¬ëœ í•­ëª©ë„ ë‹¤ì‹œ ì²˜ë¦¬í• ì§€ ì—¬ë¶€

        Returns:
            ì²˜ë¦¬ ê²°ê³¼ í†µê³„
        """

        # ì²˜ë¦¬í•  ë””ë ‰í† ë¦¬ ëª©ë¡ ì°¾ê¸°
        target_directories = self._find_target_directories(base_dir, site_code, force)

        if not target_directories:
            logger.warning("ì²˜ë¦¬í•  ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {"total": 0, "success": 0, "failed": 0, "skipped": 0}

        total_count = len(target_directories)
        results = {"total": total_count, "success": 0, "failed": 0, "skipped": 0}
        site_dir = base_dir / site_code

        print(f"\n{'='*60}")
        print(
            f"ê³µê³  ì²˜ë¦¬ ì‹œì‘: {site_code} (Site Type: {self.site_type}) ({total_count}ê°œ í´ë”)"
        )
        print(f"{'='*60}")

        # ì‹œì‘ ì‹œê°„ ê¸°ë¡
        start_time = time.time()

        for i, directory in enumerate(target_directories, 1):
            try:
                # ê°œë³„ í•­ëª© ì‹œì‘ ì‹œê°„
                item_start_time = time.time()

                # ì‚¬ì´íŠ¸ ë””ë ‰í† ë¦¬ë¡œë¶€í„°ì˜ ìƒëŒ€ ê²½ë¡œë¥¼ í´ë”ëª…ìœ¼ë¡œ ì‚¬ìš©
                relative_path = directory.relative_to(site_dir)
                folder_name = self._normalize_korean_text(
                    str(relative_path).replace("/", "_")
                )

                progress_pct = (i / total_count) * 100
                print(f"\n[{i}/{total_count} : {progress_pct:.1f}%] {folder_name}")

                # ì´ë¯¸ ì²˜ë¦¬ëœ í•­ëª© í™•ì¸ (force ì˜µì…˜ì´ ì—†ì„ ë•Œë§Œ)
                if not force and self._is_already_processed(folder_name, site_code):
                    skip_elapsed = time.time() - item_start_time
                    print(f"  âœ“ ì´ë¯¸ ì²˜ë¦¬ë¨, ê±´ë„ˆëœ€ ({skip_elapsed:.1f}ì´ˆ)")
                    results["skipped"] += 1
                    continue
                elif force and self._is_already_processed(folder_name, site_code):
                    print("  ğŸ”„ ì´ë¯¸ ì²˜ë¦¬ë¨, --force ì˜µì…˜ìœ¼ë¡œ ì¬ì²˜ë¦¬")

                success = self.process_directory_with_custom_name(
                    directory, site_code, folder_name, force
                )

                # ê°œë³„ í•­ëª© ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
                item_elapsed = time.time() - item_start_time

                if success:
                    results["success"] += 1
                    print(f"  âœ“ ì²˜ë¦¬ ì™„ë£Œ ({item_elapsed:.1f}ì´ˆ)")
                else:
                    results["failed"] += 1
                    print(f"  âœ— ì²˜ë¦¬ ì‹¤íŒ¨ ({item_elapsed:.1f}ì´ˆ)")

            except Exception as e:
                error_elapsed = time.time() - item_start_time
                results["failed"] += 1
                print(f"  âœ— ì˜ˆì™¸ ë°œìƒ: {str(e)[:100]}... ({error_elapsed:.1f}ì´ˆ)")
                logger.error(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ({directory}): {e}")

        # ì¢…ë£Œ ì‹œê°„ ë° í†µê³„ ê³„ì‚°
        end_time = time.time()
        total_elapsed = end_time - start_time
        processed_count = results["success"] + results["failed"]

        print(f"\n{'='*60}")
        print(
            f"ì²˜ë¦¬ ì™„ë£Œ: {results['success']}/{total_count} ì„±ê³µ ({(results['success']/total_count)*100:.1f}%)"
        )
        print(f"ê±´ë„ˆëœ€: {results['skipped']}, ì‹¤íŒ¨: {results['failed']}")
        print(f"")
        print(f"ğŸ“Š ì²˜ë¦¬ ì‹œê°„ í†µê³„:")
        print(f"   ì´ ì†Œìš” ì‹œê°„: {total_elapsed:.1f}ì´ˆ ({total_elapsed/60:.1f}ë¶„)")

        if processed_count > 0:
            avg_time_per_item = total_elapsed / processed_count
            print(f"   ì²˜ë¦¬í•œ í•­ëª©ë‹¹ í‰ê·  ì‹œê°„: {avg_time_per_item:.1f}ì´ˆ")

        if results["success"] > 0:
            avg_time_per_success = total_elapsed / results["success"]
            print(f"   ì„±ê³µí•œ í•­ëª©ë‹¹ í‰ê·  ì‹œê°„: {avg_time_per_success:.1f}ì´ˆ")

        print(f"{'='*60}")

        logger.info(
            f"ì²˜ë¦¬ ì™„ë£Œ - ì „ì²´: {results['total']}, ì„±ê³µ: {results['success']}, ì‹¤íŒ¨: {results['failed']}, ê±´ë„ˆëœ€: {results['skipped']}"
        )

        return results

    def process_directory_with_custom_name(
        self,
        directory_path: Path,
        site_code: str,
        folder_name: str,
        force: bool = False,
    ) -> bool:
        """
        ì‚¬ìš©ì ì •ì˜ í´ë”ëª…ìœ¼ë¡œ ë””ë ‰í† ë¦¬ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.

        Args:
            directory_path: ì²˜ë¦¬í•  ë””ë ‰í† ë¦¬ ê²½ë¡œ
            site_code: ì‚¬ì´íŠ¸ ì½”ë“œ
            folder_name: ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•  í´ë”ëª…
            force: ì´ë¯¸ ì²˜ë¦¬ëœ í•­ëª©ë„ ë‹¤ì‹œ ì²˜ë¦¬í• ì§€ ì—¬ë¶€

        Returns:
            ì²˜ë¦¬ ì„±ê³µ ì—¬ë¶€
        """
        logger.info(f"ë””ë ‰í† ë¦¬ ì²˜ë¦¬ ì‹œì‘: {folder_name} (site_code: {site_code})")

        try:
            # 0. folder_name ì¤‘ë³µ ì²´í¬ (force ì˜µì…˜ì´ ì—†ì„ ë•Œë§Œ)
            if not force:
                if self._check_folder_name_exists(folder_name, site_code):
                    logger.info(f"ì´ë¯¸ ì²˜ë¦¬ëœ í´ë” ê±´ë„ˆëœ€: {folder_name}")
                    return True  # ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬ (ì´ë¯¸ ì²˜ë¦¬ë¨)

            # 1. ì œì™¸ í‚¤ì›Œë“œ ì²´í¬
            excluded_keywords = []
            excluded_keywords = self._check_exclusion_keywords(folder_name)

            # 2. íŠ¹ìˆ˜ ì‚¬ì´íŠ¸ ì²˜ë¦¬ (ëª¨ë‘ content.md ì½ê¸°)
            content_md = ""
            title = None
            origin_url = None
            scraping_url = None
            announcement_date = None

            if site_code in ["kStartUp", "bizInfo", "smes24"]:
                # kStartUp, bizInfo, smes24ëŠ” content.mdë¥¼ ì½ê³ , JSONì—ì„œ ë‚ ì§œ ì •ë³´ë§Œ ë³´ì™„
                content_md_path = directory_path / "content.md"
                if content_md_path.exists():
                    try:
                        with open(content_md_path, "r", encoding="utf-8") as f:
                            content_md = f.read()
                        logger.info(f"content.md ì½ê¸° ì™„ë£Œ: {len(content_md)} ë¬¸ì")

                        # DO_NOT_PROCESS í”Œë˜ê·¸ í™•ì¸ (êµ¬ ë°ì´í„° ê±´ë„ˆë›°ê¸°)
                        if "DO_NOT_PROCESS" in content_md:
                            logger.info(f"â­ï¸  ê±´ë„ˆëœ€ (ARCHIVED): {folder_name} - DO_NOT_PROCESS í”Œë˜ê·¸ ê°ì§€")
                            return False

                        # content.mdì—ì„œ ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
                        title = self._extract_title_from_content(content_md)
                        origin_url = self._extract_origin_url_from_content(content_md)
                        scraping_url = self._extract_scraping_url_from_content(content_md)

                        # JSON íŒŒì¼ì—ì„œ announcement_date ë³´ì™„ (ìš°ì„ ìˆœìœ„: announcement.json â†’ data.json â†’ ê¸°íƒ€)
                        priority_json_names = ["announcement.json", "data.json", "info.json"]
                        json_file_to_use = None

                        # ìš°ì„ ìˆœìœ„ íŒŒì¼ ë¨¼ì € í™•ì¸
                        for json_name in priority_json_names:
                            json_path = directory_path / json_name
                            if json_path.exists():
                                json_file_to_use = json_path
                                logger.debug(f"ìš°ì„ ìˆœìœ„ JSON íŒŒì¼ ë°œê²¬: {json_name}")
                                break

                        # ìš°ì„ ìˆœìœ„ íŒŒì¼ì´ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ JSON ì‚¬ìš©
                        if not json_file_to_use:
                            json_files = list(directory_path.glob("*.json"))
                            if json_files:
                                json_file_to_use = json_files[0]
                                logger.debug(f"ì¼ë°˜ JSON íŒŒì¼ ì‚¬ìš©: {json_file_to_use.name}")

                        if json_file_to_use:
                            try:
                                with open(json_file_to_use, "r", encoding="utf-8") as f:
                                    json_data = json.load(f)
                                announcement_date_raw = json_data.get(
                                    "announcementDate", ""
                                )
                                if announcement_date_raw:
                                    announcement_date = self._convert_to_yyyymmdd(
                                        announcement_date_raw
                                    )
                                else:
                                    # JSONì— ì—†ìœ¼ë©´ content.mdì—ì„œ ì¶”ì¶œ
                                    announcement_date_raw = self._extract_announcement_date_from_content(
                                        content_md
                                    )
                                    if announcement_date_raw:
                                        announcement_date = self._convert_to_yyyymmdd(announcement_date_raw)
                            except Exception as e:
                                logger.warning(
                                    f"{site_code} JSON ë‚ ì§œ ì¶”ì¶œ ì‹¤íŒ¨, content.md ì‚¬ìš©: {e}"
                                )
                                announcement_date_raw = self._extract_announcement_date_from_content(
                                    content_md
                                )
                                if announcement_date_raw:
                                    announcement_date = self._convert_to_yyyymmdd(announcement_date_raw)
                        else:
                            # JSON íŒŒì¼ì´ ì—†ìœ¼ë©´ content.mdì—ì„œ ì¶”ì¶œ
                            announcement_date_raw = self._extract_announcement_date_from_content(content_md)
                            if announcement_date_raw:
                                announcement_date = self._convert_to_yyyymmdd(announcement_date_raw)

                    except Exception as e:
                        logger.error(f"content.md ì½ê¸° ì‹¤íŒ¨: {e}")
                        return self._save_processing_result(
                            folder_name,
                            site_code,
                            content_md,
                            "",
                            url_key=None,
                            status="error",
                            error_message=f"content.md ì½ê¸° ì‹¤íŒ¨: {e}",
                        )
                else:
                    logger.warning(f"content.md íŒŒì¼ì´ ì—†ìŒ: {content_md_path}")

            else:
                # ì¼ë°˜ ì‚¬ì´íŠ¸ ì²˜ë¦¬ (ê¸°ì¡´ ë¡œì§)
                content_md_path = directory_path / "content.md"
                if content_md_path.exists():
                    try:
                        with open(content_md_path, "r", encoding="utf-8") as f:
                            content_md = f.read()
                        logger.info(f"content.md ì½ê¸° ì™„ë£Œ: {len(content_md)} ë¬¸ì")

                        # DO_NOT_PROCESS í”Œë˜ê·¸ í™•ì¸ (êµ¬ ë°ì´í„° ê±´ë„ˆë›°ê¸°)
                        if "DO_NOT_PROCESS" in content_md:
                            logger.info(f"â­ï¸  ê±´ë„ˆëœ€ (ARCHIVED): {folder_name} - DO_NOT_PROCESS í”Œë˜ê·¸ ê°ì§€")
                            return False
                    except Exception as e:
                        logger.error(f"content.md ì½ê¸° ì‹¤íŒ¨: {e}")
                        return self._save_processing_result(
                            folder_name,
                            site_code,
                            content_md,
                            "",
                            url_key=None,
                            status="error",
                            error_message=f"content.md ì½ê¸° ì‹¤íŒ¨: {e}",
                        )
                else:
                    logger.warning(f"content.md íŒŒì¼ì´ ì—†ìŒ: {content_md_path}")

            # 3. content.mdì—ì„œ ì •ë³´ ì¶”ì¶œ
            # ì¼ë°˜ ì‚¬ì´íŠ¸ì˜ ê²½ìš°ë§Œ content.mdì—ì„œ ì •ë³´ ì¶”ì¶œ (API ì‚¬ì´íŠ¸ëŠ” ì´ë¯¸ ì¶”ì¶œí•¨)
            if site_code not in ["kStartUp", "bizInfo", "smes24"]:
                title = self._extract_title_from_content(content_md)
                origin_url = self._extract_origin_url_from_content(content_md)
                announcement_date_raw = self._extract_announcement_date_from_content(content_md)
                if announcement_date_raw:
                    announcement_date = self._convert_to_yyyymmdd(announcement_date_raw)

            # 3.5. origin_urlì—ì„œ url_key ì¶”ì¶œ (URL ì •ê·œí™”)
            # ìš°ì„ ìˆœìœ„ 1: domain_key_config ì‚¬ìš©
            # ìš°ì„ ìˆœìœ„ 2: í´ë°± ì •ê·œí™” (ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì •ë ¬)
            url_key = None
            if origin_url:
                try:
                    # 1ìˆœìœ„: domain_key_configì—ì„œ ë„ë©”ì¸ ì„¤ì • ì¡°íšŒ
                    url_key = self.url_key_extractor.extract_url_key(origin_url, site_code)
                    if url_key:
                        logger.debug(f"âœ“ URL ì •ê·œí™” ì™„ë£Œ (domain_key_config ì‚¬ìš©): {origin_url[:80]}... â†’ {url_key}")
                    else:
                        # 2ìˆœìœ„: domain_key_configì— ë„ë©”ì¸ ì—†ìŒ â†’ í´ë°± ì •ê·œí™”
                        logger.warning(
                            f"âš ï¸  ë„ë©”ì¸ ì„¤ì • ì—†ìŒ (domain_key_config), í´ë°± ì •ê·œí™” ìˆ˜í–‰: {origin_url[:80]}..."
                        )
                        url_key = self._fallback_normalize_url(origin_url)
                        logger.info(f"âœ“ í´ë°± ì •ê·œí™” ì ìš©: {url_key}")
                except Exception as e:
                    logger.error(f"âŒ URL ì •ê·œí™” ì¤‘ ì˜¤ë¥˜: {e}")
                    # ì˜ˆì™¸ ë°œìƒ ì‹œì—ë„ í´ë°± ì •ê·œí™” ì‹œë„
                    if origin_url:
                        url_key = self._fallback_normalize_url(origin_url)
                        logger.info(f"âœ“ ì˜ˆì™¸ í›„ í´ë°± ì •ê·œí™”: {url_key}")
                    else:
                        logger.warning("origin_urlì´ ì—†ì–´ URL ì •ê·œí™” ë¶ˆê°€")
                        url_key = None

            # 4. ì²¨ë¶€íŒŒì¼ ì²˜ë¦¬ (content.mdì™€ ë¶„ë¦¬)
            combined_content = ""
            attachment_filenames = []
            attachment_files_info = []
            attachment_error = None

            try:
                combined_content, attachment_filenames, attachment_files_info = (
                    self._process_attachments_separately(directory_path)
                )
                logger.info(
                    f"ì²¨ë¶€íŒŒì¼ ë‚´ìš© ì²˜ë¦¬ ì™„ë£Œ: {len(combined_content)} ë¬¸ì, íŒŒì¼ {len(attachment_filenames)}ê°œ"
                )
            except Exception as e:
                # ì²¨ë¶€íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨ë¥¼ ê¸°ë¡í•˜ì§€ë§Œ ê³„ì† ì§„í–‰
                attachment_error = str(e)
                logger.error(f"ì²¨ë¶€íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ (ê³„ì† ì§„í–‰): {e}")
                # ë¹ˆ ê°’ìœ¼ë¡œ ì„¤ì •í•˜ê³  ê³„ì† ì§„í–‰
                combined_content = ""
                attachment_filenames = []
                attachment_files_info = []

            # content.mdì™€ combined_content ëª¨ë‘ ì—†ëŠ” ê²½ìš°ì—ë§Œ ì—ëŸ¬ ì²˜ë¦¬
            if not content_md.strip() and not combined_content.strip():
                logger.warning("ì²˜ë¦¬í•  ë‚´ìš©ì´ ì—†ìŒ")
                error_msg = "ì²˜ë¦¬í•  ë‚´ìš©ì´ ì—†ìŒ"
                if attachment_error:
                    error_msg += f" (ì²¨ë¶€íŒŒì¼ ì˜¤ë¥˜: {attachment_error})"
                return self._save_processing_result(
                    folder_name,
                    site_code,
                    content_md,
                    combined_content,
                    attachment_filenames=attachment_filenames,
                    attachment_files_info=attachment_files_info,
                    url_key=url_key,
                    status="error",
                    error_message=error_msg,
                )

            # 5. ì œì™¸ í‚¤ì›Œë“œê°€ ìˆëŠ” ê²½ìš° ì œì™¸ ì²˜ë¦¬
            if excluded_keywords:
                exclusion_msg = (
                    f"ì œì™¸ í‚¤ì›Œë“œê°€ ì…ë ¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤: {', '.join(excluded_keywords)}"
                )
                logger.info(f"ì œì™¸ ì²˜ë¦¬: {folder_name} - {exclusion_msg}")

                return self._save_processing_result(
                    folder_name,
                    site_code,
                    content_md,
                    combined_content,
                    attachment_filenames=attachment_filenames,
                    attachment_files_info=attachment_files_info,
                    status="ì œì™¸",
                    title=title,
                    announcement_date=announcement_date,
                    origin_url=origin_url,
                    url_key=url_key,
                    scraping_url=scraping_url,
                    exclusion_keywords=excluded_keywords,
                    exclusion_reason=exclusion_msg,
                )

            # 6. ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ (URL ì •ê·œí™” ì ìš©)
            record_id = self._save_processing_result(
                folder_name,
                site_code,
                content_md,
                combined_content,
                attachment_filenames=attachment_filenames,
                attachment_files_info=attachment_files_info,
                title=title,
                announcement_date=announcement_date,
                origin_url=origin_url,
                url_key=url_key,
                scraping_url=scraping_url,
                status="ì„±ê³µ",
                force=force,
            )

            if record_id:
                logger.info(f"ë””ë ‰í† ë¦¬ ì²˜ë¦¬ ì™„ë£Œ: {folder_name}")
                return True
            else:
                logger.error(f"ë””ë ‰í† ë¦¬ ì²˜ë¦¬ ì‹¤íŒ¨: {folder_name}")
                return False

        except Exception as e:
            logger.error(f"ë””ë ‰í† ë¦¬ ì²˜ë¦¬ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
            result = self._save_processing_result(
                folder_name,
                site_code,
                "",
                "",
                url_key=None,
                status="error",
                error_message=f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}",
            )
            return result is not None

    def _check_exclusion_keywords(self, folder_name: str) -> List[str]:
        """í´ë”ëª…ì—ì„œ ì œì™¸ í‚¤ì›Œë“œë¥¼ ì²´í¬í•©ë‹ˆë‹¤."""
        matched_keywords = []

        for keyword_info in self.exclusion_keywords:
            keyword = keyword_info["keyword"].lower()
            if keyword in folder_name.lower():
                matched_keywords.append(keyword_info["keyword"])
                logger.debug(f"ì œì™¸ í‚¤ì›Œë“œ ë§¤ì¹­: '{keyword}' in '{folder_name}'")

        return matched_keywords

    def _check_folder_name_exists(self, folder_name: str, site_code: str) -> bool:
        """folder_nameì´ ë°ì´í„°ë² ì´ìŠ¤ì— ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
        try:
            from sqlalchemy import text

            with self.db_manager.SessionLocal() as session:
                result = session.execute(
                    text(
                        """
                    SELECT COUNT(*) FROM announcement_pre_processing 
                    WHERE folder_name = :folder_name AND site_code = :site_code
                """
                    ),
                    {"folder_name": folder_name, "site_code": site_code},
                )

                count = result.scalar()
                exists = count > 0

                if exists:
                    logger.debug(f"folder_name ì¤‘ë³µ ë°œê²¬: {folder_name}")

                return exists

        except Exception as e:
            logger.error(f"folder_name ì¤‘ë³µ ì²´í¬ ì‹¤íŒ¨: {e}")
            return False

    def _check_origin_url_exists(self, origin_url: str, site_code: str) -> bool:
        """origin_urlì´ ë°ì´í„°ë² ì´ìŠ¤ì— ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
        try:
            from sqlalchemy import text

            with self.db_manager.SessionLocal() as session:
                result = session.execute(
                    text(
                        """
                    SELECT COUNT(*) FROM announcement_pre_processing 
                    WHERE origin_url = :origin_url AND site_code = :site_code
                """
                    ),
                    {"origin_url": origin_url, "site_code": site_code},
                )

                count = result.scalar()
                exists = count > 0

                if exists:
                    logger.debug(f"origin_url ì¤‘ë³µ ë°œê²¬: {origin_url}")

                return exists

        except Exception as e:
            logger.error(f"origin_url ì¤‘ë³µ ì²´í¬ ì‹¤íŒ¨: {e}")
            return False

    def _is_already_processed(self, folder_name: str, site_code: str) -> bool:
        """í´ë”ê°€ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
        return self._check_folder_name_exists(folder_name, site_code)

    def _extract_title_from_content(self, content_md: str) -> str:
        """content.mdì—ì„œ ì œëª©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        if not content_md:
            return ""

        lines = content_md.split("\n")

        # ì²« ë²ˆì§¸ ë¹„ì–´ìˆì§€ ì•Šì€ ì¤„ì„ ì°¾ê¸°
        for line in lines[:10]:  # ìƒìœ„ 10ì¤„ë§Œ í™•ì¸
            line = line.strip()
            if line:
                # # ë§ˆí¬ë‹¤ìš´ í—¤ë” ì œê±°
                if line.startswith("#"):
                    title = line.lstrip("#").strip()
                    logger.debug(f"ë§ˆí¬ë‹¤ìš´ í—¤ë”ì—ì„œ ì œëª© ì¶”ì¶œ: {title}")
                    return title

                # **ì œëª©**: íŒ¨í„´ í™•ì¸ (ë§ˆí¬ë‹¤ìš´ ë³¼ë“œ)
                if line.startswith("**ì œëª©**:"):
                    title = line.replace("**ì œëª©**:", "").strip()
                    logger.debug(f"**ì œëª©** íŒ¨í„´ì—ì„œ ì œëª© ì¶”ì¶œ: {title}")
                    return title

                # ì œëª©:, ê³µê³ ëª…: íŒ¨í„´ í™•ì¸
                for prefix in ["ì œëª©:", "ê³µê³ ëª…:", "ê³µê³  ì œëª©:", "ì œëª© :"]:
                    if line.lower().startswith(prefix.lower()):
                        title = line[len(prefix) :].strip()
                        logger.debug(f"{prefix} íŒ¨í„´ì—ì„œ ì œëª© ì¶”ì¶œ: {title}")
                        return title

                # ì¼ë°˜ í…ìŠ¤íŠ¸ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ì œëª©ìœ¼ë¡œ ì‚¬ìš© (ì²« ë²ˆì§¸ ì¤„)
                logger.debug(f"ì²« ë²ˆì§¸ ì¤„ì„ ì œëª©ìœ¼ë¡œ ì‚¬ìš©: {line}")
                return line

        return ""

    def _extract_origin_url_from_content(self, content_md: str) -> str:
        """content.mdì—ì„œ ì›ë³¸ URLì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        if not content_md:
            return ""

        # ì›ë³¸ URL íŒ¨í„´ ì°¾ê¸°
        origin_patterns = [
            r"\*\*ì›ë³¸ URL\*\*[:\s]*(.+?)(?:\n|$)",
            r"ì›ë³¸ URL[:\s]*(.+?)(?:\n|$)",
            r"ì›ë³¸[:\s]*(.+?)(?:\n|$)",
            r"(https?://[^\s\)]+(?:\.go\.kr|\.or\.kr)[^\s\)]*)",
        ]

        for pattern in origin_patterns:
            matches = re.findall(pattern, content_md, re.IGNORECASE)
            if matches:
                url = matches[0].strip()
                if url and url.startswith("http"):
                    logger.debug(f"ì›ë³¸ URL ì¶”ì¶œ ì„±ê³µ: {url[:50]}...")
                    return url

        logger.debug("content.mdì—ì„œ ì›ë³¸ URLì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        return ""

    def _extract_scraping_url_from_content(self, content_md: str) -> str:
        """content.mdì—ì„œ ìŠ¤í¬ë˜í•‘ URLì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        if not content_md:
            return ""

        # ìŠ¤í¬ë˜í•‘ URL íŒ¨í„´ ì°¾ê¸°
        scraping_patterns = [
            r"\*\*ìŠ¤í¬ë˜í•‘ URL\*\*[:\s]*(.+?)(?:\n|$)",
            r"ìŠ¤í¬ë˜í•‘ URL[:\s]*(.+?)(?:\n|$)",
            r"\*\*ìˆ˜ì§‘ URL\*\*[:\s]*(.+?)(?:\n|$)",
            r"ìˆ˜ì§‘ URL[:\s]*(.+?)(?:\n|$)",
        ]

        for pattern in scraping_patterns:
            matches = re.findall(pattern, content_md, re.IGNORECASE)
            if matches:
                url = matches[0].strip()
                if url and url.startswith("http"):
                    logger.debug(f"ìŠ¤í¬ë˜í•‘ URL ì¶”ì¶œ ì„±ê³µ: {url[:50]}...")
                    return url

        logger.debug("content.mdì—ì„œ ìŠ¤í¬ë˜í•‘ URLì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        return ""

    def _extract_announcement_date_from_content(self, content_md: str) -> str:
        """content.mdì—ì„œ ê³µê³ ì¼ì„ ë¬¸ìì—´ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        if not content_md:
            return ""

        # ì‘ì„±ì¼ íŒ¨í„´ ì°¾ê¸° (ë§ˆí¬ë‹¤ìš´ í˜•ì‹)
        # ì½œë¡ (:) ë’¤ì˜ ë‚ ì§œë§Œ ì •í™•íˆ ìº¡ì²˜
        date_patterns = [
            r"\*\*ì‘ì„±ì¼\*\*:\s*([^\n]+)",  # **ì‘ì„±ì¼**: ë‚ ì§œ
            r"\*\*ì‘ì„±ì¼\*\*:\*\*\s*([^\n]+)",  # **ì‘ì„±ì¼:**: ë‚ ì§œ
            r"ì‘ì„±ì¼:\s*([^\n]+)",  # ì‘ì„±ì¼: ë‚ ì§œ
            r"\*\*ë“±ë¡ì¼\*\*:\s*([^\n]+)",  # **ë“±ë¡ì¼**: ë‚ ì§œ
            r"\*\*ë“±ë¡ì¼\*\*:\*\*\s*([^\n]+)",  # **ë“±ë¡ì¼:**: ë‚ ì§œ
            r"ë“±ë¡ì¼:\s*([^\n]+)",  # ë“±ë¡ì¼: ë‚ ì§œ
            r"\*\*ê³µê³ ì¼\*\*:\s*([^\n]+)",  # **ê³µê³ ì¼**: ë‚ ì§œ
            r"\*\*ê³µê³ ì¼\*\*:\*\*\s*([^\n]+)",  # **ê³µê³ ì¼:**: ë‚ ì§œ
            r"ê³µê³ ì¼:\s*([^\n]+)",  # ê³µê³ ì¼: ë‚ ì§œ
        ]

        for pattern in date_patterns:
            matches = re.findall(pattern, content_md, re.IGNORECASE)
            if matches:
                date_str = matches[0].strip()
                if date_str:
                    # ë§ˆí¬ë‹¤ìš´ ë³¼ë“œ(**) ì œê±°
                    date_str = re.sub(r"\*+", "", date_str).strip()
                    # ì¶”ê°€ì ì¸ ì •ë¦¬ (ê³µë°± ì œê±° ë“±)
                    date_str = date_str.strip()

                    # ë‚ ì§œ í˜•ì‹ ê²€ì¦ (ìµœì†Œí•œ ì—°ë„ê°€ í¬í•¨ë˜ì–´ì•¼ í•¨)
                    if re.search(r"\d{4}", date_str):
                        logger.debug(f"ê³µê³ ì¼ ì¶”ì¶œ ì„±ê³µ: {date_str}")
                        return date_str

        logger.debug("content.mdì—ì„œ ê³µê³ ì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        return ""

    def _extract_attachment_urls_from_content(
        self, directory_path: Path
    ) -> Dict[str, str]:
        """content.mdì—ì„œ ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ URLì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        content_md_path = directory_path / "content.md"
        attachment_urls = {}

        if not content_md_path.exists():
            return attachment_urls

        try:
            with open(content_md_path, "r", encoding="utf-8") as f:
                content = f.read()

            # **ì²¨ë¶€íŒŒì¼**: ì„¹ì…˜ ì°¾ê¸°
            # ìˆ˜ì •: ëª¨ë“  ì²¨ë¶€íŒŒì¼ ë¼ì¸ì„ ì¶”ì¶œí•˜ë„ë¡ íŒ¨í„´ ê°œì„ 
            attachments_section = re.search(
                r"\*\*ì²¨ë¶€íŒŒì¼\*\*:\s*\n+((?:.*\n?)*?)(?=\n\*\*|\Z)",
                content,
                re.MULTILINE,
            )

            if attachments_section:
                attachments_text = attachments_section.group(1)

                # ëª¨ë“  ì¤„ì„ ì²˜ë¦¬í•˜ì—¬ ë²ˆí˜¸. íŒŒì¼ëª…:URL íŒ¨í„´ ì°¾ê¸°
                lines = attachments_text.strip().split("\n")
                for line in lines:
                    line = line.strip()
                    if not line:
                        continue

                    # ë²ˆí˜¸. íŒŒì¼ëª…:URL íŒ¨í„´ (ì½œë¡  ì•ë’¤ ê³µë°± í—ˆìš©)
                    match = re.match(r"^\d+\.\s*(.+?)\s*:\s*(https?://\S+)", line)
                    if match:
                        filename = match.group(1).strip()
                        url = match.group(2).strip()
                        attachment_urls[filename] = url
                        logger.debug(f"ì²¨ë¶€íŒŒì¼ URL ë§¤í•‘: {filename} -> {url[:50]}...")

            logger.info(
                f"ì²¨ë¶€íŒŒì¼ URL ì¶”ì¶œ ì™„ë£Œ: {len(attachment_urls)}ê°œ, í‚¤: {list(attachment_urls.keys())}"
            )

        except Exception as e:
            logger.error(f"ì²¨ë¶€íŒŒì¼ URL ì¶”ì¶œ ì‹¤íŒ¨: {e}")

        return attachment_urls

    def _normalize_korean_text(self, text: str) -> str:
        """í•œê¸€ í…ìŠ¤íŠ¸ë¥¼ NFC(Composed) í˜•íƒœë¡œ ì •ê·œí™”í•©ë‹ˆë‹¤."""
        return unicodedata.normalize("NFC", text)

    def _natural_sort_key(self, path: Path) -> tuple:
        """í´ë”ëª…ì˜ ìˆ«ì ë¶€ë¶„ì„ ê¸°ì¤€ìœ¼ë¡œ ìì—° ì •ë ¬ì„ ìœ„í•œ í‚¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        import re

        folder_name = path.name
        # ìˆ«ì_ì œëª© íŒ¨í„´ì—ì„œ ìˆ«ì ë¶€ë¶„ ì¶”ì¶œ
        match = re.match(r"^(\d+)_(.*)$", folder_name)
        if match:
            number = int(match.group(1))
            title = match.group(2)
            return (number, title)
        else:
            # ìˆ«ìë¡œ ì‹œì‘í•˜ì§€ ì•ŠëŠ” ê²½ìš°ëŠ” ë§¨ ë’¤ë¡œ
            return (float("inf"), folder_name)

    def _process_attachments_separately(
        self, directory_path: Path
    ) -> tuple[str, List[str], List[Dict[str, Any]]]:
        """ì²¨ë¶€íŒŒì¼ë“¤ì„ ì²˜ë¦¬í•˜ì—¬ ë‚´ìš©ì„ ê²°í•©í•˜ê³  íŒŒì¼ëª… ëª©ë¡ì„ ë°˜íœ˜í•©ë‹ˆë‹¤."""
        attachments_dir = directory_path / "attachments"

        if not attachments_dir.exists():
            return "", [], []

        combined_content = ""
        attachment_filenames = []
        attachment_files_info = []

        # content.mdì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ URL ì¶”ì¶œ
        attachment_urls = self._extract_attachment_urls_from_content(directory_path)

        # ì²˜ë¦¬ ê°€ëŠ¥í•œ í™•ì¥ì ì •ì˜ (Excel íŒŒì¼ ì œì™¸)
        supported_extensions = {
            ".pdf",
            ".hwp",
            ".hwpx",
            ".png",
            ".jpg",
            ".jpeg",
            ".gif",
            ".bmp",
            ".webp",
            ".pptx",
            ".docx",
            ".md",
            ".zip",  # ZIP íŒŒì¼ ì§€ì› ì¶”ê°€
        }

        target_keywords = ["ì–‘ì‹", "ì„œë¥˜", "ì‹ ì²­ì„œ", "ë™ì˜ì„œ"]

        # íŒŒì¼ë“¤ì„ ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ë¶„ë¥˜
        priority_files = []  # ì§€ì›/ê³µê³  í‚¤ì›Œë“œê°€ ìˆëŠ” íŒŒì¼ë“¤
        normal_files = []  # ì¼ë°˜ íŒŒì¼ë“¤

        # ëª¨ë“  íŒŒì¼ì„ ë¨¼ì € ê²€ì‚¬í•˜ì—¬ ë¶„ë¥˜
        for file_path in attachments_dir.iterdir():
            if file_path.is_file():
                file_extension = file_path.suffix.lower()
                filename = file_path.stem
                lowercase_filename = filename.lower()

                # ì§€ì›í•˜ëŠ” í™•ì¥ìë§Œ ì²˜ë¦¬
                if file_extension and file_extension in supported_extensions:
                    # ì–‘ì‹, ì„œë¥˜ ë“± ì œì™¸ í‚¤ì›Œë“œ ì²´í¬
                    if any(
                        keyword in lowercase_filename for keyword in target_keywords
                    ):
                        continue

                    # ì§€ì›/ê³µê³  í‚¤ì›Œë“œê°€ ìˆëŠ”ì§€ í™•ì¸
                    if "ì§€ì›" in lowercase_filename or "ê³µê³ " in lowercase_filename:
                        priority_files.append(file_path)
                        logger.info(
                            f"ìš°ì„ ìˆœìœ„ íŒŒì¼ ë°œê²¬ (ì§€ì›/ê³µê³  í‚¤ì›Œë“œ): {file_path.name}"
                        )
                    else:
                        normal_files.append(file_path)

        # ìš°ì„ ìˆœìœ„ íŒŒì¼ë“¤ì„ ë¨¼ì € ì²˜ë¦¬, ê·¸ ë‹¤ìŒ ì¼ë°˜ íŒŒì¼ë“¤ ì²˜ë¦¬
        all_files_ordered = priority_files + normal_files

        for file_path in all_files_ordered:
            # ì´ë¯¸ ìœ„ì—ì„œ í•„í„°ë§ í–ˆìœ¼ë¯€ë¡œ ë°”ë¡œ ì²˜ë¦¬
            file_extension = file_path.suffix.lower()
            filename = file_path.stem

            logger.info(f"filename===={filename}{file_extension}")

            attachment_filenames.append(self._normalize_korean_text(file_path.name))
            logger.info(f"ì²¨ë¶€íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: {file_path.name}")

            # íŒŒì¼ ì •ë³´ ìˆ˜ì§‘ (ëª¨ë“  íŒŒì¼ì— ëŒ€í•´)
            # URL ë§¤ì¹­ ì‹œë„ - íŒŒì¼ëª…ìœ¼ë¡œ ë¨¼ì € ì‹œë„, ì—†ìœ¼ë©´ stemìœ¼ë¡œ ì‹œë„
            download_url = attachment_urls.get(file_path.name, "")
            if not download_url:
                # í™•ì¥ì ì—†ëŠ” ì´ë¦„ìœ¼ë¡œë„ ì‹œë„
                download_url = attachment_urls.get(file_path.stem, "")

            if download_url:
                logger.debug(
                    f"URL ë§¤ì¹­ ì„±ê³µ: {file_path.name} -> {download_url[:50]}..."
                )
            else:
                logger.debug(
                    f"URL ë§¤ì¹­ ì‹¤íŒ¨: {file_path.name}, ê°€ëŠ¥í•œ í‚¤: {list(attachment_urls.keys())[:3]}"
                )

            file_info = {
                "filename": file_path.name,  # í™•ì¥ì í¬í•¨ëœ ì „ì²´ íŒŒì¼ëª…
                "file_size": (file_path.stat().st_size if file_path.exists() else 0),
                "conversion_success": False,
                "conversion_method": self._guess_conversion_method(file_extension),
                "download_url": download_url,  # ë‹¤ìš´ë¡œë“œ URL ì¶”ê°€
            }

            # md íŒŒì¼ì´ ì•„ë‹Œ ê²½ìš°ë§Œ attachment_files_infoì— ì¶”ê°€
            if file_extension != ".md":
                attachment_files_info.append(file_info)

            # ì´ë¯¸ .md íŒŒì¼ì¸ ê²½ìš° ì§ì ‘ ì½ê¸°
            if file_extension == ".md":
                try:
                    with open(file_path, "r", encoding="utf-8") as f:
                        content = f.read()
                    if content.strip():
                        combined_content += f"\n\n=== {self._normalize_korean_text(file_path.name)} ===\n{content}"
                        logger.info(
                            f"ì²¨ë¶€íŒŒì¼ .md ì§ì ‘ ì½ê¸° ì„±ê³µ: {file_path.name} ({len(content)} ë¬¸ì)"
                        )
                        file_info["conversion_success"] = True
                    else:
                        logger.warning(
                            f"ì²¨ë¶€íŒŒì¼ .md ë‚´ìš©ì´ ë¹„ì–´ìˆìŒ: {file_path.name}"
                        )
                except Exception as e:
                    logger.error(f"ì²¨ë¶€íŒŒì¼ .md ì§ì ‘ ì½ê¸° ì‹¤íŒ¨: {e}")
                continue

            # ì²¨ë¶€íŒŒì¼ëª….md íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
            md_file_path = attachments_dir / f"{filename}.md"
            logger.debug(f"md_file_path: {md_file_path}")

            # attach_forceê°€ Trueì´ë©´ ê¸°ì¡´ .md íŒŒì¼ì„ ë¬´ì‹œí•˜ê³  ì›ë³¸ì—ì„œ ì¬ë³€í™˜
            if not self.attach_force and md_file_path.exists():
                # .md íŒŒì¼ì´ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ì½ìŒ
                try:
                    with open(md_file_path, "r", encoding="utf-8") as f:
                        content = f.read()
                    if content.strip():
                        combined_content += f"\n\n=== {self._normalize_korean_text(filename)}.md ===\n{content}"
                        logger.debug(
                            f"ì²¨ë¶€íŒŒì¼ .md ì½ê¸° ì„±ê³µ: {filename}.md ({len(content)} ë¬¸ì)"
                        )
                        file_info["conversion_success"] = True
                    else:
                        logger.warning(f"ì²¨ë¶€íŒŒì¼ .md ë‚´ìš©ì´ ë¹„ì–´ìˆìŒ: {filename}.md")
                except Exception as e:
                    logger.error(f"ì²¨ë¶€íŒŒì¼ .md ì½ê¸° ì‹¤íŒ¨: {e}")
            else:
                # .md íŒŒì¼ì´ ì—†ê±°ë‚˜ attach_forceê°€ Trueì´ë©´ ì›ë³¸ íŒŒì¼ì„ ë³€í™˜
                if self.attach_force and md_file_path.exists():
                    logger.info(
                        f"--attach-force: ê¸°ì¡´ .md íŒŒì¼ ë¬´ì‹œí•˜ê³  ì¬ë³€í™˜: {file_path.name}"
                    )
                else:
                    logger.info(f"ì²¨ë¶€íŒŒì¼ ë³€í™˜ ì‹œì‘: {file_path.name}")

                try:
                    # attachment_processorê°€ Noneì¸ ê²½ìš° ì²˜ë¦¬
                    if self.attachment_processor is None:
                        logger.warning(
                            f"AttachmentProcessorë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ íŒŒì¼ ê±´ë„ˆëœ€: {file_path.name}"
                        )
                        continue

                    content = self.attachment_processor.process_single_file(file_path)

                    if content and content.strip():
                        combined_content += f"\n\n=== {self._normalize_korean_text(file_path.name)} ===\n{content}"
                        logger.info(
                            f"ì²¨ë¶€íŒŒì¼ ë³€í™˜ ì„±ê³µ: {file_path.name} ({len(content)} ë¬¸ì)"
                        )
                        file_info["conversion_success"] = True

                        # ë³€í™˜ëœ ë‚´ìš©ì„ .md íŒŒì¼ë¡œ ì €ì¥
                        try:
                            with open(md_file_path, "w", encoding="utf-8") as f:
                                f.write(content)
                            logger.debug(f"ë³€í™˜ëœ ë‚´ìš©ì„ .mdë¡œ ì €ì¥: {md_file_path}")
                        except Exception as save_e:
                            logger.warning(f".md íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {save_e}")
                    else:
                        logger.warning(f"ì²¨ë¶€íŒŒì¼ì—ì„œ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨: {file_path.name}")

                except Exception as e:
                    error_msg = str(e)
                    if (
                        "Invalid code point" in error_msg
                        or "PDFSyntaxError" in error_msg
                        or "No /Root object" in error_msg
                    ):
                        logger.warning(f"ì†ìƒëœ PDF íŒŒì¼ ê±´ë„ˆë›°ê¸°: {file_path.name}")
                    elif "UnicodeDecodeError" in error_msg:
                        logger.warning(f"ì¸ì½”ë”© ë¬¸ì œë¡œ íŒŒì¼ ê±´ë„ˆë›°ê¸°: {file_path.name}")
                    else:
                        logger.error(f"ì²¨ë¶€íŒŒì¼ ë³€í™˜ ì‹¤íŒ¨ ({file_path.name}): {e}")

                    # ë³€í™˜ ì‹¤íŒ¨í•œ íŒŒì¼ ì •ë³´ ê¸°ë¡
                    file_info["conversion_success"] = False
                    file_info["error_message"] = error_msg[
                        :200
                    ]  # ì˜¤ë¥˜ ë©”ì‹œì§€ ì¼ë¶€ë§Œ ì €ì¥

        logger.info(
            f"ì²¨ë¶€íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ: {len(attachment_filenames)}ê°œ íŒŒì¼, {len(combined_content)} ë¬¸ì"
        )
        return combined_content.strip(), attachment_filenames, attachment_files_info

    def _guess_conversion_method(self, file_extension: str) -> str:
        """íŒŒì¼ í™•ì¥ìì— ë”°ë¥¸ ë³€í™˜ ë°©ë²•ì„ ì¶”ì •í•©ë‹ˆë‹¤."""
        ext_lower = file_extension.lower()

        if ext_lower == ".pdf":
            return "pdf_docling"
        elif ext_lower in [".hwp", ".hwpx"]:
            return "hwp_markdown"
        elif ext_lower in [".png", ".jpg", ".jpeg", ".gif", ".bmp", ".webp"]:
            return "ocr"
        else:
            return "unknown"

    def _convert_to_yyyymmdd(self, date_str: str) -> str:
        """ë‚ ì§œ ë¬¸ìì—´ì„ YYYYMMDD í¬ë§·ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        try:
            # ë‹¤ì–‘í•œ ë‚ ì§œ í¬ë§· ì‹œë„
            from datetime import datetime

            # ê°€ëŠ¥í•œ ë‚ ì§œ í¬ë§·ë“¤
            date_formats = [
                "%Y-%m-%d",
                "%Y.%m.%d",
                "%Y/%m/%d",
                "%Y%m%d",
                "%Yë…„ %mì›” %dì¼",
                "%Y-%m-%d %H:%M:%S",
                "%Y.%m.%d %H:%M:%S",
            ]

            for fmt in date_formats:
                try:
                    dt = datetime.strptime(date_str.strip(), fmt)
                    return dt.strftime("%Y%m%d")  # YYYYMMDD í˜•ì‹
                except ValueError:
                    continue

            # ëª¨ë“  í¬ë§· ì‹¤íŒ¨ì‹œ ì›ë³¸ ë°˜í™˜
            logger.warning(f"ë‚ ì§œ ë³€í™˜ ì‹¤íŒ¨, ì›ë³¸ ë°˜í™˜: {date_str}")
            return date_str

        except Exception as e:
            logger.error(f"ë‚ ì§œ ë³€í™˜ ì¤‘ ì˜¤ë¥˜: {e}")
            return date_str

    def _fallback_normalize_url(self, url: str | None) -> str | None:
        """
        ë„ë©”ì¸ ì„¤ì •ì´ ì—†ì„ ë•Œ ìµœì†Œí•œì˜ URL ì •ê·œí™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.

        âš ï¸ ì£¼ì˜: domain_key_configì— ë„ë©”ì¸ì´ ìˆìœ¼ë©´ ì´ ë©”ì„œë“œëŠ” ì‚¬ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
        domain_key_configê°€ ìš°ì„ ìˆœìœ„ 1ì´ê³ , ì´ê²ƒì€ í´ë°±(fallback)ì…ë‹ˆë‹¤.

        âš ï¸ ì¤‘ìš”: domain_key_configì™€ DomainKeyExtractorì™€ ë™ì¼í•˜ê²Œ ì•ŒíŒŒë²³ ìˆœìœ¼ë¡œ ì •ë ¬í•©ë‹ˆë‹¤!
        URL íŒŒë¼ë¯¸í„° ìˆœì„œì™€ ë¬´ê´€í•˜ê²Œ ë™ì¼í•œ í‚¤ë¥¼ ìƒì„±í•˜ì—¬ ì¤‘ë³µ ê°ì§€ ì •í™•ë„ë¥¼ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

        âš ï¸ í˜ì´ì§€ë„¤ì´ì…˜/ê²€ìƒ‰ íŒŒë¼ë¯¸í„° ìë™ ì œì™¸: page, pageIndex, searchCnd ë“±ì€ url_keyì—ì„œ ì œì™¸ë©ë‹ˆë‹¤.

        ë™ì‘:
        1. URLì„ íŒŒì‹±í•˜ì—¬ ë„ë©”ì¸ê³¼ ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì¶”ì¶œ
        2. í˜ì´ì§€ë„¤ì´ì…˜/ê²€ìƒ‰ íŒŒë¼ë¯¸í„° ì œì™¸
        3. ë‚¨ì€ íŒŒë¼ë¯¸í„°ë¥¼ **ì•ŒíŒŒë²³ ìˆœìœ¼ë¡œ ì •ë ¬**
        4. "domain|key1=val1&key2=val2" í˜•ì‹ìœ¼ë¡œ ë°˜í™˜ (ì •ë ¬ëœ ìˆœì„œ)

        Args:
            url: ì›ë³¸ URL (None ê°€ëŠ¥)

        Returns:
            ì •ê·œí™”ëœ URL í‚¤ ë˜ëŠ” None

        Examples:
            >>> _fallback_normalize_url("https://example.com?b=2&a=1")
            'example.com|a=1&b=2'  # â† ì•ŒíŒŒë²³ ì •ë ¬ë¨

            >>> _fallback_normalize_url("https://example.com?nttId=123&page=1")
            'example.com|nttId=123'  # â† page ì œì™¸ë¨

            >>> _fallback_normalize_url("https://example.com/path?id=1")
            'example.com|id=1'

            >>> _fallback_normalize_url(None)
            None
        """
        if not url:
            return None

        try:
            from urllib.parse import urlparse, parse_qsl

            # ì œì™¸í•  í˜ì´ì§€ë„¤ì´ì…˜/ê²€ìƒ‰/ì •ë ¬ íŒŒë¼ë¯¸í„° ëª©ë¡
            EXCLUDED_PARAMS = {
                # í˜ì´ì§€ë„¤ì´ì…˜ (ê¸°ì¡´)
                'page', 'pageNo', 'pageNum', 'pageIndex', 'pageSize', 'pageUnit',
                'offset', 'limit', 'start', 'Start', 'end',
                'currentPage', 'curPage', 'pageNumber', 'pn',
                'ofr_pageSize',

                # í˜ì´ì§€ë„¤ì´ì…˜ (Phase 10 ì¶”ê°€ - ëˆ„ë½ëœ ë³€í˜•)
                'homepage_pbs_yn',    # 16,095ê°œ
                'cpage',              # 2,497ê°œ
                'startPage',          # 1,348ê°œ
                'q_currPage',         # 728ê°œ
                'pageLine',           # 438ê°œ
                'pageCd',             # 390ê°œ
                'recordCountPerPage', # 227ê°œ
                'pageId',             # 205ê°œ
                'page_id',            # 196ê°œ
                'pageid',             # 196ê°œ
                'GotoPage',           # 149ê°œ
                'q_rowPerPage',       # 51ê°œ

                # ê²€ìƒ‰ ê´€ë ¨
                'search', 'searchWord', 'searchType', 'searchCategory',
                'searchCnd', 'searchKrwd', 'searchGosiSe', 'search_type',
                'keyword', 'query', 'q',

                # Phase 15 ì¶”ê°€: ê²Œì‹œíŒ ê²€ìƒ‰/ì¹´í…Œê³ ë¦¬ íŒŒë¼ë¯¸í„°
                'searchCtgry',        # ê²€ìƒ‰ ì¹´í…Œê³ ë¦¬ (ì›ì£¼, ë³´ì€, ì˜ì›”, íƒœë°± ë“±)
                'integrDeptCode',     # í†µí•© ë¶€ì„œ ì½”ë“œ (ì›ì£¼, ë³´ì€, ì˜ì›” ë“±)
                'searchCnd2',         # ê²€ìƒ‰ ì¡°ê±´ 2 (ì„œê·€í¬)
                'depNm',              # ë¶€ì„œëª… (ì„œê·€í¬)

                # ì •ë ¬ ê´€ë ¨
                'sort', 'order', 'orderBy', 'sortField', 'sortOrder',
                # ë·° ëª¨ë“œ
                'view', 'viewMode', 'display', 'listType',
            }

            parsed = urlparse(url)
            domain = parsed.netloc

            if not domain:
                logger.warning(f"ë„ë©”ì¸ ì¶”ì¶œ ì‹¤íŒ¨, ì›ë³¸ URL ë°˜í™˜: {url}")
                return url

            # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° íŒŒì‹± (ë¹ˆ ê°’ë„ í¬í•¨)
            params = parse_qsl(parsed.query, keep_blank_values=True)

            if params:
                # âœ… í˜ì´ì§€ë„¤ì´ì…˜/ê²€ìƒ‰ íŒŒë¼ë¯¸í„° ì œì™¸
                filtered_params = [(k, v) for k, v in params if k not in EXCLUDED_PARAMS]

                if filtered_params:
                    # âœ… ì•ŒíŒŒë²³ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ íŒŒë¼ë¯¸í„° ìˆœì„œ ë¬´ê´€í•˜ê²Œ ë™ì¼í•œ í‚¤ ìƒì„±
                    # domain_key_configì™€ DomainKeyExtractorë„ ë™ì¼í•˜ê²Œ ì•ŒíŒŒë²³ ì •ë ¬ ì‚¬ìš©
                    sorted_params = sorted(filtered_params)
                    param_str = "&".join(f"{k}={v}" for k, v in sorted_params)
                    normalized_key = f"{domain}|{param_str}"

                    # ì œì™¸ëœ íŒŒë¼ë¯¸í„°ê°€ ìˆìœ¼ë©´ ë¡œê·¸ ë‚¨ê¹€
                    excluded_count = len(params) - len(filtered_params)
                    if excluded_count > 0:
                        excluded_keys = [k for k, v in params if k in EXCLUDED_PARAMS]
                        logger.debug(f"í˜ì´ì§€ë„¤ì´ì…˜ íŒŒë¼ë¯¸í„° {excluded_count}ê°œ ì œì™¸: {excluded_keys}")
                else:
                    # ëª¨ë“  íŒŒë¼ë¯¸í„°ê°€ í˜ì´ì§€ë„¤ì´ì…˜ì´ë©´ ê²½ë¡œë¡œ í´ë°±
                    if parsed.path and parsed.path != '/':
                        normalized_key = f"{domain}|path={parsed.path}"
                    else:
                        normalized_key = f"{domain}|no_params"
                    logger.warning(f"ëª¨ë“  íŒŒë¼ë¯¸í„°ê°€ í˜ì´ì§€ë„¤ì´ì…˜! ê²½ë¡œ ì‚¬ìš©: {url}")
            else:
                # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì—†ìœ¼ë©´ ê²½ë¡œ í¬í•¨
                if parsed.path and parsed.path != '/':
                    normalized_key = f"{domain}|path={parsed.path}"
                else:
                    # ê²½ë¡œë„ ì—†ìœ¼ë©´ ë„ë©”ì¸ë§Œ
                    normalized_key = f"{domain}|no_params"

            logger.info(f"âœ“ í´ë°± ì •ê·œí™” ì™„ë£Œ: {url[:80]}... â†’ {normalized_key}")
            return normalized_key

        except Exception as e:
            logger.error(f"í´ë°± ì •ê·œí™” ì¤‘ ì˜¤ë¥˜, ì›ë³¸ ë°˜í™˜: {e}")
            return url

    def _update_api_url_registry(
        self, session, origin_url: str, preprocessing_id: int, site_code: str,
        scraping_url: str = None, url_key_hash: str = None
    ) -> bool:
        """
        api_url_registry í…Œì´ë¸”ì˜ preprocessing_idë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.

        Args:
            session: SQLAlchemy ì„¸ì…˜
            origin_url: ì›ë³¸ URL
            preprocessing_id: announcement_pre_processing í…Œì´ë¸”ì˜ ID
            site_code: ì‚¬ì´íŠ¸ ì½”ë“œ
            scraping_url: ìŠ¤í¬ë˜í•‘ URL (API ì‚¬ì´íŠ¸ì˜ ê²½ìš° ìš°ì„  ë§¤ì¹­)
            url_key_hash: ì •ê·œí™”ëœ URL í•´ì‹œ (ê°€ì¥ ìš°ì„ ì ìœ¼ë¡œ ë§¤ì¹­)

        Returns:
            ì—…ë°ì´íŠ¸ ì„±ê³µ ì—¬ë¶€
        """
        try:
            from sqlalchemy import text

            # API ì‚¬ì´íŠ¸ë§Œ ì²˜ë¦¬
            if site_code not in ["kStartUp", "bizInfo", "smes24"]:
                logger.debug(f"API ì‚¬ì´íŠ¸ê°€ ì•„ë‹ˆë¯€ë¡œ api_url_registry ì—…ë°ì´íŠ¸ ê±´ë„ˆëœ€: {site_code}")
                return True

            # âš ï¸ í…Œì´ë¸” ì»¬ëŸ¼ êµ¬ì¡°:
            # - api_url_registry.announcement_url: ê³µê³  URL (bizInfo, smes24 ì‚¬ìš©)
            # - api_url_registry.scrap_url: ìŠ¤í¬ë˜í•‘ URL (kStartUp ì‚¬ìš©)
            # - api_url_registry.url_key_hash: ì •ê·œí™”ëœ URL í•´ì‹œ (ìš°ì„  ë§¤ì¹­)

            # ğŸ†• 0ìˆœìœ„: url_key_hashë¡œ ë§¤ì¹­ (ê°€ì¥ ì •í™•, ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ìˆœì„œ ë¬´ê´€)
            if url_key_hash:
                try:
                    update_sql = text("""
                        UPDATE api_url_registry
                        SET preprocessing_id = :preprocessing_id,
                            update_at = NOW()
                        WHERE url_key_hash = :url_key_hash
                        AND site_code = :site_code
                        LIMIT 1
                    """)

                    result = session.execute(
                        update_sql,
                        {
                            "preprocessing_id": preprocessing_id,
                            "url_key_hash": url_key_hash,
                            "site_code": site_code
                        }
                    )

                    rows_affected = result.rowcount
                    if rows_affected > 0:
                        logger.info(
                            f"âœ… api_url_registry ì—…ë°ì´íŠ¸ ì„±ê³µ ({site_code}, url_key_hash): "
                            f"hash={url_key_hash[:16]}..., preprocessing_id={preprocessing_id}"
                        )
                        return True
                    else:
                        logger.debug(
                            f"url_key_hashë¡œ ë§¤ì¹­ ì‹¤íŒ¨, ë¬¸ìì—´ ë§¤ì¹­ìœ¼ë¡œ í´ë°±: {url_key_hash[:16]}..."
                        )
                except Exception as e:
                    # url_key_hash ì»¬ëŸ¼ì´ ì—†ì„ ìˆ˜ ìˆìŒ (ì—ëŸ¬ ë¬´ì‹œí•˜ê³  ê¸°ì¡´ ë¡œì§ìœ¼ë¡œ í´ë°±)
                    logger.debug(f"url_key_hash ë§¤ì¹­ ì‹¤íŒ¨ (ì»¬ëŸ¼ ì—†ì„ ìˆ˜ ìˆìŒ), ë¬¸ìì—´ ë§¤ì¹­ìœ¼ë¡œ í´ë°±: {e}")

            if site_code == "kStartUp":
                # kStartUp: scrap_url ì»¬ëŸ¼ ì‚¬ìš© (announcement_urlì€ ì‹ ë¢°í•  ìˆ˜ ì—†ìŒ)
                if not scraping_url:
                    logger.debug("kStartUp: scraping_urlì´ ì—†ì–´ api_url_registry ì—…ë°ì´íŠ¸ ë¶ˆê°€")
                    return False

                update_sql = text("""
                    UPDATE api_url_registry
                    SET preprocessing_id = :preprocessing_id,
                        update_at = NOW()
                    WHERE scrap_url = :scraping_url
                    LIMIT 1
                """)

                result = session.execute(
                    update_sql,
                    {
                        "preprocessing_id": preprocessing_id,
                        "scraping_url": scraping_url
                    }
                )

                rows_affected = result.rowcount
                if rows_affected > 0:
                    logger.info(
                        f"api_url_registry ì—…ë°ì´íŠ¸ ì„±ê³µ (kStartUp, scrap_url): "
                        f"url={scraping_url[:50]}..., preprocessing_id={preprocessing_id}"
                    )
                    return True
                else:
                    logger.debug(
                        f"api_url_registryì— ë§¤ì¹­ë˜ëŠ” ë ˆì½”ë“œ ì—†ìŒ (kStartUp, scrap_url): "
                        f"scraping_url={scraping_url[:50]}..."
                    )
                    return False

            else:
                # bizInfo, smes24: announcement_url ì»¬ëŸ¼ ì‚¬ìš©
                # ìš°ì„ ìˆœìœ„: scraping_url â†’ origin_url

                # 1ì°¨ ì‹œë„: scraping_urlë¡œ ë§¤ì¹­
                if scraping_url:
                    update_sql = text("""
                        UPDATE api_url_registry
                        SET preprocessing_id = :preprocessing_id,
                            update_at = NOW()
                        WHERE announcement_url = :scraping_url
                        LIMIT 1
                    """)

                    result = session.execute(
                        update_sql,
                        {
                            "preprocessing_id": preprocessing_id,
                            "scraping_url": scraping_url
                        }
                    )

                    rows_affected = result.rowcount
                    if rows_affected > 0:
                        logger.info(
                            f"api_url_registry ì—…ë°ì´íŠ¸ ì„±ê³µ ({site_code}, announcement_url with scraping_url): "
                            f"url={scraping_url[:50]}..., preprocessing_id={preprocessing_id}"
                        )
                        return True

                # 2ì°¨ ì‹œë„: origin_urlë¡œ ë§¤ì¹­ (scraping_urlë¡œ ì‹¤íŒ¨í•œ ê²½ìš°)
                if origin_url:
                    update_sql = text("""
                        UPDATE api_url_registry
                        SET preprocessing_id = :preprocessing_id,
                            update_at = NOW()
                        WHERE announcement_url = :origin_url
                        LIMIT 1
                    """)

                    result = session.execute(
                        update_sql,
                        {
                            "preprocessing_id": preprocessing_id,
                            "origin_url": origin_url
                        }
                    )
                    # commitì€ _save_processing_resultì—ì„œ í•œ ë²ˆë§Œ ìˆ˜í–‰

                    rows_affected = result.rowcount
                    if rows_affected > 0:
                        logger.info(
                            f"api_url_registry ì—…ë°ì´íŠ¸ ì„±ê³µ ({site_code}, announcement_url with origin_url): "
                            f"url={origin_url[:50]}..., preprocessing_id={preprocessing_id}"
                        )
                        return True

                # ë‘˜ ë‹¤ ì‹¤íŒ¨
                logger.debug(
                    f"api_url_registryì— ë§¤ì¹­ë˜ëŠ” ë ˆì½”ë“œ ì—†ìŒ ({site_code}, announcement_url): "
                    f"scraping_url={scraping_url[:50] if scraping_url else 'None'}..., "
                    f"origin_url={origin_url[:50] if origin_url else 'None'}..."
                )
                return False

        except Exception as e:
            # í…Œì´ë¸”ì´ ì¡´ì¬í•˜ì§€ ì•Šê±°ë‚˜ ì»¬ëŸ¼ì´ ì—†ëŠ” ê²½ìš° ê²½ê³ ë§Œ ì¶œë ¥
            logger.warning(f"api_url_registry ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ (ë¬´ì‹œí•˜ê³  ê³„ì†): {e}")
            return False

    def _get_priority(self, site_type: str) -> int:
        """
        site_typeì˜ ìš°ì„ ìˆœìœ„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        ë†’ì„ìˆ˜ë¡ ìš°ì„ ìˆœìœ„ ë†’ìŒ.

        Args:
            site_type: ì‚¬ì´íŠ¸ íƒ€ì… (Eminwon, Homepage, Scraper, api_scrap ë“±)

        Returns:
            ìš°ì„ ìˆœìœ„ ê°’ (0-3)
        """
        priority_map = {
            'Eminwon': 3,
            'Homepage': 3,
            'Scraper': 3,
            'api_scrap': 1,
            'Unknown': 0,
        }
        return priority_map.get(site_type, 0)

    def _log_api_url_processing(
        self,
        session,
        site_code: str,
        url_key: str,
        url_key_hash: str,
        processing_status: str,
        announcement_id: str = None,
        announcement_url: str = None,
        scraping_url: str = None,
        preprocessing_id: int = None,
        existing_preprocessing_id: int = None,
        api_url_registry_id: int = None,
        existing_site_type: str = None,
        existing_site_code: str = None,
        duplicate_reason: dict = None,
        error_message: str = None,
        title: str = None,
        folder_name: str = None,
    ) -> bool:
        """
        API URL ì²˜ë¦¬ ì‹œë„ë¥¼ ë¡œê·¸ì— ê¸°ë¡í•©ë‹ˆë‹¤.

        Args:
            session: SQLAlchemy ì„¸ì…˜
            site_code: ì‚¬ì´íŠ¸ ì½”ë“œ (kStartUp, bizInfo, smes24, prv_* ë“±)
            url_key: ì •ê·œí™”ëœ URL í‚¤
            url_key_hash: URL í‚¤ í•´ì‹œ (MD5)
            processing_status: ì²˜ë¦¬ ìƒíƒœ
                - 'new_inserted': ìƒˆë¡œ ì‚½ì…ë¨
                - 'duplicate_updated': ì¤‘ë³µì´ì§€ë§Œ ì—…ë°ì´íŠ¸ë¨ (ìš°ì„ ìˆœìœ„ ë†’ìŒ)
                - 'duplicate_skipped': ì¤‘ë³µì´ë¼ ìŠ¤í‚µë¨ (ìš°ì„ ìˆœìœ„ ë‚®ìŒ)
                - 'duplicate_preserved': ê¸°ì¡´ ë°ì´í„° ìœ ì§€ë¨
                - 'failed': ì²˜ë¦¬ ì‹¤íŒ¨
                - 'no_url_key': URL ì •ê·œí™” ì‹¤íŒ¨
            preprocessing_id: ìƒì„±/ì—…ë°ì´íŠ¸ëœ ë ˆì½”ë“œ ID
            existing_preprocessing_id: ì´ë¯¸ ì¡´ì¬í•˜ë˜ ë ˆì½”ë“œ ID
            existing_site_type: ê¸°ì¡´ ë ˆì½”ë“œì˜ site_type
            existing_site_code: ê¸°ì¡´ ë ˆì½”ë“œì˜ site_code
            duplicate_reason: ì¤‘ë³µ ì‚¬ìœ  (dict)
            error_message: ì˜¤ë¥˜ ë©”ì‹œì§€
            title: ê³µê³  ì œëª©
            folder_name: í´ë”ëª…

        Returns:
            ë¡œê·¸ ê¸°ë¡ ì„±ê³µ ì—¬ë¶€
        """
        try:
            from sqlalchemy import text
            import json

            # duplicate_reasonì„ JSONìœ¼ë¡œ ë³€í™˜
            duplicate_reason_json = None
            if duplicate_reason:
                duplicate_reason_json = json.dumps(duplicate_reason, ensure_ascii=False)

            sql = text("""
                INSERT INTO api_url_processing_log (
                    site_code,
                    announcement_id,
                    announcement_url,
                    scraping_url,
                    url_key,
                    url_key_hash,
                    processing_status,
                    preprocessing_id,
                    existing_preprocessing_id,
                    api_url_registry_id,
                    existing_site_type,
                    existing_site_code,
                    duplicate_reason,
                    error_message,
                    title,
                    folder_name,
                    created_at
                ) VALUES (
                    :site_code,
                    :announcement_id,
                    :announcement_url,
                    :scraping_url,
                    :url_key,
                    :url_key_hash,
                    :processing_status,
                    :preprocessing_id,
                    :existing_preprocessing_id,
                    :api_url_registry_id,
                    :existing_site_type,
                    :existing_site_code,
                    :duplicate_reason,
                    :error_message,
                    :title,
                    :folder_name,
                    NOW()
                )
            """)

            session.execute(sql, {
                "site_code": site_code,
                "announcement_id": announcement_id,
                "announcement_url": announcement_url,
                "scraping_url": scraping_url,
                "url_key": url_key,
                "url_key_hash": url_key_hash,
                "processing_status": processing_status,
                "preprocessing_id": preprocessing_id,
                "existing_preprocessing_id": existing_preprocessing_id,
                "api_url_registry_id": api_url_registry_id,
                "existing_site_type": existing_site_type,
                "existing_site_code": existing_site_code,
                "duplicate_reason": duplicate_reason_json,
                "error_message": error_message,
                "title": title,
                "folder_name": folder_name,
            })

            logger.debug(
                f"API URL ì²˜ë¦¬ ë¡œê·¸ ê¸°ë¡: site_code={site_code}, "
                f"status={processing_status}, url_key_hash={url_key_hash[:16] if url_key_hash else 'None'}..."
            )
            return True

        except Exception as e:
            logger.warning(f"API URL ì²˜ë¦¬ ë¡œê·¸ ê¸°ë¡ ì‹¤íŒ¨ (ë¬´ì‹œí•˜ê³  ê³„ì†): {e}")
            return False

    def _log_announcement_duplicate(
        self,
        session,
        preprocessing_id: int,
        url_key_hash: str,
        duplicate_type: str,
        site_code: str,
        folder_name: str,
        domain: str = None,
        domain_configured: bool = False,
        existing_record: dict = None,
        error_message: str = None,
    ) -> bool:
        """
        announcement_duplicate_log í…Œì´ë¸”ì— ì¤‘ë³µ ì²˜ë¦¬ ë¡œê·¸ë¥¼ ê¸°ë¡í•©ë‹ˆë‹¤.

        Args:
            session: SQLAlchemy ì„¸ì…˜
            preprocessing_id: ì €ì¥/ì—…ë°ì´íŠ¸ëœ ë ˆì½”ë“œ ID
            url_key_hash: URL í‚¤ í•´ì‹œ (MD5) - domain_key_config ì—†ìœ¼ë©´ NULL
            duplicate_type: ì¤‘ë³µ ìœ í˜•
                - 'unconfigured_domain': domain_key_configì— ì„¤ì • ì—†ìŒ
                - 'new_inserted': ì‹ ê·œ ì‚½ì… (domain_key_config ìˆê³  ì¤‘ë³µ ì—†ìŒ)
                - 'replaced': ê¸°ì¡´ ë°ì´í„° êµì²´ (ìš°ì„ ìˆœìœ„ ë†’ìŒ)
                - 'kept_existing': ê¸°ì¡´ ë°ì´í„° ìœ ì§€ (ìš°ì„ ìˆœìœ„ ë‚®ìŒ)
                - 'same_type_duplicate': ë™ì¼ íƒ€ì… ì¬ìˆ˜ì§‘ (ìš°ì„ ìˆœìœ„ ë™ì¼)
                - 'error': ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜
            site_code: ì‚¬ì´íŠ¸ ì½”ë“œ
            folder_name: í´ë”ëª…
            domain: ë„ë©”ì¸ëª…
            domain_configured: domain_key_configì— ì„¤ì • ì—¬ë¶€
            existing_record: ê¸°ì¡´ ë ˆì½”ë“œ ì •ë³´ (ì¤‘ë³µ ì‹œ)
            error_message: ì—ëŸ¬ ë©”ì‹œì§€ (ì˜¤ë¥˜ ì‹œ)

        Returns:
            ë¡œê·¸ ê¸°ë¡ ì„±ê³µ ì—¬ë¶€
        """
        try:
            from sqlalchemy import text
            import json
            from datetime import datetime

            # ìš°ì„ ìˆœìœ„ ê³„ì‚°
            new_priority = self._get_priority(self.site_type)
            existing_priority = None
            existing_preprocessing_id = None
            existing_site_type = None
            existing_site_code = None
            duplicate_detail = None

            # ê¸°ì¡´ ë ˆì½”ë“œ ì •ë³´ ì¶”ì¶œ
            if existing_record:
                existing_preprocessing_id = existing_record.get('id')
                existing_site_type = existing_record.get('site_type')
                existing_site_code = existing_record.get('site_code')
                existing_priority = self._get_priority(existing_site_type)

                # ìƒì„¸ ì •ë³´ JSON ìƒì„±
                if duplicate_type == 'replaced':
                    decision = 'ê¸°ì¡´ ë°ì´í„° êµì²´'
                    reason = f'ìš°ì„ ìˆœìœ„ ë†’ìŒ: {self.site_type}({new_priority}) > {existing_site_type}({existing_priority})'
                elif duplicate_type == 'kept_existing':
                    decision = 'ê¸°ì¡´ ë°ì´í„° ìœ ì§€'
                    reason = f'ìš°ì„ ìˆœìœ„ ë‚®ìŒ: {self.site_type}({new_priority}) < {existing_site_type}({existing_priority})'
                elif duplicate_type == 'same_type_duplicate':
                    decision = 'ìµœì‹  ë°ì´í„°ë¡œ ì—…ë°ì´íŠ¸'
                    reason = f'ìš°ì„ ìˆœìœ„ ë™ì¼: {self.site_type}({new_priority}) = {existing_site_type}({existing_priority})'
                else:
                    decision = 'ì•Œ ìˆ˜ ì—†ìŒ'
                    reason = f'duplicate_type={duplicate_type}'

                duplicate_detail = {
                    'decision': decision,
                    'reason': reason,
                    'existing_folder': existing_record.get('folder_name'),
                    'existing_url_key': existing_record.get('url_key'),
                    'priority_comparison': f'{new_priority} vs {existing_priority}',
                    'domain': domain,
                    'domain_configured': domain_configured,
                    'timestamp': datetime.now().isoformat()
                }

            elif duplicate_type == 'unconfigured_domain':
                # domain_key_configì— ì—†ëŠ” ê²½ìš°
                duplicate_detail = {
                    'decision': 'ì‹ ê·œ ë“±ë¡ (domain_key_config ì—†ìŒ)',
                    'reason': 'domain_key_config í…Œì´ë¸”ì— ì„¤ì •ì´ ì—†ì–´ì„œ ì¤‘ë³µ ì²´í¬ ìƒëµ',
                    'domain': domain,
                    'domain_configured': False,
                    'timestamp': datetime.now().isoformat()
                }

            elif duplicate_type == 'new_inserted':
                # domain_key_configì— ìˆì§€ë§Œ url_key_hash ì¤‘ë³µ ì—†ìŒ
                duplicate_detail = {
                    'decision': 'ì‹ ê·œ ë“±ë¡',
                    'reason': 'url_key_hash ì¤‘ë³µ ì—†ìŒ',
                    'domain': domain,
                    'domain_configured': domain_configured,
                    'timestamp': datetime.now().isoformat()
                }

            # ê¸°ì¡´ í´ë”ëª… ì¶”ì¶œ
            existing_folder_name = None
            if existing_record:
                existing_folder_name = existing_record.get('folder_name')

            # announcement_duplicate_log INSERT
            sql = text("""
                INSERT INTO announcement_duplicate_log (
                    preprocessing_id,
                    existing_preprocessing_id,
                    duplicate_type,
                    url_key_hash,
                    new_site_type,
                    new_site_code,
                    existing_site_type,
                    existing_site_code,
                    new_priority,
                    existing_priority,
                    new_folder_name,
                    existing_folder_name,
                    duplicate_detail,
                    error_message
                ) VALUES (
                    :preprocessing_id,
                    :existing_preprocessing_id,
                    :duplicate_type,
                    :url_key_hash,
                    :new_site_type,
                    :new_site_code,
                    :existing_site_type,
                    :existing_site_code,
                    :new_priority,
                    :existing_priority,
                    :new_folder_name,
                    :existing_folder_name,
                    :duplicate_detail,
                    :error_message
                )
            """)

            # JSON ì§ë ¬í™”
            duplicate_detail_json = None
            if duplicate_detail:
                duplicate_detail_json = json.dumps(duplicate_detail, ensure_ascii=False)

            # íŒŒë¼ë¯¸í„° ë°”ì¸ë”©
            params = {
                'preprocessing_id': preprocessing_id,
                'existing_preprocessing_id': existing_preprocessing_id,
                'duplicate_type': duplicate_type,
                'url_key_hash': url_key_hash,  # unconfigured_domainì¼ ë•Œ NULL
                'new_site_type': self.site_type,
                'new_site_code': site_code,
                'existing_site_type': existing_site_type,
                'existing_site_code': existing_site_code,
                'new_priority': new_priority,
                'existing_priority': existing_priority,
                'new_folder_name': folder_name,
                'existing_folder_name': existing_folder_name,
                'duplicate_detail': duplicate_detail_json,
                'error_message': error_message
            }

            # ì‹¤í–‰
            session.execute(sql, params)
            # session.commit()ëŠ” í˜¸ì¶œí•˜ì§€ ì•ŠìŒ (ìƒìœ„ í•¨ìˆ˜ì—ì„œ commit)

            logger.debug(
                f"ì¤‘ë³µ ë¡œê·¸ ê¸°ë¡ ì™„ë£Œ: {duplicate_type} - "
                f"preprocessing_id={preprocessing_id}, "
                f"domain_configured={domain_configured}, "
                f"url_key_hash={url_key_hash[:16] if url_key_hash else 'None'}..."
            )

            return True

        except Exception as e:
            logger.error(f"ì¤‘ë³µ ë¡œê·¸ ê¸°ë¡ ì‹¤íŒ¨: {e}")
            # ë¡œê·¸ ê¸°ë¡ ì‹¤íŒ¨í•´ë„ ë©”ì¸ ì²˜ë¦¬ëŠ” ê³„ì† ì§„í–‰
            return False

    def _save_processing_result(
        self,
        folder_name: str,
        site_code: str,
        content_md: str,
        combined_content: str,
        attachment_filenames: List[str] = None,
        status: str = "ì„±ê³µ",
        exclusion_keywords: List[str] = None,
        exclusion_reason: str = None,
        error_message: str = None,
        force: bool = False,
        title: str = None,
        origin_url: str = None,
        url_key: str = None,
        scraping_url: str = None,
        announcement_date: str = None,
        attachment_files_info: List[Dict[str, Any]] = None,
    ) -> Optional[int]:
        """ì²˜ë¦¬ ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤."""
        try:
            from sqlalchemy import text

            with self.db_manager.SessionLocal() as session:
                # ================================================
                # ğŸ†• ì˜ˆì™¸ ì¼€ì´ìŠ¤: smes24 + bizinfo URL ì¤‘ë³µ ì²´í¬
                # ================================================
                # smes24ì˜ origin_urlì´ bizInfoì˜ scraping_urlê³¼ ì¼ì¹˜í•˜ë©´ ìŠ¤í‚µ
                if site_code == 'smes24' and origin_url and 'bizinfo.go.kr' in origin_url.lower():
                    try:
                        existing_bizinfo = session.execute(
                            text("""
                                SELECT id, site_type, site_code, folder_name, url_key, created_at
                                FROM announcement_pre_processing
                                WHERE scraping_url = :origin_url
                                AND site_code = 'bizInfo'
                                LIMIT 1
                            """),
                            {"origin_url": origin_url}
                        ).fetchone()

                        if existing_bizinfo:
                            logger.info(
                                f"ğŸš« ì¤‘ë³µ ìŠ¤í‚µ (ì˜ˆì™¸ ë¡œì§): smes24 origin_urlì´ bizInfo scraping_urlê³¼ ì¼ì¹˜\n"
                                f"   smes24 folder: {folder_name}\n"
                                f"   origin_url: {origin_url[:100]}...\n"
                                f"   ê¸°ì¡´ bizInfo: ID={existing_bizinfo.id}, folder={existing_bizinfo.folder_name}\n"
                                f"   ê¸°ì¡´ url_key: {existing_bizinfo.url_key}\n"
                                f"   â†’ bizInfo ìš°ì„  (ì§€ìì²´ ì›ë³¸ ë°ì´í„° ìœ ì§€)"
                            )

                            return existing_bizinfo.id  # ê¸°ì¡´ ID ë°˜í™˜í•˜ê³  ì¢…ë£Œ

                    except Exception as e:
                        logger.error(f"ì˜ˆì™¸ ì¼€ì´ìŠ¤ ì¤‘ë³µ ì²´í¬ ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {e}")
                        # ì—ëŸ¬ ë°œìƒ ì‹œ ê¸°ì¡´ ë¡œì§ìœ¼ë¡œ í´ë°±

                # UPSERT ì‹¤í–‰ ì „ì— ê¸°ì¡´ ë ˆì½”ë“œ ì¡°íšŒ (ìš°ì„ ìˆœìœ„ ë¹„êµë¥¼ ìœ„í•´)
                existing_record_before_upsert = None
                if force and url_key:
                    try:
                        existing_record_before_upsert = session.execute(
                            text("""
                                SELECT id, site_type, site_code, folder_name
                                FROM announcement_pre_processing
                                WHERE url_key = :url_key
                                LIMIT 1
                            """),
                            {"url_key": url_key}
                        ).fetchone()

                        if existing_record_before_upsert:
                            logger.debug(
                                f"UPSERT ì „ ê¸°ì¡´ ë ˆì½”ë“œ ë°œê²¬: ID={existing_record_before_upsert.id}, "
                                f"site_type={existing_record_before_upsert.site_type}, "
                                f"site_code={existing_record_before_upsert.site_code}"
                            )
                    except Exception as e:
                        logger.warning(f"UPSERT ì „ ê¸°ì¡´ ë ˆì½”ë“œ ì¡°íšŒ ì‹¤íŒ¨ (ë¬´ì‹œí•˜ê³  ê³„ì†): {e}")

                if force:
                    # UPSERT ë¡œì§ with site_type ìš°ì„ ìˆœìœ„ (ì§€ìì²´ > API)
                    sql = text(
                        """
                        INSERT INTO announcement_pre_processing (
                            folder_name, site_type, site_code, content_md, combined_content,
                            attachment_filenames, attachment_files_list, exclusion_keyword, exclusion_reason,
                            title, origin_url, url_key, scraping_url, announcement_date,
                            processing_status, error_message, created_at, updated_at
                        ) VALUES (
                            :folder_name, :site_type, :site_code, :content_md, :combined_content,
                            :attachment_filenames, :attachment_files_list, :exclusion_keyword, :exclusion_reason,
                            :title, :origin_url, :url_key, :scraping_url, :announcement_date,
                            :processing_status, :error_message, NOW(), NOW()
                        )
                        ON DUPLICATE KEY UPDATE
                            site_type = IF(
                                VALUES(site_type) IN ('Eminwon', 'Homepage', 'Scraper') OR
                                site_type NOT IN ('Eminwon', 'Homepage', 'Scraper'),
                                VALUES(site_type),
                                site_type
                            ),
                            content_md = IF(
                                VALUES(site_type) IN ('Eminwon', 'Homepage', 'Scraper') OR
                                site_type NOT IN ('Eminwon', 'Homepage', 'Scraper'),
                                VALUES(content_md),
                                content_md
                            ),
                            combined_content = IF(
                                VALUES(site_type) IN ('Eminwon', 'Homepage', 'Scraper') OR
                                site_type NOT IN ('Eminwon', 'Homepage', 'Scraper'),
                                VALUES(combined_content),
                                combined_content
                            ),
                            attachment_filenames = IF(
                                VALUES(site_type) IN ('Eminwon', 'Homepage', 'Scraper') OR
                                site_type NOT IN ('Eminwon', 'Homepage', 'Scraper'),
                                VALUES(attachment_filenames),
                                attachment_filenames
                            ),
                            attachment_files_list = IF(
                                VALUES(site_type) IN ('Eminwon', 'Homepage', 'Scraper') OR
                                site_type NOT IN ('Eminwon', 'Homepage', 'Scraper'),
                                VALUES(attachment_files_list),
                                attachment_files_list
                            ),
                            exclusion_keyword = IF(
                                VALUES(site_type) IN ('Eminwon', 'Homepage', 'Scraper') OR
                                site_type NOT IN ('Eminwon', 'Homepage', 'Scraper'),
                                VALUES(exclusion_keyword),
                                exclusion_keyword
                            ),
                            exclusion_reason = IF(
                                VALUES(site_type) IN ('Eminwon', 'Homepage', 'Scraper') OR
                                site_type NOT IN ('Eminwon', 'Homepage', 'Scraper'),
                                VALUES(exclusion_reason),
                                exclusion_reason
                            ),
                            processing_status = IF(
                                VALUES(site_type) IN ('Eminwon', 'Homepage', 'Scraper') OR
                                site_type NOT IN ('Eminwon', 'Homepage', 'Scraper'),
                                VALUES(processing_status),
                                processing_status
                            ),
                            title = IF(
                                VALUES(site_type) IN ('Eminwon', 'Homepage', 'Scraper') OR
                                site_type NOT IN ('Eminwon', 'Homepage', 'Scraper'),
                                VALUES(title),
                                title
                            ),
                            origin_url = IF(
                                VALUES(site_type) IN ('Eminwon', 'Homepage', 'Scraper') OR
                                site_type NOT IN ('Eminwon', 'Homepage', 'Scraper'),
                                VALUES(origin_url),
                                origin_url
                            ),
                            url_key = IF(
                                VALUES(site_type) IN ('Eminwon', 'Homepage', 'Scraper') OR
                                site_type NOT IN ('Eminwon', 'Homepage', 'Scraper'),
                                VALUES(url_key),
                                url_key
                            ),
                            scraping_url = IF(
                                VALUES(site_type) IN ('Eminwon', 'Homepage', 'Scraper') OR
                                site_type NOT IN ('Eminwon', 'Homepage', 'Scraper'),
                                VALUES(scraping_url),
                                scraping_url
                            ),
                            announcement_date = IF(
                                VALUES(site_type) IN ('Eminwon', 'Homepage', 'Scraper') OR
                                site_type NOT IN ('Eminwon', 'Homepage', 'Scraper'),
                                VALUES(announcement_date),
                                announcement_date
                            ),
                            error_message = IF(
                                VALUES(site_type) IN ('Eminwon', 'Homepage', 'Scraper') OR
                                site_type NOT IN ('Eminwon', 'Homepage', 'Scraper'),
                                VALUES(error_message),
                                error_message
                            ),
                            updated_at = NOW()
                    """
                    )
                else:
                    # ì¼ë°˜ INSERT with UPSERT (ì¤‘ë³µ ì²˜ë¦¬)
                    sql = text(
                        """
                        INSERT INTO announcement_pre_processing (
                            folder_name, site_type, site_code, content_md, combined_content,
                            attachment_filenames, attachment_files_list, exclusion_keyword, exclusion_reason,
                            title, origin_url, url_key, scraping_url, announcement_date,
                            processing_status, error_message, created_at, updated_at
                        ) VALUES (
                            :folder_name, :site_type, :site_code, :content_md, :combined_content,
                            :attachment_filenames, :attachment_files_list, :exclusion_keyword, :exclusion_reason,
                            :title, :origin_url, :url_key, :scraping_url, :announcement_date,
                            :processing_status, :error_message, NOW(), NOW()
                        )
                        ON DUPLICATE KEY UPDATE
                            folder_name = VALUES(folder_name),
                            site_type = VALUES(site_type),
                            site_code = VALUES(site_code),
                            content_md = VALUES(content_md),
                            combined_content = VALUES(combined_content),
                            attachment_filenames = VALUES(attachment_filenames),
                            attachment_files_list = VALUES(attachment_files_list),
                            exclusion_keyword = VALUES(exclusion_keyword),
                            exclusion_reason = VALUES(exclusion_reason),
                            title = VALUES(title),
                            origin_url = VALUES(origin_url),
                            url_key = VALUES(url_key),
                            scraping_url = VALUES(scraping_url),
                            announcement_date = VALUES(announcement_date),
                            processing_status = VALUES(processing_status),
                            error_message = VALUES(error_message),
                            updated_at = NOW()
                    """
                    )

                # JSONìœ¼ë¡œ ì§ë ¬í™”
                attachment_files_json = (
                    json.dumps(attachment_files_info, ensure_ascii=False)
                    if attachment_files_info
                    else None
                )

                # Homepage ë˜ëŠ” Eminwonì¸ ê²½ìš° DBì— ì €ì¥í•  site_codeì— "prv_" ì ‘ë‘ì‚¬ ì¶”ê°€
                # ë‹¨, ì›ë³¸ site_codeëŠ” ë³€ê²½í•˜ì§€ ì•ŠìŒ (API ì—…ë°ì´íŠ¸ ë“±ì—ì„œ ì‚¬ìš©)
                db_site_code = ("prv_" + site_code) if self.site_type in ("Homepage", "Eminwon") else site_code

                params = {
                    "folder_name": folder_name,
                    "site_type": self.site_type,
                    "site_code": db_site_code,
                    "content_md": content_md,
                    "combined_content": combined_content,
                    "attachment_filenames": (
                        ", ".join(attachment_filenames)
                        if attachment_filenames
                        else None
                    ),
                    "attachment_files_list": attachment_files_json,
                    "exclusion_keyword": (
                        ", ".join(exclusion_keywords) if exclusion_keywords else None
                    ),
                    "exclusion_reason": exclusion_reason,
                    "title": title,
                    "origin_url": origin_url,
                    "url_key": url_key,
                    "scraping_url": scraping_url,
                    "announcement_date": announcement_date,
                    "processing_status": status,
                    "error_message": error_message,
                }

                result = session.execute(sql, params)
                record_id = result.lastrowid
                affected_rows = result.rowcount

                # ================================================
                # ğŸ”§ url_key_hash ì¡°íšŒ (GENERATED COLUMNì´ë¯€ë¡œ DBì—ì„œ ìë™ ìƒì„±)
                # ================================================
                # url_keyê°€ ìˆëŠ” ê²½ìš° DBì—ì„œ ìë™ ìƒì„±ëœ url_key_hash ì¡°íšŒ
                url_key_hash = None
                if url_key:
                    url_key_hash_result = session.execute(
                        text("SELECT url_key_hash FROM announcement_pre_processing WHERE id = :id"),
                        {"id": record_id}
                    ).fetchone()
                    if url_key_hash_result:
                        url_key_hash = url_key_hash_result.url_key_hash
                        logger.debug(f"DB ìƒì„± url_key_hash: {url_key_hash[:16]}... (record_id={record_id})")
                    else:
                        logger.warning(f"url_key_hash ì¡°íšŒ ì‹¤íŒ¨: record_id={record_id}")

                # ================================================
                # ğŸ†• API URL ì²˜ë¦¬ ë¡œê·¸ ê¸°ë¡
                # ================================================
                # url_keyê°€ ì—†ìœ¼ë©´ 'no_url_key' ìƒíƒœë¡œ ê¸°ë¡
                if not url_key:
                    # ================================================
                    # ğŸ†• api_url_processing_log ê¸°ë¡ (ì‚­ì œ ì˜ˆì •)
                    # ================================================
                    self._log_api_url_processing(
                        session=session,
                        site_code=db_site_code,  # â† site_code â†’ db_site_code (ì¼ê´€ì„±)
                        url_key=None,
                        url_key_hash=None,
                        processing_status='no_url_key',
                        preprocessing_id=record_id,
                        title=title,
                        folder_name=folder_name,
                        error_message="URL ì •ê·œí™” ì‹¤íŒ¨ (url_key ì—†ìŒ)"
                    )

                    # ================================================
                    # ğŸ†• announcement_duplicate_log ê¸°ë¡ (ì‹ ê·œ)
                    # ================================================
                    # url_keyê°€ ì—†ìŒ â†’ domain_key_configì— ì„¤ì • ì—†ìŒ or URL ì¶”ì¶œ ì‹¤íŒ¨
                    from urllib.parse import urlparse
                    domain = None
                    if origin_url:
                        try:
                            parsed_url = urlparse(origin_url)
                            domain = parsed_url.netloc
                        except Exception as e:
                            logger.warning(f"URL íŒŒì‹± ì‹¤íŒ¨: {origin_url}, {e}")

                    self._log_announcement_duplicate(
                        session=session,
                        preprocessing_id=record_id,
                        url_key_hash=None,
                        duplicate_type='unconfigured_domain',  # domain_key_config ì—†ê±°ë‚˜ URL ì¶”ì¶œ ì‹¤íŒ¨
                        site_code=db_site_code,
                        folder_name=folder_name,
                        domain=domain,
                        domain_configured=False,
                        existing_record=None,
                        error_message="URL ì •ê·œí™” ì‹¤íŒ¨ (url_key ì—†ìŒ)"
                    )
                else:
                    # âš ï¸ url_key_hashëŠ” GENERATED COLUMNì´ë¯€ë¡œ ì´ë¯¸ ìœ„ì—ì„œ DB ì¡°íšŒë¡œ ì·¨ë“í•¨
                    # (line 2037-2047ì—ì„œ ì¡°íšŒ)

                    # domain_key_config í™•ì¸
                    from urllib.parse import urlparse
                    parsed_url = urlparse(origin_url)
                    domain = parsed_url.netloc
                    domain_has_config = self.url_key_extractor.get_domain_config(domain, parsed_url.path)

                    # ì²˜ë¦¬ ìƒíƒœ ê²°ì •
                    processing_status = None
                    existing_preprocessing_id = None
                    existing_site_type = None
                    existing_site_code = None
                    duplicate_reason = None

                    # domain_key_config í™•ì¸ ë° ì²˜ë¦¬
                    if not domain_has_config:
                        # domain_key_configì— ì—†ëŠ” ê²½ìš° (API ì™¸ë¶€ ë„ë©”ì¸ ë“±)
                        logger.debug(
                            f"domain_key_config ì—†ìŒ: domain={domain}, url_key={url_key[:50]}... "
                            f"fallbackìœ¼ë¡œ url_key ìƒì„±ë¨"
                        )
                        processing_status = 'new_inserted'  # domain_key_config ì—†ì–´ë„ ì‹ ê·œë¡œ ì²˜ë¦¬
                        duplicate_reason = {
                            "reason": f"domain_key_config not found, treated as new (domain={domain})",
                            "domain": domain,
                            "url_key": url_key
                        }

                    # domain_key_config ìˆëŠ” ê²½ìš°: ì •ìƒ ì¤‘ë³µ ì²´í¬
                    elif affected_rows == 1:
                        # ìƒˆë¡œ INSERTë¨
                        processing_status = 'new_inserted'  # â† ì¤‘ë³µ ì²´í¬ ê²°ê³¼ (duplicate_typeìš©)
                        logger.debug(f"ìƒˆ ë ˆì½”ë“œ ì‚½ì…: ID={record_id}, url_key_hash={url_key_hash[:16]}...")

                    elif affected_rows == 2:
                        # UPDATEë¨ (ON DUPLICATE KEY UPDATE ì‹¤í–‰)
                        logger.debug(f"ì¤‘ë³µ ê°ì§€ (affected_rows=2): url_key_hash={url_key_hash[:16]}...")

                        # UPSERT ì „ì— ì¡°íšŒí•œ ê¸°ì¡´ ë ˆì½”ë“œ ì •ë³´ ì‚¬ìš©
                        if existing_record_before_upsert:
                            # ì—…ë°ì´íŠ¸ ì „ì˜ ì •í™•í•œ ê°’ìœ¼ë¡œ ìš°ì„ ìˆœìœ„ ë¹„êµ
                            existing_site_type = existing_record_before_upsert.site_type
                            existing_site_code = existing_record_before_upsert.site_code
                            existing_preprocessing_id = existing_record_before_upsert.id

                            # ìš°ì„ ìˆœìœ„ ë¹„êµ
                            current_priority = self._get_priority(self.site_type)
                            existing_priority = self._get_priority(existing_site_type)

                            if current_priority > existing_priority:
                                # í˜„ì¬ê°€ ë” ë†’ì€ ìš°ì„ ìˆœìœ„ â†’ ì—…ë°ì´íŠ¸ë¨
                                processing_status = 'duplicate_updated'
                                duplicate_reason = {
                                    "reason": f"{self.site_type} (priority {current_priority}) > {existing_site_type} (priority {existing_priority})",
                                    "current_priority": current_priority,
                                    "existing_priority": existing_priority,
                                    "updated": True
                                }
                                logger.info(
                                    f"âœ“ ìš°ì„ ìˆœìœ„ ë†’ìŒ: {self.site_type}({current_priority}) > "
                                    f"{existing_site_type}({existing_priority}) â†’ ì—…ë°ì´íŠ¸ë¨"
                                )
                            elif current_priority == existing_priority:
                                # ê°™ì€ ìš°ì„ ìˆœìœ„ â†’ ì—…ë°ì´íŠ¸ë¨ (ìµœì‹  ë°ì´í„° ìš°ì„ )
                                processing_status = 'duplicate_updated'
                                duplicate_reason = {
                                    "reason": f"{self.site_type} (priority {current_priority}) == {existing_site_type} (priority {existing_priority}), ìµœì‹  ë°ì´í„° ìš°ì„ ",
                                    "current_priority": current_priority,
                                    "existing_priority": existing_priority,
                                    "updated": True
                                }
                                logger.info(
                                    f"âœ“ ìš°ì„ ìˆœìœ„ ë™ì¼: {self.site_type}({current_priority}) == "
                                    f"{existing_site_type}({existing_priority}) â†’ ì—…ë°ì´íŠ¸ë¨ (ìµœì‹  ë°ì´í„°)"
                                )
                            else:
                                # í˜„ì¬ê°€ ë” ë‚®ì€ ìš°ì„ ìˆœìœ„ â†’ ê¸°ì¡´ ìœ ì§€
                                processing_status = 'duplicate_preserved'
                                duplicate_reason = {
                                    "reason": f"{self.site_type} (priority {current_priority}) < {existing_site_type} (priority {existing_priority})",
                                    "current_priority": current_priority,
                                    "existing_priority": existing_priority,
                                    "updated": False
                                }
                                logger.info(
                                    f"âš ï¸  ìš°ì„ ìˆœìœ„ ë‚®ìŒ: {self.site_type}({current_priority}) < "
                                    f"{existing_site_type}({existing_priority}) â†’ ê¸°ì¡´ ë°ì´í„° ìœ ì§€"
                                )
                        else:
                            # UPSERT ì „ ì¡°íšŒ ì‹¤íŒ¨ â†’ ì—…ë°ì´íŠ¸ë¨ìœ¼ë¡œ ê°„ì£¼
                            processing_status = 'duplicate_updated'
                            duplicate_reason = {"reason": "UPSERT ì „ ê¸°ì¡´ ë ˆì½”ë“œ ì¡°íšŒ ì‹¤íŒ¨, ì—…ë°ì´íŠ¸ë¨ìœ¼ë¡œ ê°„ì£¼"}
                            logger.warning("UPSERT ì „ ê¸°ì¡´ ë ˆì½”ë“œ ì¡°íšŒ ì‹¤íŒ¨, ì—…ë°ì´íŠ¸ë¨ìœ¼ë¡œ ê°„ì£¼")

                    else:
                        # ì˜ˆìƒì¹˜ ëª»í•œ ê²½ìš°
                        processing_status = 'failed'
                        duplicate_reason = {"reason": f"Unexpected affected_rows: {affected_rows}"}
                        logger.warning(f"ì˜ˆìƒì¹˜ ëª»í•œ affected_rows: {affected_rows}")

                    # ë¡œê·¸ ê¸°ë¡
                    if processing_status:
                        # ================================================
                        # ğŸ†• api_url_processing_log ê¸°ë¡ (ì‚­ì œ ì˜ˆì •)
                        # ================================================
                        self._log_api_url_processing(
                            session=session,
                            site_code=db_site_code,  # â† site_code â†’ db_site_code (ì¼ê´€ì„±)
                            url_key=url_key,
                            url_key_hash=url_key_hash,
                            processing_status=processing_status,
                            announcement_url=origin_url,
                            scraping_url=scraping_url,
                            preprocessing_id=record_id,
                            existing_preprocessing_id=existing_preprocessing_id,
                            existing_site_type=existing_site_type,
                            existing_site_code=existing_site_code,
                            duplicate_reason=duplicate_reason,
                            title=title,
                            folder_name=folder_name
                        )

                        # ================================================
                        # ğŸ†• announcement_duplicate_log ê¸°ë¡ (ì‹ ê·œ)
                        # ================================================
                        # âš ï¸ ì¤‘ìš”: processing_statusëŠ” "ì¤‘ë³µ ì²´í¬ ê²°ê³¼"ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë‚´ë¶€ ë³€ìˆ˜
                        #          announcement_pre_processing.processing_status ì»¬ëŸ¼ê³¼ëŠ” ë‹¤ë¦„!
                        # processing_status ê°’:
                        #   - 'new_inserted': affected_rows=1 (ì‹ ê·œ INSERT)
                        #   - 'duplicate_updated': affected_rows=2 (UPDATEë¨)
                        #   - 'duplicate_preserved': affected_rows=2 + ìš°ì„ ìˆœìœ„ ë‚®ìŒ
                        #   - 'failed': affected_rows ì˜ˆìƒì¹˜ ëª»í•œ ê°’

                        # duplicate_type ë§¤í•‘
                        duplicate_type_map = {
                            'new_inserted': 'new_inserted',
                            'duplicate_updated': 'replaced',  # ê¸°ë³¸ê°’ (ìš°ì„ ìˆœìœ„ ë¹„êµë¡œ ì„¸ë¶€í™”)
                            'duplicate_preserved': 'kept_existing',
                            'failed': 'error'
                        }

                        # duplicate_type ê²°ì •
                        announcement_duplicate_type = duplicate_type_map.get(processing_status, 'unknown')  # ê¸°ë³¸ê°’ì„ 'unknown'ìœ¼ë¡œ ë³€ê²½

                        # duplicate_updatedì˜ ê²½ìš° ìš°ì„ ìˆœìœ„ ë¹„êµë¡œ ì„¸ë¶€ íƒ€ì… ê²°ì •
                        if processing_status == 'duplicate_updated' and existing_record_before_upsert:
                            current_priority = self._get_priority(self.site_type)
                            existing_priority_value = self._get_priority(existing_record_before_upsert.site_type)

                            if current_priority == existing_priority_value:
                                # ìš°ì„ ìˆœìœ„ ë™ì¼ â†’ same_type_duplicate
                                announcement_duplicate_type = 'same_type_duplicate'
                            elif current_priority > existing_priority_value:
                                # ìš°ì„ ìˆœìœ„ ë†’ìŒ â†’ replaced
                                announcement_duplicate_type = 'replaced'
                            # current_priority < existing_priority_valueëŠ” ì´ë¡ ì ìœ¼ë¡œ ë°œìƒí•˜ì§€ ì•ŠìŒ (UPSERT ì¡°ê±´ìƒ)

                        # existing_record dict ì¤€ë¹„
                        existing_record_dict = None
                        if existing_record_before_upsert:
                            existing_record_dict = {
                                'id': existing_record_before_upsert.id,
                                'site_type': existing_record_before_upsert.site_type,
                                'site_code': existing_record_before_upsert.site_code,
                                'folder_name': existing_record_before_upsert.folder_name,
                                'url_key': url_key  # url_keyëŠ” ë™ì¼
                            }

                        # announcement_duplicate_log ê¸°ë¡
                        self._log_announcement_duplicate(
                            session=session,
                            preprocessing_id=record_id,
                            url_key_hash=url_key_hash,
                            duplicate_type=announcement_duplicate_type,
                            site_code=db_site_code,
                            folder_name=folder_name,
                            domain=domain,
                            domain_configured=True,  # url_keyê°€ ìˆìœ¼ë¯€ë¡œ domain_key_config ìˆìŒ
                            existing_record=existing_record_dict,
                            error_message=None
                        )

                # API ì‚¬ì´íŠ¸ì¸ ê²½ìš° api_url_registry í…Œì´ë¸” ì—…ë°ì´íŠ¸ (commit ì „ì— ì‹¤í–‰)
                api_registry_updated = False
                if origin_url:
                    api_registry_updated = self._update_api_url_registry(
                        session, origin_url, record_id, db_site_code, scraping_url,
                        url_key_hash=url_key_hash  # ğŸ†• url_key_hash ì¶”ê°€
                    )

                    # API ì‚¬ì´íŠ¸ì¸ë° api_url_registry ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ ì‹œ ê²½ê³ 
                    if not api_registry_updated and db_site_code in ["kStartUp", "bizInfo", "smes24"]:
                        logger.warning(
                            f"âš ï¸  API ì‚¬ì´íŠ¸ì´ì§€ë§Œ api_url_registry ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: "
                            f"site_code={db_site_code}, origin_url={origin_url[:80]}..."
                        )

                # ëª¨ë“  ë³€ê²½ì‚¬í•­ì„ í•œ ë²ˆì— ì»¤ë°‹
                session.commit()
                logger.info(f"ì²˜ë¦¬ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: ID {record_id}, ìƒíƒœ: {status}")

                return record_id

        except Exception as e:
            logger.error(f"ì²˜ë¦¬ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            return None


def determine_site_type(directory_name: str, site_code: str) -> str:
    """ë””ë ‰í† ë¦¬ëª…ê³¼ ì‚¬ì´íŠ¸ ì½”ë“œì—ì„œ site_typeì„ ê²°ì •í•©ë‹ˆë‹¤."""
    # ì ˆëŒ€ ê²½ë¡œ ì •ê·œí™” (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´)
    dir_lower = directory_name.lower()

    # íŠ¹ìˆ˜ API ì‚¬ì´íŠ¸ ì²´í¬
    if site_code in ["kStartUp", "bizInfo", "smes24"]:
        return "api_scrap"

    # í”„ë¡œë•ì…˜ í™˜ê²½ ê²½ë¡œ ì²´í¬ (/home/zium/moabojo/incremental/*)
    if "/incremental/api" in dir_lower or "\\incremental\\api" in dir_lower:
        return "api_scrap"
    elif "/incremental/eminwon" in dir_lower or "\\incremental\\eminwon" in dir_lower:
        return "Eminwon"
    elif "/incremental/homepage" in dir_lower or "\\incremental\\homepage" in dir_lower:
        return "Homepage"
    elif "/incremental/btp" in dir_lower or "\\incremental\\btp" in dir_lower:
        return "Scraper"

    # ê¸°ì¡´ ê°œë°œ í™˜ê²½ ê²½ë¡œ ì²´í¬ (í•˜ìœ„ í˜¸í™˜ì„±)
    elif "scraped" in dir_lower:
        return "Homepage"
    elif "eminwon" in dir_lower:
        return "Eminwon"
    elif "data_dir" in dir_lower:
        return "Scraper"

    else:
        return "Unknown"


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description="ê³µê³  ì‚¬ì „ ì²˜ë¦¬ í”„ë¡œê·¸ë¨",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  python announcement_pre_processor.py -d scraped_data --site-code site001
  python announcement_pre_processor.py -d eminwon_data --site-code emw001
  python announcement_pre_processor.py -d scraped_data --site-code site001 --force
  python announcement_pre_processor.py -d eminwon_data --site-code emw001 --attach-force
        """,
    )

    parser.add_argument(
        "-d", "--directory", type=str, required=True, help="ë°ì´í„° ë””ë ‰í† ë¦¬ëª… (í•„ìˆ˜)"
    )

    parser.add_argument(
        "--site-code", type=str, required=True, help="ì‚¬ì´íŠ¸ ì½”ë“œ (í•„ìˆ˜)"
    )

    parser.add_argument(
        "--force", action="store_true", help="ì´ë¯¸ ì²˜ë¦¬ëœ í•­ëª©ë„ ë‹¤ì‹œ ì²˜ë¦¬"
    )

    parser.add_argument(
        "--attach-force",
        action="store_true",
        help="ì²¨ë¶€íŒŒì¼ ê°•ì œ ì¬ì²˜ë¦¬ (ê¸°ì¡´ .md íŒŒì¼ ë¬´ì‹œí•˜ê³  ì›ë³¸ íŒŒì¼ì—ì„œ ë‹¤ì‹œ ë³€í™˜)",
    )

    args = parser.parse_args()

    try:
        # ê¸°ë³¸ ë””ë ‰í† ë¦¬ ê²°ì •
        current_dir = Path.cwd()
        base_directory = current_dir / args.directory

        if not base_directory.exists():
            logger.error(f"ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {base_directory}")
            sys.exit(1)

        # site_type ê²°ì • (ì ˆëŒ€ ê²½ë¡œ ì‚¬ìš©)
        site_type = determine_site_type(str(base_directory), args.site_code)

        # Unknown site_type ê²€ì¦
        if site_type == "Unknown":
            logger.error(
                f"\n{'='*60}\n"
                f"âŒ site_typeì„ ê²°ì •í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n"
                f"{'='*60}\n"
                f"ì…ë ¥ ì •ë³´:\n"
                f"  - directory: {args.directory}\n"
                f"  - site_code: {args.site_code}\n"
                f"\n"
                f"ì§€ì›ë˜ëŠ” ë””ë ‰í† ë¦¬ íŒ¨í„´:\n"
                f"  í”„ë¡œë•ì…˜:\n"
                f"    - /incremental/api â†’ api_scrap\n"
                f"    - /incremental/eminwon â†’ Eminwon\n"
                f"    - /incremental/homepage â†’ Homepage\n"
                f"    - /incremental/btp â†’ Scraper\n"
                f"\n"
                f"  ê°œë°œ í™˜ê²½:\n"
                f"    - 'scraped' í¬í•¨ â†’ Homepage\n"
                f"    - 'eminwon' í¬í•¨ â†’ Eminwon\n"
                f"    - 'data_dir' í¬í•¨ â†’ Scraper\n"
                f"\n"
                f"  íŠ¹ìˆ˜ ì‚¬ì´íŠ¸:\n"
                f"    - site_code: kStartUp, bizInfo, smes24 â†’ api_scrap\n"
                f"{'='*60}\n"
            )
            sys.exit(1)

        logger.info(f"ê¸°ë³¸ ë””ë ‰í† ë¦¬: {base_directory}")
        logger.info(f"Site Type: {site_type}")
        logger.info(f"Site Code: {args.site_code}")

        # í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
        logger.info("ê³µê³  ì‚¬ì „ ì²˜ë¦¬ í”„ë¡œê·¸ë¨ ì‹œì‘")

        processor = AnnouncementPreProcessor(
            site_type=site_type,
            attach_force=args.attach_force,
            site_code=args.site_code,
            lazy_init=False,
        )

        # ì‚¬ì´íŠ¸ ë””ë ‰í† ë¦¬ ì²˜ë¦¬ ì‹¤í–‰
        results = processor.process_site_directories(
            base_directory, args.site_code, args.force
        )

        # ê²°ê³¼ ì¶œë ¥
        print(f"\n=== ìµœì¢… ìš”ì•½ ===")
        print(f"ì „ì²´ ëŒ€ìƒ: {results['total']}ê°œ")
        print(f"ì²˜ë¦¬ ì„±ê³µ: {results['success']}ê°œ")
        print(f"ì²˜ë¦¬ ì‹¤íŒ¨: {results['failed']}ê°œ")
        print(f"ê±´ë„ˆë›´ í•­ëª©: {results['skipped']}ê°œ")

        if results["failed"] > 0:
            print(
                f"\nì‹¤íŒ¨í•œ í•­ëª©ì´ {results['failed']}ê°œ ìˆìŠµë‹ˆë‹¤. ë¡œê·¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."
            )
            sys.exit(1)
        else:
            print("\nëª¨ë“  ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            sys.exit(0)

    except KeyboardInterrupt:
        logger.info("ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
        sys.exit(1)
    except Exception as e:
        logger.error(f"í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
