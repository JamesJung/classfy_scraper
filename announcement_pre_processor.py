#!/usr/bin/env python3
"""
ê³µê³  ì‚¬ì „ ì²˜ë¦¬ í”„ë¡œê·¸ë¨

ì‚¬ìš©ë²•:
    python announcement_pre_processor.py -d [ë””ë ‰í† ë¦¬ëª…] --site-code [ì‚¬ì´íŠ¸ì½”ë“œ]

ì˜ˆì‹œ:
    python announcement_pre_processor.py -d scraped_data --site-code site001
    python announcement_pre_processor.py -d eminwon_data --site-code emw001
"""

import argparse
import json
import os
import re
import sys
import time
import unicodedata
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python pathì— ì¶”ê°€
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.config.config import ConfigManager
from src.config.logConfig import setup_logging
from src.utils.attachmentProcessor import AttachmentProcessor
from src.models.announcementPrvDatabase import AnnouncementPrvDatabaseManager

logger = setup_logging(__name__)
config = ConfigManager().get_config()


class AnnouncementPreProcessor:
    """ê³µê³  ì‚¬ì „ ì²˜ë¦¬ ë©”ì¸ í´ë˜ìŠ¤"""

    def __init__(self, site_type: str, attach_force: bool = False):
        self.attachment_processor = AttachmentProcessor()
        self.db_manager = AnnouncementPrvDatabaseManager()
        self.attach_force = attach_force
        self.site_type = site_type

        # ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸” ìƒì„± (ì—†ëŠ” ê²½ìš°)
        self._ensure_database_tables()

        # ì œì™¸ í‚¤ì›Œë“œ ë¡œë“œ
        self.exclusion_keywords = self._load_exclusion_keywords()

    def _ensure_database_tables(self):
        """ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸”ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³  ìƒì„±í•©ë‹ˆë‹¤."""
        try:
            if self.db_manager.test_connection():
                # announcement_pre_processing í…Œì´ë¸” ìƒì„±
                from sqlalchemy import text

                with self.db_manager.SessionLocal() as session:
                    session.execute(
                        text(
                            """
                        CREATE TABLE IF NOT EXISTS announcement_pre_processing (
                            id INT PRIMARY KEY AUTO_INCREMENT,
                            folder_name VARCHAR(255) UNIQUE,
                            site_type VARCHAR(50),
                            site_code VARCHAR(50),
                            content_md LONGTEXT,
                            combined_content LONGTEXT,
                            attachment_filenames TEXT,
                            attachment_files_list JSON,
                            exclusion_keyword TEXT,
                            exclusion_reason TEXT,
                            title VARCHAR(500),
                            origin_url VARCHAR(1000),
                            announcement_date VARCHAR(50),
                            processing_status VARCHAR(50),
                            error_message TEXT,
                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
                            INDEX idx_site_code (site_code),
                            INDEX idx_processing_status (processing_status),
                            INDEX idx_origin_url (origin_url)
                        ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci
                    """
                        )
                    )
                    session.commit()
                logger.info("ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸” í™•ì¸/ìƒì„± ì™„ë£Œ")
            else:
                logger.warning(
                    "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨ - ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤ (DB ì €ì¥ ë¶ˆê°€)"
                )
        except Exception as e:
            logger.warning(
                f"ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e} - ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤ (DB ì €ì¥ ë¶ˆê°€)"
            )

    def _load_exclusion_keywords(self) -> List[Dict[str, Any]]:
        """ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì œì™¸ í‚¤ì›Œë“œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            from sqlalchemy import text

            with self.db_manager.SessionLocal() as session:
                result = session.execute(
                    text(
                        """
                    SELECT EXCLUSION_ID, KEYWORD, DESCRIPTION
                    FROM EXCLUSION_KEYWORDS
                    WHERE IS_ACTIVE = TRUE
                    ORDER BY EXCLUSION_ID
                """
                    )
                )

                keywords = []
                for row in result:
                    keywords.append(
                        {"id": row[0], "keyword": row[1], "description": row[2]}
                    )

                logger.info(f"ì œì™¸ í‚¤ì›Œë“œ ë¡œë“œ ì™„ë£Œ: {len(keywords)}ê°œ")
                return keywords

        except Exception as e:
            logger.warning(f"ì œì™¸ í‚¤ì›Œë“œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return []

    def process_directory(self, directory_path: Path, site_code: str) -> bool:
        """
        ë‹¨ì¼ ë””ë ‰í† ë¦¬ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.

        Args:
            directory_path: ì²˜ë¦¬í•  ë””ë ‰í† ë¦¬ ê²½ë¡œ
            site_code: ì‚¬ì´íŠ¸ ì½”ë“œ

        Returns:
            ì²˜ë¦¬ ì„±ê³µ ì—¬ë¶€
        """
        folder_name = directory_path.name
        return self.process_directory_with_custom_name(
            directory_path, site_code, folder_name
        )

    def _find_target_directories(
        self, base_dir: Path, site_code: str, force: bool = False
    ) -> List[Path]:
        """
        ì²˜ë¦¬í•  ëŒ€ìƒ ë””ë ‰í† ë¦¬ë“¤ì„ ì°¾ìŠµë‹ˆë‹¤.

        Args:
            base_dir: ê¸°ë³¸ ë””ë ‰í† ë¦¬
            site_code: ì‚¬ì´íŠ¸ ì½”ë“œ
            force: ì´ë¯¸ ì²˜ë¦¬ëœ í•­ëª©ë„ ë‹¤ì‹œ ì²˜ë¦¬í• ì§€ ì—¬ë¶€

        Returns:
            ì²˜ë¦¬ ëŒ€ìƒ ë””ë ‰í† ë¦¬ ëª©ë¡
        """
        site_dir = base_dir / site_code

        if not site_dir.exists():
            logger.error(f"ì‚¬ì´íŠ¸ ë””ë ‰í† ë¦¬ê°€ ì—†ìŒ: {site_dir}")
            return []

        target_directories = []

        # ëª¨ë“  í•˜ìœ„ ë””ë ‰í† ë¦¬ì—ì„œ content.md ë˜ëŠ” attachments í´ë”ê°€ ìˆëŠ” ë””ë ‰í† ë¦¬ ì°¾ê¸°
        logger.info(f"ë””ë ‰í† ë¦¬ ê²€ìƒ‰ ì‹œì‘: {site_dir}")

        for root_path in site_dir.rglob("*"):
            if root_path.is_dir():
                # content.md íŒŒì¼ì´ ìˆê±°ë‚˜ attachments í´ë”ê°€ ìˆëŠ” ë””ë ‰í† ë¦¬ë§Œ ëŒ€ìƒìœ¼ë¡œ í•¨
                has_content_md = (root_path / "content.md").exists()
                has_attachments = (root_path / "attachments").exists() and any(
                    (root_path / "attachments").iterdir()
                )

                if has_content_md or has_attachments:
                    target_directories.append(root_path)
                    # Windowsì™€ Unix ê²½ë¡œ í‘œì‹œ í†µì¼
                    rel_path_str = str(root_path.relative_to(site_dir)).replace("\\", "/")
                    logger.debug(f"ëŒ€ìƒ ë””ë ‰í† ë¦¬ ë°œê²¬: {rel_path_str}")

        # í´ë”ëª…ìœ¼ë¡œ ì •ë ¬
        target_directories = sorted(target_directories, key=self._natural_sort_key)

        logger.info(f"ë°œê²¬ëœ ì „ì²´ ë””ë ‰í† ë¦¬: {len(target_directories)}ê°œ")

        # ì²˜ìŒ ëª‡ ê°œ í´ë”ëª… ë¡œê¹…
        if target_directories:
            logger.info(f"ì²« 5ê°œ í´ë”: {[d.name for d in target_directories[:5]]}")

        # force ì˜µì…˜ì´ ì—†ì„ ë•Œë§Œ ì´ë¯¸ ì²˜ë¦¬ëœ í´ë” ì œì™¸
        if not force:
            processed_folders = set(self._get_processed_folders(site_code))

            filtered_directories = []
            for directory in target_directories:
                # ì‚¬ì´íŠ¸ ë””ë ‰í† ë¦¬ë¡œë¶€í„°ì˜ ìƒëŒ€ ê²½ë¡œë¥¼ í´ë”ëª…ìœ¼ë¡œ ì‚¬ìš©
                relative_path = directory.relative_to(site_dir)
                # Windowsì™€ Unix ê²½ë¡œ êµ¬ë¶„ì ëª¨ë‘ ì²˜ë¦¬
                path_str = str(relative_path).replace("\\", "_").replace("/", "_")
                folder_name = self._normalize_korean_text(path_str)

                if folder_name not in processed_folders:
                    filtered_directories.append(directory)
                else:
                    logger.debug(f"ì´ë¯¸ ì²˜ë¦¬ëœ í´ë” ê±´ë„ˆëœ€: {folder_name}")

            logger.info(f"ì „ì²´ ë°œê²¬ëœ ë””ë ‰í† ë¦¬: {len(target_directories)}ê°œ")
            logger.info(f"ì²˜ë¦¬ ëŒ€ìƒ ë””ë ‰í† ë¦¬: {len(filtered_directories)}ê°œ")
            logger.info(f"ì´ë¯¸ ì²˜ë¦¬ëœ í´ë”: {len(processed_folders)}ê°œ")

            return filtered_directories
        else:
            # force ì˜µì…˜ì´ ìˆìœ¼ë©´ ëª¨ë“  ë””ë ‰í† ë¦¬ ë°˜í™˜
            logger.info(
                f"--force ì˜µì…˜: ëª¨ë“  ë””ë ‰í† ë¦¬ ì²˜ë¦¬ ({len(target_directories)}ê°œ)"
            )
            return target_directories

    def _get_processed_folders(self, site_code: str) -> List[str]:
        """ì´ë¯¸ ì²˜ë¦¬ëœ í´ë” ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        try:
            from sqlalchemy import text

            with self.db_manager.SessionLocal() as session:
                result = session.execute(
                    text(
                        """
                    SELECT folder_name 
                    FROM announcement_pre_processing 
                    WHERE site_code = :site_code
                """
                    ),
                    {"site_code": site_code},
                )

                return [row[0] for row in result]

        except Exception as e:
            logger.error(f"ì²˜ë¦¬ëœ í´ë” ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []

    def process_site_directories(
        self, base_dir: Path, site_code: str, force: bool = False
    ) -> Dict[str, int]:
        """
        íŠ¹ì • ì‚¬ì´íŠ¸ì˜ ëª¨ë“  ë””ë ‰í† ë¦¬ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.

        Args:
            base_dir: ê¸°ë³¸ ë””ë ‰í† ë¦¬
            site_code: ì‚¬ì´íŠ¸ ì½”ë“œ
            force: ì´ë¯¸ ì²˜ë¦¬ëœ í•­ëª©ë„ ë‹¤ì‹œ ì²˜ë¦¬í• ì§€ ì—¬ë¶€

        Returns:
            ì²˜ë¦¬ ê²°ê³¼ í†µê³„
        """
        # ì²˜ë¦¬í•  ë””ë ‰í† ë¦¬ ëª©ë¡ ì°¾ê¸°
        target_directories = self._find_target_directories(base_dir, site_code, force)

        if not target_directories:
            logger.warning("ì²˜ë¦¬í•  ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {"total": 0, "success": 0, "failed": 0, "skipped": 0}

        total_count = len(target_directories)
        results = {"total": total_count, "success": 0, "failed": 0, "skipped": 0}
        site_dir = base_dir / site_code

        print(f"\n{'='*60}")
        print(
            f"ê³µê³  ì²˜ë¦¬ ì‹œì‘: {site_code} (Site Type: {self.site_type}) ({total_count}ê°œ í´ë”)"
        )
        print(f"{'='*60}")

        # ì‹œì‘ ì‹œê°„ ê¸°ë¡
        start_time = time.time()

        for i, directory in enumerate(target_directories, 1):
            try:
                # ê°œë³„ í•­ëª© ì‹œì‘ ì‹œê°„
                item_start_time = time.time()

                # ì‚¬ì´íŠ¸ ë””ë ‰í† ë¦¬ë¡œë¶€í„°ì˜ ìƒëŒ€ ê²½ë¡œë¥¼ í´ë”ëª…ìœ¼ë¡œ ì‚¬ìš©
                relative_path = directory.relative_to(site_dir)
                # Windowsì™€ Unix ê²½ë¡œ êµ¬ë¶„ì ëª¨ë‘ ì²˜ë¦¬
                path_str = str(relative_path).replace("\\", "_").replace("/", "_")
                folder_name = self._normalize_korean_text(path_str)

                progress_pct = (i / total_count) * 100
                print(f"\n[{i}/{total_count} : {progress_pct:.1f}%] {folder_name}")

                # ì´ë¯¸ ì²˜ë¦¬ëœ í•­ëª© í™•ì¸ (force ì˜µì…˜ì´ ì—†ì„ ë•Œë§Œ)
                if not force and self._is_already_processed(folder_name, site_code):
                    skip_elapsed = time.time() - item_start_time
                    print(f"  âœ“ ì´ë¯¸ ì²˜ë¦¬ë¨, ê±´ë„ˆëœ€ ({skip_elapsed:.1f}ì´ˆ)")
                    results["skipped"] += 1
                    continue
                elif force and self._is_already_processed(folder_name, site_code):
                    print("  ğŸ”„ ì´ë¯¸ ì²˜ë¦¬ë¨, --force ì˜µì…˜ìœ¼ë¡œ ì¬ì²˜ë¦¬")

                success = self.process_directory_with_custom_name(
                    directory, site_code, folder_name, force
                )

                # ê°œë³„ í•­ëª© ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
                item_elapsed = time.time() - item_start_time

                if success:
                    results["success"] += 1
                    print(f"  âœ“ ì²˜ë¦¬ ì™„ë£Œ ({item_elapsed:.1f}ì´ˆ)")
                else:
                    results["failed"] += 1
                    print(f"  âœ— ì²˜ë¦¬ ì‹¤íŒ¨ ({item_elapsed:.1f}ì´ˆ)")

            except Exception as e:
                error_elapsed = time.time() - item_start_time
                results["failed"] += 1
                print(f"  âœ— ì˜ˆì™¸ ë°œìƒ: {str(e)[:100]}... ({error_elapsed:.1f}ì´ˆ)")
                logger.error(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ({directory}): {e}")

        # ì¢…ë£Œ ì‹œê°„ ë° í†µê³„ ê³„ì‚°
        end_time = time.time()
        total_elapsed = end_time - start_time
        processed_count = results["success"] + results["failed"]

        print(f"\n{'='*60}")
        print(
            f"ì²˜ë¦¬ ì™„ë£Œ: {results['success']}/{total_count} ì„±ê³µ ({(results['success']/total_count)*100:.1f}%)"
        )
        print(f"ê±´ë„ˆëœ€: {results['skipped']}, ì‹¤íŒ¨: {results['failed']}")
        print(f"")
        print(f"ğŸ“Š ì²˜ë¦¬ ì‹œê°„ í†µê³„:")
        print(f"   ì´ ì†Œìš” ì‹œê°„: {total_elapsed:.1f}ì´ˆ ({total_elapsed/60:.1f}ë¶„)")

        if processed_count > 0:
            avg_time_per_item = total_elapsed / processed_count
            print(f"   ì²˜ë¦¬í•œ í•­ëª©ë‹¹ í‰ê·  ì‹œê°„: {avg_time_per_item:.1f}ì´ˆ")

        if results["success"] > 0:
            avg_time_per_success = total_elapsed / results["success"]
            print(f"   ì„±ê³µí•œ í•­ëª©ë‹¹ í‰ê·  ì‹œê°„: {avg_time_per_success:.1f}ì´ˆ")

        print(f"{'='*60}")

        logger.info(
            f"ì²˜ë¦¬ ì™„ë£Œ - ì „ì²´: {results['total']}, ì„±ê³µ: {results['success']}, ì‹¤íŒ¨: {results['failed']}, ê±´ë„ˆëœ€: {results['skipped']}"
        )

        return results

    def process_directory_with_custom_name(
        self,
        directory_path: Path,
        site_code: str,
        folder_name: str,
        force: bool = False,
    ) -> bool:
        """
        ì‚¬ìš©ì ì •ì˜ í´ë”ëª…ìœ¼ë¡œ ë””ë ‰í† ë¦¬ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.

        Args:
            directory_path: ì²˜ë¦¬í•  ë””ë ‰í† ë¦¬ ê²½ë¡œ
            site_code: ì‚¬ì´íŠ¸ ì½”ë“œ
            folder_name: ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•  í´ë”ëª…
            force: ì´ë¯¸ ì²˜ë¦¬ëœ í•­ëª©ë„ ë‹¤ì‹œ ì²˜ë¦¬í• ì§€ ì—¬ë¶€

        Returns:
            ì²˜ë¦¬ ì„±ê³µ ì—¬ë¶€
        """
        logger.info(f"ë””ë ‰í† ë¦¬ ì²˜ë¦¬ ì‹œì‘: {folder_name}")

        try:
            # 0. folder_name ì¤‘ë³µ ì²´í¬ (force ì˜µì…˜ì´ ì—†ì„ ë•Œë§Œ)
            if not force:
                if self._check_folder_name_exists(folder_name, site_code):
                    logger.info(f"ì´ë¯¸ ì²˜ë¦¬ëœ í´ë” ê±´ë„ˆëœ€: {folder_name}")
                    return True  # ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬ (ì´ë¯¸ ì²˜ë¦¬ë¨)

            # 1. ì œì™¸ í‚¤ì›Œë“œ ì²´í¬
            excluded_keywords = self._check_exclusion_keywords(folder_name)

            # 2. content.md íŒŒì¼ ì½ê¸°
            content_md_path = directory_path / "content.md"
            content_md = ""

            if content_md_path.exists():
                try:
                    with open(content_md_path, "r", encoding="utf-8") as f:
                        content_md = f.read()
                    logger.info(f"content.md ì½ê¸° ì™„ë£Œ: {len(content_md)} ë¬¸ì")
                except Exception as e:
                    logger.error(f"content.md ì½ê¸° ì‹¤íŒ¨: {e}")
                    return self._save_processing_result(
                        folder_name,
                        site_code,
                        content_md,
                        "",
                        status="error",
                        error_message=f"content.md ì½ê¸° ì‹¤íŒ¨: {e}",
                    )
            else:
                logger.warning(f"content.md íŒŒì¼ì´ ì—†ìŒ: {content_md_path}")

            # 3. content.mdë§Œìœ¼ë¡œ ê¸°ë³¸ ê²€ì¦
            if not content_md.strip():
                logger.warning("content.md ë‚´ìš©ì´ ì—†ìŒ")
                return self._save_processing_result(
                    folder_name,
                    site_code,
                    content_md,
                    "",
                    attachment_filenames=[],
                    status="error",
                    error_message="content.md ë‚´ìš©ì´ ì—†ìŒ",
                )

            title = self._extract_title_from_content(content_md) or "ì •ë³´ ì—†ìŒ"
            origin_url = (
                self._extract_origin_url_from_content(content_md) or "ì •ë³´ ì—†ìŒ"
            )
            announcement_date = (
                self._extract_announcement_date_from_content(content_md) or "ì •ë³´ ì—†ìŒ"
            )

            # 3.5. origin_url ì¤‘ë³µ ì²´í¬
            is_duplicate_url = False
            if origin_url and origin_url != "ì •ë³´ ì—†ìŒ":
                is_duplicate_url = self._check_origin_url_exists(origin_url, site_code)

            # 4. ì²¨ë¶€íŒŒì¼ ì²˜ë¦¬ (content.mdì™€ ë¶„ë¦¬)
            try:
                combined_content, attachment_filenames, attachment_files_info = (
                    self._process_attachments_separately(directory_path)
                )

                if not content_md.strip() and not combined_content.strip():
                    logger.warning("ì²˜ë¦¬í•  ë‚´ìš©ì´ ì—†ìŒ")
                    return self._save_processing_result(
                        folder_name,
                        site_code,
                        content_md,
                        combined_content,
                        attachment_filenames=attachment_filenames,
                        attachment_files_info=attachment_files_info,
                        status="error",
                        error_message="ì²˜ë¦¬í•  ë‚´ìš©ì´ ì—†ìŒ",
                    )

                logger.info(
                    f"ì²¨ë¶€íŒŒì¼ ë‚´ìš© ì²˜ë¦¬ ì™„ë£Œ: {len(combined_content)} ë¬¸ì, íŒŒì¼ {len(attachment_filenames)}ê°œ"
                )

            except Exception as e:
                logger.error(f"ì²¨ë¶€íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                return self._save_processing_result(
                    folder_name,
                    site_code,
                    content_md,
                    "",
                    attachment_filenames=[],
                    status="error",
                    error_message=f"ì²¨ë¶€íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}",
                )

            # 5. ì œì™¸ í‚¤ì›Œë“œê°€ ìˆëŠ” ê²½ìš° ì œì™¸ ì²˜ë¦¬
            if excluded_keywords:
                exclusion_msg = (
                    f"ì œì™¸ í‚¤ì›Œë“œê°€ ì…ë ¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤: {', '.join(excluded_keywords)}"
                )
                logger.info(f"ì œì™¸ ì²˜ë¦¬: {folder_name} - {exclusion_msg}")

                return self._save_processing_result(
                    folder_name,
                    site_code,
                    content_md,
                    combined_content,
                    attachment_filenames=attachment_filenames,
                    attachment_files_info=attachment_files_info,
                    status="ì œì™¸",
                    title=title,
                    announcement_date=announcement_date,
                    origin_url=origin_url,
                    exclusion_keywords=excluded_keywords,
                    exclusion_reason=exclusion_msg,
                )

            # 6. ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ (ì¤‘ë³µ URL ì—¬ë¶€ì— ë”°ë¼ ìƒíƒœ ê²°ì •)
            final_status = "ì¤‘ë³µ" if is_duplicate_url else "ì„±ê³µ"

            record_id = self._save_processing_result(
                folder_name,
                site_code,
                content_md,
                combined_content,
                attachment_filenames=attachment_filenames,
                attachment_files_info=attachment_files_info,
                title=title,
                announcement_date=announcement_date,
                origin_url=origin_url,
                status=final_status,
                force=force,
            )

            if is_duplicate_url:
                logger.info(f"origin_url ì¤‘ë³µìœ¼ë¡œ 'ì¤‘ë³µ' ìƒíƒœë¡œ ì €ì¥: {folder_name}")

            if record_id:
                logger.info(f"ë””ë ‰í† ë¦¬ ì²˜ë¦¬ ì™„ë£Œ: {folder_name}")
                return True
            else:
                logger.error(f"ë””ë ‰í† ë¦¬ ì²˜ë¦¬ ì‹¤íŒ¨: {folder_name}")
                return False

        except Exception as e:
            logger.error(f"ë””ë ‰í† ë¦¬ ì²˜ë¦¬ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
            result = self._save_processing_result(
                folder_name,
                site_code,
                "",
                "",
                status="error",
                error_message=f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}",
            )
            return result is not None

    def _check_exclusion_keywords(self, folder_name: str) -> List[str]:
        """í´ë”ëª…ì—ì„œ ì œì™¸ í‚¤ì›Œë“œë¥¼ ì²´í¬í•©ë‹ˆë‹¤."""
        matched_keywords = []

        for keyword_info in self.exclusion_keywords:
            keyword = keyword_info["keyword"].lower()
            if keyword in folder_name.lower():
                matched_keywords.append(keyword_info["keyword"])
                logger.debug(f"ì œì™¸ í‚¤ì›Œë“œ ë§¤ì¹­: '{keyword}' in '{folder_name}'")

        return matched_keywords

    def _check_folder_name_exists(self, folder_name: str, site_code: str) -> bool:
        """folder_nameì´ ë°ì´í„°ë² ì´ìŠ¤ì— ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
        try:
            from sqlalchemy import text

            with self.db_manager.SessionLocal() as session:
                result = session.execute(
                    text(
                        """
                    SELECT COUNT(*) FROM announcement_pre_processing 
                    WHERE folder_name = :folder_name AND site_code = :site_code
                """
                    ),
                    {"folder_name": folder_name, "site_code": site_code},
                )

                count = result.scalar()
                exists = count > 0

                if exists:
                    logger.debug(f"folder_name ì¤‘ë³µ ë°œê²¬: {folder_name}")

                return exists

        except Exception as e:
            logger.error(f"folder_name ì¤‘ë³µ ì²´í¬ ì‹¤íŒ¨: {e}")
            return False

    def _check_origin_url_exists(self, origin_url: str, site_code: str) -> bool:
        """origin_urlì´ ë°ì´í„°ë² ì´ìŠ¤ì— ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
        try:
            from sqlalchemy import text

            with self.db_manager.SessionLocal() as session:
                result = session.execute(
                    text(
                        """
                    SELECT COUNT(*) FROM announcement_pre_processing 
                    WHERE origin_url = :origin_url AND site_code = :site_code
                """
                    ),
                    {"origin_url": origin_url, "site_code": site_code},
                )

                count = result.scalar()
                exists = count > 0

                if exists:
                    logger.debug(f"origin_url ì¤‘ë³µ ë°œê²¬: {origin_url}")

                return exists

        except Exception as e:
            logger.error(f"origin_url ì¤‘ë³µ ì²´í¬ ì‹¤íŒ¨: {e}")
            return False

    def _is_already_processed(self, folder_name: str, site_code: str) -> bool:
        """í´ë”ê°€ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
        return self._check_folder_name_exists(folder_name, site_code)

    def _extract_title_from_content(self, content_md: str) -> str:
        """content.mdì—ì„œ ì œëª©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        if not content_md:
            return ""

        lines = content_md.split("\n")

        # ì²« ë²ˆì§¸ ë¹„ì–´ìˆì§€ ì•Šì€ ì¤„ì„ ì°¾ê¸°
        for line in lines[:10]:  # ìƒìœ„ 10ì¤„ë§Œ í™•ì¸
            line = line.strip()
            if line:
                # # ë§ˆí¬ë‹¤ìš´ í—¤ë” ì œê±°
                if line.startswith("#"):
                    title = line.lstrip("#").strip()
                    logger.debug(f"ë§ˆí¬ë‹¤ìš´ í—¤ë”ì—ì„œ ì œëª© ì¶”ì¶œ: {title}")
                    return title

                # **ì œëª©**: íŒ¨í„´ í™•ì¸ (ë§ˆí¬ë‹¤ìš´ ë³¼ë“œ)
                if line.startswith("**ì œëª©**:"):
                    title = line.replace("**ì œëª©**:", "").strip()
                    logger.debug(f"**ì œëª©** íŒ¨í„´ì—ì„œ ì œëª© ì¶”ì¶œ: {title}")
                    return title

                # ì œëª©:, ê³µê³ ëª…: íŒ¨í„´ í™•ì¸
                for prefix in ["ì œëª©:", "ê³µê³ ëª…:", "ê³µê³  ì œëª©:", "ì œëª© :"]:
                    if line.lower().startswith(prefix.lower()):
                        title = line[len(prefix) :].strip()
                        logger.debug(f"{prefix} íŒ¨í„´ì—ì„œ ì œëª© ì¶”ì¶œ: {title}")
                        return title

                # ì¼ë°˜ í…ìŠ¤íŠ¸ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ì œëª©ìœ¼ë¡œ ì‚¬ìš© (ì²« ë²ˆì§¸ ì¤„)
                logger.debug(f"ì²« ë²ˆì§¸ ì¤„ì„ ì œëª©ìœ¼ë¡œ ì‚¬ìš©: {line}")
                return line

        return ""

    def _extract_origin_url_from_content(self, content_md: str) -> str:
        """content.mdì—ì„œ ì›ë³¸ URLì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        if not content_md:
            return ""

        # ì›ë³¸ URL íŒ¨í„´ ì°¾ê¸°
        origin_patterns = [
            r"\*\*ì›ë³¸ URL\*\*[:\s]*(.+?)(?:\n|$)",
            r"ì›ë³¸ URL[:\s]*(.+?)(?:\n|$)",
            r"ì›ë³¸[:\s]*(.+?)(?:\n|$)",
            r"(https?://[^\s\)]+(?:\.go\.kr|\.or\.kr)[^\s\)]*)",
        ]

        for pattern in origin_patterns:
            matches = re.findall(pattern, content_md, re.IGNORECASE)
            if matches:
                url = matches[0].strip()
                if url and url.startswith("http"):
                    logger.debug(f"ì›ë³¸ URL ì¶”ì¶œ ì„±ê³µ: {url[:50]}...")
                    return url

        logger.debug("content.mdì—ì„œ ì›ë³¸ URLì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        return ""

    def _extract_announcement_date_from_content(self, content_md: str) -> str:
        """content.mdì—ì„œ ê³µê³ ì¼ì„ ë¬¸ìì—´ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        if not content_md:
            return ""

        # ì‘ì„±ì¼ íŒ¨í„´ ì°¾ê¸° (ë§ˆí¬ë‹¤ìš´ í˜•ì‹)
        date_patterns = [
            r"\*\*ì‘ì„±ì¼\*\*[:\s]*(.+?)(?:\n|$)",
            r"ì‘ì„±ì¼[:\s]*(.+?)(?:\n|$)",
            r"\*\*ë“±ë¡ì¼\*\*[:\s]*(.+?)(?:\n|$)",
            r"ë“±ë¡ì¼[:\s]*(.+?)(?:\n|$)",
            r"\*\*ê³µê³ ì¼\*\*[:\s]*(.+?)(?:\n|$)",
            r"ê³µê³ ì¼[:\s]*(.+?)(?:\n|$)",
        ]

        for pattern in date_patterns:
            matches = re.findall(pattern, content_md, re.IGNORECASE)
            if matches:
                date_str = matches[0].strip()
                if date_str:
                    logger.debug(f"ê³µê³ ì¼ ì¶”ì¶œ ì„±ê³µ: {date_str}")
                    return date_str

        logger.debug("content.mdì—ì„œ ê³µê³ ì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        return ""

    def _extract_attachment_urls_from_content(
        self, directory_path: Path
    ) -> Dict[str, str]:
        """content.mdì—ì„œ ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ URLì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        content_md_path = directory_path / "content.md"
        attachment_urls = {}

        if not content_md_path.exists():
            return attachment_urls

        try:
            with open(content_md_path, "r", encoding="utf-8") as f:
                content = f.read()

            # **ì²¨ë¶€íŒŒì¼**: ì„¹ì…˜ ì°¾ê¸°
            attachments_section = re.search(
                r"\*\*ì²¨ë¶€íŒŒì¼\*\*:\s*\n+(.*?)(?=\n\*\*|$)",
                content,
                re.DOTALL | re.MULTILINE,
            )

            if attachments_section:
                attachments_text = attachments_section.group(1)

                # ëª¨ë“  ì¤„ì„ ì²˜ë¦¬í•˜ì—¬ ë²ˆí˜¸. íŒŒì¼ëª…:URL íŒ¨í„´ ì°¾ê¸°
                lines = attachments_text.strip().split("\n")
                for line in lines:
                    line = line.strip()
                    if not line:
                        continue

                    # ë²ˆí˜¸. íŒŒì¼ëª…:URL íŒ¨í„´
                    match = re.match(r"^\d+\.\s*(.+?):(https?://[^\s]+)", line)
                    if match:
                        filename = match.group(1).strip()
                        url = match.group(2).strip()
                        attachment_urls[filename] = url
                        logger.debug(f"ì²¨ë¶€íŒŒì¼ URL ë§¤í•‘: {filename} -> {url[:50]}...")

            logger.info(
                f"ì²¨ë¶€íŒŒì¼ URL ì¶”ì¶œ ì™„ë£Œ: {len(attachment_urls)}ê°œ, í‚¤: {list(attachment_urls.keys())}"
            )

        except Exception as e:
            logger.error(f"ì²¨ë¶€íŒŒì¼ URL ì¶”ì¶œ ì‹¤íŒ¨: {e}")

        return attachment_urls

    def _normalize_korean_text(self, text: str) -> str:
        """í•œê¸€ í…ìŠ¤íŠ¸ë¥¼ NFC(Composed) í˜•íƒœë¡œ ì •ê·œí™”í•©ë‹ˆë‹¤."""
        return unicodedata.normalize("NFC", text)

    def _natural_sort_key(self, path: Path) -> tuple:
        """í´ë”ëª…ì˜ ìˆ«ì ë¶€ë¶„ì„ ê¸°ì¤€ìœ¼ë¡œ ìì—° ì •ë ¬ì„ ìœ„í•œ í‚¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        import re

        folder_name = path.name
        # ìˆ«ì_ì œëª© íŒ¨í„´ì—ì„œ ìˆ«ì ë¶€ë¶„ ì¶”ì¶œ
        match = re.match(r"^(\d+)_(.*)$", folder_name)
        if match:
            number = int(match.group(1))
            title = match.group(2)
            return (number, title)
        else:
            # ìˆ«ìë¡œ ì‹œì‘í•˜ì§€ ì•ŠëŠ” ê²½ìš°ëŠ” ë§¨ ë’¤ë¡œ
            return (float("inf"), folder_name)

    def _process_attachments_separately(
        self, directory_path: Path
    ) -> tuple[str, List[str], List[Dict[str, Any]]]:
        """ì²¨ë¶€íŒŒì¼ë“¤ì„ ì²˜ë¦¬í•˜ì—¬ ë‚´ìš©ì„ ê²°í•©í•˜ê³  íŒŒì¼ëª… ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        attachments_dir = directory_path / "attachments"

        if not attachments_dir.exists():
            return "", [], []

        combined_content = ""
        attachment_filenames = []
        attachment_files_info = []

        # content.mdì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ URL ì¶”ì¶œ
        attachment_urls = self._extract_attachment_urls_from_content(directory_path)

        # ì²˜ë¦¬ ê°€ëŠ¥í•œ í™•ì¥ì ì •ì˜ (Excel íŒŒì¼ ì œì™¸)
        supported_extensions = {
            ".pdf",
            ".hwp",
            ".hwpx",
            ".png",
            ".jpg",
            ".jpeg",
            ".gif",
            ".bmp",
            ".webp",
            ".pptx",
            ".docx",
            ".md",
        }

        target_keywords = ["ì–‘ì‹", "ì„œë¥˜", "ì‹ ì²­ì„œ", "ë™ì˜ì„œ"]

        for file_path in attachments_dir.iterdir():
            if file_path.is_file():
                file_extension = file_path.suffix.lower()
                filename = file_path.stem

                logger.info(f"filename===={filename}{file_extension}")
                lowercase_filename = filename.lower()

                if any(keyword in lowercase_filename for keyword in target_keywords):
                    logger.info(f"ì–‘ì‹, ì‹ ì²­ì„œ ë“±ì€ SKIP===={filename}")
                    continue

                # í™•ì¥ìê°€ ì—†ê±°ë‚˜ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ì€ ê±´ë„ˆë›°ê¸°
                if not file_extension or file_extension not in supported_extensions:
                    logger.info(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ ê±´ë„ˆëœ€: {file_path.name}")
                    continue

                attachment_filenames.append(self._normalize_korean_text(file_path.name))
                logger.info(f"ì²¨ë¶€íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: {file_path.name}")

                # md íŒŒì¼ì€ attachment_files_infoì— ì¶”ê°€í•˜ì§€ ì•ŠìŒ
                if file_extension != ".md":
                    # íŒŒì¼ ì •ë³´ ìˆ˜ì§‘
                    # URL ë§¤ì¹­ ì‹œë„ - íŒŒì¼ëª…ìœ¼ë¡œ ë¨¼ì € ì‹œë„, ì—†ìœ¼ë©´ stemìœ¼ë¡œ ì‹œë„
                    download_url = attachment_urls.get(file_path.name, "")
                    if not download_url:
                        # í™•ì¥ì ì—†ëŠ” ì´ë¦„ìœ¼ë¡œë„ ì‹œë„
                        download_url = attachment_urls.get(file_path.stem, "")

                    if download_url:
                        logger.debug(
                            f"URL ë§¤ì¹­ ì„±ê³µ: {file_path.name} -> {download_url[:50]}..."
                        )
                    else:
                        logger.debug(
                            f"URL ë§¤ì¹­ ì‹¤íŒ¨: {file_path.name}, ê°€ëŠ¥í•œ í‚¤: {list(attachment_urls.keys())[:3]}"
                        )

                    file_info = {
                        "filename": self._normalize_korean_text(file_path.name),  # í™•ì¥ì í¬í•¨ëœ ì „ì²´ íŒŒì¼ëª… (ì •ê·œí™”)
                        "file_size": (
                            file_path.stat().st_size if file_path.exists() else 0
                        ),
                        "conversion_success": False,
                        "conversion_method": self._guess_conversion_method(
                            file_extension
                        ),
                        "download_url": download_url,  # ë‹¤ìš´ë¡œë“œ URL ì¶”ê°€
                    }
                    attachment_files_info.append(file_info)

                # ì´ë¯¸ .md íŒŒì¼ì¸ ê²½ìš° ì§ì ‘ ì½ê¸°
                if file_extension == ".md":
                    try:
                        with open(file_path, "r", encoding="utf-8") as f:
                            content = f.read()
                        if content.strip():
                            combined_content += f"\n\n=== {self._normalize_korean_text(file_path.name)} ===\n{content}"
                            logger.info(
                                f"ì²¨ë¶€íŒŒì¼ .md ì§ì ‘ ì½ê¸° ì„±ê³µ: {file_path.name} ({len(content)} ë¬¸ì)"
                            )
                            file_info["conversion_success"] = True
                        else:
                            logger.warning(
                                f"ì²¨ë¶€íŒŒì¼ .md ë‚´ìš©ì´ ë¹„ì–´ìˆìŒ: {file_path.name}"
                            )
                    except Exception as e:
                        logger.error(f"ì²¨ë¶€íŒŒì¼ .md ì§ì ‘ ì½ê¸° ì‹¤íŒ¨: {e}")
                    continue

                # ì²¨ë¶€íŒŒì¼ëª….md íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
                md_file_path = attachments_dir / f"{filename}.md"
                logger.debug(f"md_file_path: {md_file_path}")

                # attach_forceê°€ Trueì´ë©´ ê¸°ì¡´ .md íŒŒì¼ì„ ë¬´ì‹œí•˜ê³  ì›ë³¸ì—ì„œ ì¬ë³€í™˜
                if not self.attach_force and md_file_path.exists():
                    # .md íŒŒì¼ì´ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ì½ìŒ
                    try:
                        with open(md_file_path, "r", encoding="utf-8") as f:
                            content = f.read()
                        if content.strip():
                            combined_content += f"\n\n=== {self._normalize_korean_text(filename)}.md ===\n{content}"
                            logger.debug(
                                f"ì²¨ë¶€íŒŒì¼ .md ì½ê¸° ì„±ê³µ: {filename}.md ({len(content)} ë¬¸ì)"
                            )
                            file_info["conversion_success"] = True
                        else:
                            logger.warning(
                                f"ì²¨ë¶€íŒŒì¼ .md ë‚´ìš©ì´ ë¹„ì–´ìˆìŒ: {filename}.md"
                            )
                    except Exception as e:
                        logger.error(f"ì²¨ë¶€íŒŒì¼ .md ì½ê¸° ì‹¤íŒ¨: {e}")
                else:
                    # .md íŒŒì¼ì´ ì—†ê±°ë‚˜ attach_forceê°€ Trueì´ë©´ ì›ë³¸ íŒŒì¼ì„ ë³€í™˜
                    if self.attach_force and md_file_path.exists():
                        logger.info(
                            f"--attach-force: ê¸°ì¡´ .md íŒŒì¼ ë¬´ì‹œí•˜ê³  ì¬ë³€í™˜: {file_path.name}"
                        )
                    else:
                        logger.info(f"ì²¨ë¶€íŒŒì¼ ë³€í™˜ ì‹œì‘: {file_path.name}")

                    try:
                        content = self.attachment_processor.process_single_file(
                            file_path
                        )

                        if content and content.strip():
                            combined_content += f"\n\n=== {self._normalize_korean_text(file_path.name)} ===\n{content}"
                            logger.info(
                                f"ì²¨ë¶€íŒŒì¼ ë³€í™˜ ì„±ê³µ: {file_path.name} ({len(content)} ë¬¸ì)"
                            )
                            file_info["conversion_success"] = True

                            # ë³€í™˜ëœ ë‚´ìš©ì„ .md íŒŒì¼ë¡œ ì €ì¥
                            try:
                                with open(md_file_path, "w", encoding="utf-8") as f:
                                    f.write(content)
                                logger.debug(
                                    f"ë³€í™˜ëœ ë‚´ìš©ì„ .mdë¡œ ì €ì¥: {md_file_path}"
                                )
                            except Exception as save_e:
                                logger.warning(f".md íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {save_e}")
                        else:
                            logger.warning(
                                f"ì²¨ë¶€íŒŒì¼ì—ì„œ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨: {file_path.name}"
                            )

                    except Exception as e:
                        logger.error(f"ì²¨ë¶€íŒŒì¼ ë³€í™˜ ì‹¤íŒ¨ ({file_path.as_posix()}): {e}")

        logger.info(
            f"ì²¨ë¶€íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ: {len(attachment_filenames)}ê°œ íŒŒì¼, {len(combined_content)} ë¬¸ì"
        )
        return combined_content.strip(), attachment_filenames, attachment_files_info

    def _guess_conversion_method(self, file_extension: str) -> str:
        """íŒŒì¼ í™•ì¥ìì— ë”°ë¥¸ ë³€í™˜ ë°©ë²•ì„ ì¶”ì •í•©ë‹ˆë‹¤."""
        ext_lower = file_extension.lower()

        if ext_lower == ".pdf":
            return "pdf_docling"
        elif ext_lower in [".hwp", ".hwpx"]:
            return "hwp_markdown"
        elif ext_lower in [".png", ".jpg", ".jpeg", ".gif", ".bmp", ".webp"]:
            return "ocr"
        else:
            return "unknown"

    def _save_processing_result(
        self,
        folder_name: str,
        site_code: str,
        content_md: str,
        combined_content: str,
        attachment_filenames: List[str] = None,
        status: str = "ì„±ê³µ",
        exclusion_keywords: List[str] = None,
        exclusion_reason: str = None,
        error_message: str = None,
        force: bool = False,
        title: str = None,
        origin_url: str = None,
        announcement_date: str = None,
        attachment_files_info: List[Dict[str, Any]] = None,
    ) -> Optional[int]:
        """ì²˜ë¦¬ ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤."""
        try:
            from sqlalchemy import text

            with self.db_manager.SessionLocal() as session:
                if force:
                    # UPSERT ë¡œì§
                    sql = text(
                        """
                        INSERT INTO announcement_pre_processing (
                            folder_name, site_type, site_code, content_md, combined_content,
                            attachment_filenames, attachment_files_list, exclusion_keyword, exclusion_reason, 
                            title, origin_url, announcement_date,
                            processing_status, error_message, created_at, updated_at
                        ) VALUES (
                            :folder_name, :site_type, :site_code, :content_md, :combined_content,
                            :attachment_filenames, :attachment_files_list, :exclusion_keyword, :exclusion_reason, 
                            :title, :origin_url, :announcement_date,
                            :processing_status, :error_message, NOW(), NOW()
                        )
                        ON DUPLICATE KEY UPDATE
                            site_type = VALUES(site_type),
                            content_md = VALUES(content_md),
                            combined_content = VALUES(combined_content),
                            attachment_filenames = VALUES(attachment_filenames),
                            attachment_files_list = VALUES(attachment_files_list),
                            exclusion_keyword = VALUES(exclusion_keyword),
                            exclusion_reason = VALUES(exclusion_reason),
                            processing_status = VALUES(processing_status),
                            title = VALUES(title),
                            origin_url = VALUES(origin_url),
                            announcement_date = VALUES(announcement_date),
                            error_message = VALUES(error_message),
                            updated_at = NOW()
                    """
                    )
                else:
                    # ì¼ë°˜ INSERT
                    sql = text(
                        """
                        INSERT INTO announcement_pre_processing (
                            folder_name, site_type, site_code, content_md, combined_content,
                            attachment_filenames, attachment_files_list, exclusion_keyword, exclusion_reason, 
                            title, origin_url, announcement_date,
                            processing_status, error_message, created_at, updated_at
                        ) VALUES (
                            :folder_name, :site_type, :site_code, :content_md, :combined_content,
                            :attachment_filenames, :attachment_files_list, :exclusion_keyword, :exclusion_reason, 
                            :title, :origin_url, :announcement_date,
                            :processing_status, :error_message, NOW(), NOW()
                        )
                    """
                    )

                # JSONìœ¼ë¡œ ì§ë ¬í™”
                attachment_files_json = (
                    json.dumps(attachment_files_info, ensure_ascii=False)
                    if attachment_files_info
                    else None
                )

                params = {
                    "folder_name": folder_name,
                    "site_type": self.site_type,
                    "site_code": site_code,
                    "content_md": content_md,
                    "combined_content": combined_content,
                    "attachment_filenames": (
                        ", ".join(attachment_filenames)
                        if attachment_filenames
                        else None
                    ),
                    "attachment_files_list": attachment_files_json,
                    "exclusion_keyword": (
                        ", ".join(exclusion_keywords) if exclusion_keywords else None
                    ),
                    "exclusion_reason": exclusion_reason,
                    "title": title,
                    "origin_url": origin_url,
                    "announcement_date": announcement_date,
                    "processing_status": status,
                    "error_message": error_message,
                }

                result = session.execute(sql, params)
                session.commit()

                record_id = result.lastrowid
                logger.info(f"ì²˜ë¦¬ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: ID {record_id}, ìƒíƒœ: {status}")
                return record_id

        except Exception as e:
            logger.error(f"ì²˜ë¦¬ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            return None


def determine_site_type(directory_name: str) -> str:
    """ë””ë ‰í† ë¦¬ëª…ì—ì„œ site_typeì„ ê²°ì •í•©ë‹ˆë‹¤."""
    if "scraped" in directory_name.lower():
        return "Homepage"
    elif "eminwon" in directory_name.lower():
        return "Eminwon"
    elif "data" in directory_name.lower():
        return "Scraper"
    else:
        return "Unknown"


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description="ê³µê³  ì‚¬ì „ ì²˜ë¦¬ í”„ë¡œê·¸ë¨",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  python announcement_pre_processor.py -d scraped_data --site-code site001
  python announcement_pre_processor.py -d eminwon_data --site-code emw001
  python announcement_pre_processor.py -d scraped_data --site-code site001 --force
  python announcement_pre_processor.py -d eminwon_data --site-code emw001 --attach-force
        """,
    )

    parser.add_argument(
        "-d", "--directory", type=str, required=True, help="ë°ì´í„° ë””ë ‰í† ë¦¬ëª… (í•„ìˆ˜)"
    )

    parser.add_argument(
        "--site-code", type=str, required=True, help="ì‚¬ì´íŠ¸ ì½”ë“œ (í•„ìˆ˜)"
    )

    parser.add_argument(
        "--force", action="store_true", help="ì´ë¯¸ ì²˜ë¦¬ëœ í•­ëª©ë„ ë‹¤ì‹œ ì²˜ë¦¬"
    )

    parser.add_argument(
        "--attach-force",
        action="store_true",
        help="ì²¨ë¶€íŒŒì¼ ê°•ì œ ì¬ì²˜ë¦¬ (ê¸°ì¡´ .md íŒŒì¼ ë¬´ì‹œí•˜ê³  ì›ë³¸ íŒŒì¼ì—ì„œ ë‹¤ì‹œ ë³€í™˜)",
    )

    args = parser.parse_args()

    try:
        # ê¸°ë³¸ ë””ë ‰í† ë¦¬ ê²°ì •
        current_dir = Path.cwd()
        base_directory = current_dir / args.directory

        if not base_directory.exists():
            logger.error(f"ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {base_directory}")
            sys.exit(1)

        # site_type ê²°ì •
        site_type = determine_site_type(args.directory)

        logger.info(f"ê¸°ë³¸ ë””ë ‰í† ë¦¬: {base_directory}")
        logger.info(f"Site Type: {site_type}")
        logger.info(f"Site Code: {args.site_code}")

        # í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
        logger.info("ê³µê³  ì‚¬ì „ ì²˜ë¦¬ í”„ë¡œê·¸ë¨ ì‹œì‘")
        processor = AnnouncementPreProcessor(
            site_type=site_type, attach_force=args.attach_force
        )

        # ì‚¬ì´íŠ¸ ë””ë ‰í† ë¦¬ ì²˜ë¦¬ ì‹¤í–‰
        results = processor.process_site_directories(
            base_directory, args.site_code, args.force
        )

        # ê²°ê³¼ ì¶œë ¥
        print(f"\n=== ìµœì¢… ìš”ì•½ ===")
        print(f"ì „ì²´ ëŒ€ìƒ: {results['total']}ê°œ")
        print(f"ì²˜ë¦¬ ì„±ê³µ: {results['success']}ê°œ")
        print(f"ì²˜ë¦¬ ì‹¤íŒ¨: {results['failed']}ê°œ")
        print(f"ê±´ë„ˆë›´ í•­ëª©: {results['skipped']}ê°œ")

        if results["failed"] > 0:
            print(
                f"\nì‹¤íŒ¨í•œ í•­ëª©ì´ {results['failed']}ê°œ ìˆìŠµë‹ˆë‹¤. ë¡œê·¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."
            )
            sys.exit(1)
        else:
            print("\nëª¨ë“  ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            sys.exit(0)

    except KeyboardInterrupt:
        logger.info("ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
        sys.exit(1)
    except Exception as e:
        logger.error(f"í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
