#!/usr/bin/env python3
import os
import sys
import subprocess
import time
import pymysql
from dotenv import load_dotenv
from datetime import datetime
from pathlib import Path
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
import json
import argparse

load_dotenv()

DB_HOST = os.getenv("DB_HOST")
DB_PORT = int(os.getenv("DB_PORT", 3306))
DB_USER = os.getenv("DB_USER")
DB_PASSWORD = os.getenv("DB_PASSWORD")
DB_NAME = os.getenv("DB_NAME")

# ì´ë©”ì¼ ì„¤ì •
SMTP_HOST = os.getenv("SMTP_HOST", "smtp.gmail.com")
SMTP_PORT = int(os.getenv("SMTP_PORT", 587))
SMTP_USER = os.getenv("SMTP_USER")
SMTP_PASSWORD = os.getenv("SMTP_PASSWORD")
ALERT_RECIPIENTS = os.getenv("ALERT_RECIPIENTS", "").split(",")
ALERT_SENDER_NAME = os.getenv("ALERT_SENDER_NAME", "Scraper Alert System")
ALERT_ENABLED = os.getenv("ALERT_ENABLED", "true").lower() == "true"

NODE_DIR = Path(__file__).parent / "node"
SCRAPER_DIR = NODE_DIR / "scraper"
BASE_OUTPUT_DIR = Path(__file__).parent / "scraped_incremental_v2"


def get_db_connection():
    return pymysql.connect(
        host=DB_HOST,
        port=DB_PORT,
        user=DB_USER,
        password=DB_PASSWORD,
        database=DB_NAME,
        charset="utf8mb4",
        cursorclass=pymysql.cursors.DictCursor,
    )


def save_scraper_log(
    site_code,
    status,
    elapsed_time=0,
    error_message=None,
    scraper_file=None,
    from_date=None,
    output_dir=None,
    scraped_count=0,
):
    """ìŠ¤í¬ë˜í¼ ì‹¤í–‰ ë¡œê·¸ë¥¼ DBì— ì €ì¥"""
    conn = get_db_connection()
    try:
        with conn.cursor() as cursor:
            # í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ ìƒì„±
            cursor.execute(
                """
                CREATE TABLE IF NOT EXISTS scraper_execution_log (
                    id INT PRIMARY KEY AUTO_INCREMENT,
                    site_code VARCHAR(50) NOT NULL,
                    execution_date DATE NOT NULL,
                    status VARCHAR(20) NOT NULL,
                    error_message TEXT,
                    elapsed_time FLOAT,
                    scraper_file VARCHAR(255),
                    from_date DATE,
                    to_date DATE,
                    output_dir VARCHAR(500),
                    scraped_count INT DEFAULT 0,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    INDEX idx_site_code (site_code),
                    INDEX idx_execution_date (execution_date),
                    INDEX idx_status (status)
                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci
            """
            )

            # ë¡œê·¸ ì €ì¥
            cursor.execute(
                """
                INSERT INTO scraper_execution_log 
                (site_code, execution_date, status, error_message, elapsed_time, 
                 scraper_file, from_date, to_date, output_dir, scraped_count)
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
            """,
                (
                    site_code,
                    datetime.now().date(),
                    status,
                    error_message,
                    elapsed_time,
                    scraper_file,
                    from_date,
                    datetime.now().date(),
                    output_dir,
                    scraped_count,
                ),
            )

            log_id = cursor.lastrowid
            conn.commit()
            return log_id
    except Exception as e:
        print(f"ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {e}")
        return None
    finally:
        conn.close()


def save_alert_history(log_id, site_code, alert_type, alert_message, recipients):
    """ì•Œë¦¼ ë°œì†¡ ê¸°ë¡ì„ DBì— ì €ì¥"""
    conn = get_db_connection()
    try:
        with conn.cursor() as cursor:
            # í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ ìƒì„±
            cursor.execute(
                """
                CREATE TABLE IF NOT EXISTS scraper_alert_history (
                    id INT PRIMARY KEY AUTO_INCREMENT,
                    log_id INT,
                    site_code VARCHAR(50) NOT NULL,
                    alert_type VARCHAR(20) NOT NULL,
                    alert_message TEXT,
                    recipients TEXT,
                    sent_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    INDEX idx_site_code (site_code),
                    INDEX idx_alert_type (alert_type)
                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci
            """
            )

            cursor.execute(
                """
                INSERT INTO scraper_alert_history 
                (log_id, site_code, alert_type, alert_message, recipients)
                VALUES (%s, %s, %s, %s, %s)
            """,
                (log_id, site_code, alert_type, alert_message, json.dumps(recipients)),
            )
            conn.commit()
    except Exception as e:
        print(f"ì•Œë¦¼ ê¸°ë¡ ì €ì¥ ì‹¤íŒ¨: {e}")
    finally:
        conn.close()


def send_summary_email(results, total_elapsed):
    """ì‹¤í–‰ ì™„ë£Œ í›„ ë¬¸ì œê°€ ìˆì—ˆë˜ ì‚¬ì´íŠ¸ë“¤ì— ëŒ€í•œ ì¢…í•© ë³´ê³ ì„œ ì´ë©”ì¼ ë°œì†¡"""
    if not ALERT_ENABLED or not SMTP_USER or not SMTP_PASSWORD:
        return False

    if not ALERT_RECIPIENTS or ALERT_RECIPIENTS == [""]:
        return False

    try:
        # ë¬¸ì œê°€ ìˆì—ˆë˜ ì‚¬ì´íŠ¸ë“¤ ì •ë¦¬
        problems = []

        for item in results.get("timeout", []):
            problems.append(
                {
                    "site": item["site_code"],
                    "status": "TIMEOUT",
                    "error": item.get("error", "15ë¶„ íƒ€ì„ì•„ì›ƒ ì´ˆê³¼"),
                    "time": item.get("elapsed_time", 0),
                }
            )

        for item in results.get("error", []):
            problems.append(
                {
                    "site": item["site_code"],
                    "status": "ERROR",
                    "error": item.get("error", "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜"),
                    "time": item.get("elapsed_time", 0),
                }
            )

        for item in results.get("failed", []):
            problems.append(
                {
                    "site": item["site_code"],
                    "status": "FAILED",
                    "error": item.get("error", "ì‹¤í–‰ ì‹¤íŒ¨"),
                    "time": item.get("elapsed_time", 0),
                }
            )

        if not problems:
            return False

        # HTML í…Œì´ë¸” ìƒì„±
        problem_rows = ""
        for p in problems:
            problem_rows += f"""
            <tr>
                <td>{p['site']}</td>
                <td style="color: {'red' if p['status'] == 'TIMEOUT' else 'orange'};">{p['status']}</td>
                <td>{p['error'][:100]}...</td>
                <td>{p['time']:.1f}ì´ˆ</td>
            </tr>
            """

        subject = f"[ìŠ¤í¬ë˜í¼ ì¼ì¼ ë³´ê³ ì„œ] {datetime.now().strftime('%Y-%m-%d')} - ë¬¸ì œ ë°œìƒ {len(problems)}ê±´"

        html_body = f"""
        <html>
            <head>
                <style>
                    body {{ font-family: Arial, sans-serif; }}
                    .summary-box {{
                        background-color: #f0f0f0;
                        padding: 15px;
                        margin: 10px 0;
                        border-radius: 5px;
                    }}
                    .problem-table {{
                        border-collapse: collapse;
                        width: 100%;
                        margin-top: 20px;
                    }}
                    .problem-table th {{
                        background-color: #333;
                        color: white;
                        padding: 10px;
                        text-align: left;
                    }}
                    .problem-table td {{
                        padding: 8px;
                        border-bottom: 1px solid #ddd;
                    }}
                    .stats-grid {{
                        display: grid;
                        grid-template-columns: repeat(3, 1fr);
                        gap: 10px;
                        margin: 20px 0;
                    }}
                    .stat-card {{
                        background: white;
                        border: 1px solid #ddd;
                        padding: 10px;
                        text-align: center;
                        border-radius: 5px;
                    }}
                    .stat-number {{
                        font-size: 24px;
                        font-weight: bold;
                        color: #333;
                    }}
                    .stat-label {{
                        color: #666;
                        font-size: 12px;
                        margin-top: 5px;
                    }}
                </style>
            </head>
            <body>
                <h2>ğŸ“Š ìŠ¤í¬ë˜í¼ ì¼ì¼ ì‹¤í–‰ ë³´ê³ ì„œ</h2>
                
                <div class="summary-box">
                    <h3>ì‹¤í–‰ ìš”ì•½</h3>
                    <div class="stats-grid">
                        <div class="stat-card">
                            <div class="stat-number">{len(results.get('success', []))}</div>
                            <div class="stat-label">ì„±ê³µ</div>
                        </div>
                        <div class="stat-card">
                            <div class="stat-number" style="color: orange;">{len(results.get('failed', []))}</div>
                            <div class="stat-label">ì‹¤íŒ¨</div>
                        </div>
                        <div class="stat-card">
                            <div class="stat-number" style="color: red;">{len(results.get('timeout', []))}</div>
                            <div class="stat-label">íƒ€ì„ì•„ì›ƒ</div>
                        </div>
                    </div>
                    <p>
                        <strong>ì‹¤í–‰ ì‹œê°„:</strong> {total_elapsed:.1f}ì´ˆ ({total_elapsed/60:.1f}ë¶„)<br>
                        <strong>ì™„ë£Œ ì‹œê°:</strong> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
                    </p>
                </div>
                
                <h3>âš ï¸ ë¬¸ì œ ë°œìƒ ì‚¬ì´íŠ¸ ({len(problems)}ê±´)</h3>
                <table class="problem-table">
                    <thead>
                        <tr>
                            <th>ì‚¬ì´íŠ¸ ì½”ë“œ</th>
                            <th>ìƒíƒœ</th>
                            <th>ì˜¤ë¥˜ ë©”ì‹œì§€</th>
                            <th>ì‹¤í–‰ ì‹œê°„</th>
                        </tr>
                    </thead>
                    <tbody>
                        {problem_rows}
                    </tbody>
                </table>
                
                <p style="margin-top: 30px; color: #666; font-size: 12px;">
                    ì´ ë©”ì¼ì€ ìë™ìœ¼ë¡œ ë°œì†¡ë˜ì—ˆìŠµë‹ˆë‹¤. ë¬¸ì˜ì‚¬í•­ì€ ì‹œìŠ¤í…œ ê´€ë¦¬ìì—ê²Œ ì—°ë½ì£¼ì„¸ìš”.
                </p>
            </body>
        </html>
        """

        # ì´ë©”ì¼ ê°ì²´ ìƒì„±
        msg = MIMEMultipart("alternative")
        msg["Subject"] = subject
        msg["From"] = f"{ALERT_SENDER_NAME} <{SMTP_USER}>"
        msg["To"] = ", ".join(
            [email.strip() for email in ALERT_RECIPIENTS if email.strip()]
        )

        # HTML íŒŒíŠ¸ ì¶”ê°€
        html_part = MIMEText(html_body, "html")
        msg.attach(html_part)

        # SMTP ì„œë²„ ì—°ê²° ë° ë°œì†¡
        with smtplib.SMTP(SMTP_HOST, SMTP_PORT) as server:
            server.starttls()
            server.login(SMTP_USER, SMTP_PASSWORD)
            server.send_message(msg)

        print(f"\nğŸ“§ ì¼ì¼ ì¢…í•© ë³´ê³ ì„œ ë°œì†¡ ì™„ë£Œ: {', '.join(ALERT_RECIPIENTS)}")
        return True

    except Exception as e:
        print(f"\nâŒ ì¢…í•© ë³´ê³ ì„œ ë°œì†¡ ì‹¤íŒ¨: {e}")
        return False


def send_alert_email(site_code, status, error_message, elapsed_time, from_date):
    """íƒ€ì„ì•„ì›ƒ ë˜ëŠ” ì˜¤ë¥˜ ë°œìƒì‹œ ì´ë©”ì¼ ì•Œë¦¼ ë°œì†¡"""
    if not ALERT_ENABLED or not SMTP_USER or not SMTP_PASSWORD:
        print("ì´ë©”ì¼ ì•Œë¦¼ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆê±°ë‚˜ ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤.")
        return False

    if not ALERT_RECIPIENTS or ALERT_RECIPIENTS == [""]:
        print("ìˆ˜ì‹ ì ì´ë©”ì¼ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return False

    try:
        # ì´ë©”ì¼ ë‚´ìš© êµ¬ì„±
        subject = f"[ìŠ¤í¬ë˜í¼ ì•Œë¦¼] {site_code} - {status}"

        html_body = f"""
        <html>
            <head>
                <style>
                    body {{ font-family: Arial, sans-serif; }}
                    .alert-box {{ 
                        border: 2px solid #ff0000;
                        padding: 15px;
                        margin: 10px 0;
                        background-color: #fff5f5;
                    }}
                    .info-table {{
                        border-collapse: collapse;
                        width: 100%;
                        margin-top: 10px;
                    }}
                    .info-table td {{
                        padding: 8px;
                        border: 1px solid #ddd;
                    }}
                    .info-table td:first-child {{
                        font-weight: bold;
                        background-color: #f2f2f2;
                        width: 30%;
                    }}
                </style>
            </head>
            <body>
                <h2>ìŠ¤í¬ë˜í¼ ì‹¤í–‰ ì•Œë¦¼</h2>
                <div class="alert-box">
                    <h3>âš ï¸ {status.upper()} ë°œìƒ</h3>
                    <table class="info-table">
                        <tr>
                            <td>ì‚¬ì´íŠ¸ ì½”ë“œ</td>
                            <td>{site_code}</td>
                        </tr>
                        <tr>
                            <td>ìƒíƒœ</td>
                            <td>{status}</td>
                        </tr>
                        <tr>
                            <td>ì‹œì‘ ë‚ ì§œ</td>
                            <td>{from_date}</td>
                        </tr>
                        <tr>
                            <td>ì‹¤í–‰ ì‹œê°„</td>
                            <td>{elapsed_time:.1f}ì´ˆ</td>
                        </tr>
                        <tr>
                            <td>ì˜¤ë¥˜ ë©”ì‹œì§€</td>
                            <td><pre>{error_message or 'N/A'}</pre></td>
                        </tr>
                        <tr>
                            <td>ë°œìƒ ì‹œê°</td>
                            <td>{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</td>
                        </tr>
                    </table>
                </div>
                <p>ì´ ë©”ì¼ì€ ìë™ìœ¼ë¡œ ë°œì†¡ë˜ì—ˆìŠµë‹ˆë‹¤.</p>
            </body>
        </html>
        """

        # ì´ë©”ì¼ ê°ì²´ ìƒì„±
        msg = MIMEMultipart("alternative")
        msg["Subject"] = subject
        msg["From"] = f"{ALERT_SENDER_NAME} <{SMTP_USER}>"
        msg["To"] = ", ".join(
            [email.strip() for email in ALERT_RECIPIENTS if email.strip()]
        )

        # HTML íŒŒíŠ¸ ì¶”ê°€
        html_part = MIMEText(html_body, "html")
        msg.attach(html_part)

        # SMTP ì„œë²„ ì—°ê²° ë° ë°œì†¡
        with smtplib.SMTP(SMTP_HOST, SMTP_PORT) as server:
            server.starttls()
            server.login(SMTP_USER, SMTP_PASSWORD)
            server.send_message(msg)

        print(f"  ğŸ“§ ì•Œë¦¼ ì´ë©”ì¼ ë°œì†¡ ì™„ë£Œ: {', '.join(ALERT_RECIPIENTS)}")
        return True

    except Exception as e:
        print(f"  âŒ ì´ë©”ì¼ ë°œì†¡ ì‹¤íŒ¨: {e}")
        return False


def get_sites_to_scrape(site_code=None):
    conn = get_db_connection()
    try:
        with conn.cursor() as cursor:
            if site_code:
                # íŠ¹ì • ì‚¬ì´íŠ¸ë§Œ ì¡°íšŒ
                cursor.execute(
                    """
                    SELECT 
                        site_code,
                        latest_announcement_date
                    FROM homepage_site_announcement_date
                    WHERE site_code = %s
                    """,
                    (site_code,)
                )
            else:
                # ëª¨ë“  ì‚¬ì´íŠ¸ ì¡°íšŒ
                cursor.execute(
                    """
                    SELECT 
                        site_code,
                        latest_announcement_date
                    FROM homepage_site_announcement_date
                    ORDER BY site_code
                    """
                )
            return cursor.fetchall()
    finally:
        conn.close()


def get_latest_date_from_scraped_files(site_code, output_dir):
    """ìŠ¤í¬ë˜í•‘ëœ íŒŒì¼ì—ì„œ ìµœì‹  ë‚ ì§œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    from pathlib import Path
    import re

    output_path = Path(output_dir)
    if not output_path.exists():
        return None

    # 001_ë¡œ ì‹œì‘í•˜ëŠ” ì²« ë²ˆì§¸ í´ë” ì°¾ê¸°
    first_dir = None
    for item_dir in sorted(output_path.iterdir()):
        if item_dir.is_dir() and item_dir.name.startswith("001_"):
            first_dir = item_dir
            break

    if not first_dir:
        # 001_ë¡œ ì‹œì‘í•˜ëŠ” í´ë”ê°€ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ë””ë ‰í† ë¦¬ ì‚¬ìš©
        dirs = [d for d in output_path.iterdir() if d.is_dir()]
        if dirs:
            first_dir = sorted(dirs)[0]
        else:
            return None

    # content.md íŒŒì¼ ì½ê¸°
    content_md_path = first_dir / "content.md"
    if not content_md_path.exists():
        print(f"  âš ï¸ content.md íŒŒì¼ ì—†ìŒ: {first_dir.name}")
        return None

    try:
        with open(content_md_path, "r", encoding="utf-8") as f:
            content = f.read()

        # ë‚ ì§œ ì¶”ì¶œ íŒ¨í„´ (ìš°ì„ ìˆœìœ„ ìˆœì„œ)
        date_patterns = [
            r"\*\*ì‘ì„±ì¼\*\*[:\s]*(.+?)(?:\n|$)",
            r"\*\*ë“±ë¡ì¼\*\*[:\s]*(.+?)(?:\n|$)",
            r"\*\*ê³µê³ ì¼\*\*[:\s]*(.+?)(?:\n|$)",
            r"ì‘ì„±ì¼[:\s]*(.+?)(?:\n|$)",
            r"ë“±ë¡ì¼[:\s]*(.+?)(?:\n|$)",
            r"ê³µê³ ì¼[:\s]*(.+?)(?:\n|$)",
            r"ë‚ ì§œ\s*[:\s]*(.+?)(?:\n|$)",  # ë‚ ì§œ: í˜•ì‹ ì¶”ê°€
            r"^\d{4}[-.ë…„]\d{1,2}[-.ì›”]\d{1,2}[ì¼]?$",  # ë‚ ì§œë§Œ ìˆëŠ” ë¼ì¸
        ]

        announcement_date = None
        for pattern in date_patterns:
            date_match = re.search(pattern, content, re.IGNORECASE | re.MULTILINE)
            if date_match:
                # íŒ¨í„´ì— ê·¸ë£¹ì´ ìˆëŠ” ê²½ìš°
                if date_match.groups():
                    announcement_date = date_match.group(1).strip()
                else:
                    announcement_date = date_match.group(0).strip()
                break

        if announcement_date:
            print(
                f"  ğŸ“„ íŒŒì¼ì—ì„œ ì¶”ì¶œí•œ ìµœì‹  ë‚ ì§œ: {announcement_date} (from {first_dir.name})"
            )
            return announcement_date
        else:
            print(f"  âš ï¸ ë‚ ì§œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {first_dir.name}")
            return None

    except Exception as e:
        print(f"  âŒ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
        return None


def update_latest_announcement_date(site_code, output_dir=None):
    """ìŠ¤í¬ë˜í•‘ ì™„ë£Œ í›„ í•´ë‹¹ ì‚¬ì´íŠ¸ì˜ ìµœì‹  ê³µê³  ë‚ ì§œë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""

    # ìŠ¤í¬ë˜í•‘ëœ íŒŒì¼ì—ì„œ ìµœì‹  ë‚ ì§œ ê°€ì ¸ì˜¤ê¸°
    latest_date_str = None
    if output_dir:
        latest_date_str = get_latest_date_from_scraped_files(site_code, output_dir)

    if not latest_date_str:
        print(f"  âš ï¸ ë‚ ì§œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ DB ì—…ë°ì´íŠ¸ ìŠ¤í‚µ")
        return False

    # ë‚ ì§œ í˜•ì‹ ë³€í™˜ (YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ)
    from datetime import datetime
    import re

    try:
        # ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ ì²˜ë¦¬
        date_obj = None

        # HH:MM:SS í˜•ì‹ (ì‹œê°„ë§Œ ìˆëŠ” ê²½ìš° - ì˜¤ëŠ˜ ë‚ ì§œë¡œ ì²˜ë¦¬)
        if re.match(r"^\d{1,2}:\d{2}:\d{2}$", latest_date_str):
            print(f"  âš ï¸ ì‹œê°„ë§Œ ìˆìŒ ({latest_date_str}), ì˜¤ëŠ˜ ë‚ ì§œë¡œ ì„¤ì •")
            date_obj = datetime.now()
        # YYYY-MM-DD í˜•ì‹
        elif re.match(r"^\d{4}-\d{2}-\d{2}$", latest_date_str):
            date_obj = datetime.strptime(latest_date_str, "%Y-%m-%d")
        # YYYY.MM.DD í˜•ì‹
        elif re.match(r"^\d{4}\.\d{2}\.\d{2}$", latest_date_str):
            date_obj = datetime.strptime(latest_date_str, "%Y.%m.%d")
        # YYYYMMDD í˜•ì‹
        elif re.match(r"^\d{8}$", latest_date_str):
            date_obj = datetime.strptime(latest_date_str, "%Y%m%d")
        # YYYYë…„ MMì›” DDì¼ í˜•ì‹ (ì‹œê°„ í¬í•¨ ê°€ëŠ¥)
        elif re.match(r"^\d{4}ë…„\s*\d{1,2}ì›”\s*\d{1,2}ì¼", latest_date_str):
            # ì‹œê°„ ë¶€ë¶„ ì œê±°í•˜ê³  ë‚ ì§œë§Œ ì¶”ì¶œ
            match = re.match(r"(\d{4})ë…„\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼", latest_date_str)
            if match:
                year, month, day = match.groups()
                formatted_date_str = f"{year}-{month.zfill(2)}-{day.zfill(2)}"
                date_obj = datetime.strptime(formatted_date_str, "%Y-%m-%d")
        else:
            print(f"  âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ë‚ ì§œ í˜•ì‹: {latest_date_str}")
            return False

        # YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        formatted_date = date_obj.strftime("%Y-%m-%d")

        # DB ì—…ë°ì´íŠ¸
        conn = get_db_connection()
        try:
            with conn.cursor() as cursor:
                cursor.execute(
                    """
                    UPDATE homepage_site_announcement_date
                    SET latest_announcement_date = %s
                    WHERE site_code = %s
                """,
                    (formatted_date, site_code),
                )

                conn.commit()
                print(f"  ğŸ“… DB ì—…ë°ì´íŠ¸ ì„±ê³µ: {site_code} â†’ {formatted_date}")
                return True
        finally:
            conn.close()

    except Exception as e:
        print(f"  âŒ DB ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {site_code} - {e}")
        return False


def check_scraper_exists(site_code):
    scraper_path = SCRAPER_DIR / f"{site_code}_scraper.js"
    return scraper_path.exists(), scraper_path


def run_scraper(site_code, from_date):
    start_time = time.time()

    exists, scraper_path = check_scraper_exists(site_code)

    if not exists:
        return {
            "site_code": site_code,
            "status": "skipped",
            "reason": f"ìŠ¤í¬ë˜í¼ íŒŒì¼ ì—†ìŒ: {scraper_path}",
            "elapsed_time": time.time() - start_time,
        }

    target_year = from_date.year
    from_date_str = from_date.strftime("%Y-%m-%d")
    today_str = datetime.now().strftime("%Y-%m-%d")

    # ìŠ¤í¬ë˜í¼ê°€ ë‚´ë¶€ì ìœ¼ë¡œ site_codeë¥¼ ì¶”ê°€í•˜ë¯€ë¡œ, ì—¬ê¸°ì„œëŠ” ë‚ ì§œ ë””ë ‰í† ë¦¬ê¹Œì§€ë§Œ ìƒì„±
    base_dir_for_date = BASE_OUTPUT_DIR / today_str
    base_dir_for_date.mkdir(parents=True, exist_ok=True)

    # ì‹¤ì œ output_dirëŠ” ìŠ¤í¬ë˜í¼ê°€ ìƒì„±í•  ê²ƒì´ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” base_dirë§Œ ì „ë‹¬
    expected_output_dir = base_dir_for_date / site_code  # ì˜ˆìƒ ì¶œë ¥ ë””ë ‰í† ë¦¬ (ë¡œê¹…ìš©)

    # ì´ë¯¸ ìŠ¤í¬ë˜í•‘ëœ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
    if expected_output_dir.exists():
        print(f"  ğŸ“ ê¸°ì¡´ ìŠ¤í¬ë˜í•‘ íŒŒì¼ ë°œê²¬: {expected_output_dir}")

        # ì²« ë²ˆì§¸ í´ë”ì˜ content.mdì—ì„œ ë‚ ì§œ í™•ì¸
        latest_scraped_date = get_latest_date_from_scraped_files(
            site_code, expected_output_dir
        )

        if latest_scraped_date:
            # DBì˜ ë‚ ì§œì™€ ë¹„êµ
            db_date_str = from_date_str

            # ë‚ ì§œ í˜•ì‹ í†µì¼í•˜ì—¬ ë¹„êµ
            try:
                # ìŠ¤í¬ë˜í•‘ëœ ë‚ ì§œë¥¼ YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                import re

                scraped_date_normalized = None

                if re.match(r"^\d{4}-\d{2}-\d{2}$", latest_scraped_date):
                    scraped_date_normalized = latest_scraped_date
                elif re.match(r"^\d{4}\.\d{2}\.\d{2}$", latest_scraped_date):
                    scraped_date_normalized = latest_scraped_date.replace(".", "-")
                elif re.match(r"^\d{8}$", latest_scraped_date):
                    scraped_date_normalized = f"{latest_scraped_date[:4]}-{latest_scraped_date[4:6]}-{latest_scraped_date[6:8]}"

                if scraped_date_normalized and scraped_date_normalized == db_date_str:
                    print(
                        f"  âŠ™ ì´ë¯¸ ìµœì‹  ë°ì´í„°: DB ë‚ ì§œ({db_date_str}) = ìŠ¤í¬ë˜í•‘ ë‚ ì§œ({scraped_date_normalized})"
                    )
                    return {
                        "site_code": site_code,
                        "status": "skipped",
                        "reason": f"ì´ë¯¸ ìµœì‹  ë°ì´í„° ë³´ìœ  (ë‚ ì§œ: {db_date_str})",
                        "elapsed_time": time.time() - start_time,
                    }
                else:
                    print(
                        f"  ğŸ“… ë‚ ì§œ ì°¨ì´ ë°œê²¬: DB({db_date_str}) != ìŠ¤í¬ë˜í•‘({scraped_date_normalized or latest_scraped_date})"
                    )
            except Exception as e:
                print(f"  âš ï¸ ë‚ ì§œ ë¹„êµ ì¤‘ ì˜¤ë¥˜: {e}")
    else:
        print(f"  ğŸ“‚ ì‹ ê·œ ìŠ¤í¬ë˜í•‘ (ì¶œë ¥ ë””ë ‰í† ë¦¬ ì—†ìŒ)")

    try:
        # ìŠ¤í¬ë˜í¼ì— ì „ë‹¬í•  arguments (named arguments í˜•ì‹)
        cmd = [
            "node",
            str(scraper_path),
            "--output",
            str(base_dir_for_date),  # ë‚ ì§œ ë””ë ‰í† ë¦¬ê¹Œì§€ë§Œ ì „ë‹¬
            "--date",
            from_date_str,  # ì‹œì‘ ë‚ ì§œ
            # "--site",
            # site_code,  # ì‚¬ì´íŠ¸ ì½”ë“œ
            # "--force",  # ê¸°ì¡´ í´ë” ë®ì–´ì“°ê¸°
        ]

        print(f"\n[{site_code}] ìŠ¤í¬ë˜í¼ ì‹¤í–‰")
        print(f"  ìŠ¤í¬ë˜í¼ íŒŒì¼: {scraper_path}")
        print(f"  ì‹œì‘ì¼: {from_date_str}")
        print(f"  ì¢…ë£Œì¼: {today_str}")
        print(f"  ê¸°ë³¸ ì¶œë ¥ ë””ë ‰í† ë¦¬: {base_dir_for_date}")
        # print(f"  ì˜ˆìƒ ìµœì¢… ë””ë ‰í† ë¦¬: {expected_output_dir}")
        # print(f"  ì‘ì—… ë””ë ‰í† ë¦¬: {NODE_DIR}")
        print(f"  ëª…ë ¹: {' '.join(cmd)}")

        # í™˜ê²½ë³€ìˆ˜ ì„¤ì • (í•„ìš”ì‹œ)
        env = os.environ.copy()
        env["NODE_ENV"] = "production"

        # ì‹¤ì‹œê°„ ì¶œë ¥ì„ ìœ„í•´ subprocess.Popen ì‚¬ìš©
        process = subprocess.Popen(
            cmd,
            cwd=str(NODE_DIR),
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,  # stderrë¥¼ stdoutìœ¼ë¡œ í•©ì¹¨
            text=True,
            bufsize=1,  # ë¼ì¸ ë²„í¼ë§
            env=env,
        )
        
        # ì‹¤ì‹œê°„ ë¡œê·¸ ì¶œë ¥
        stdout_lines = []
        for line in iter(process.stdout.readline, ''):
            if line:
                line = line.rstrip()
                print(f"  [{site_code}] {line}")  # ì‹¤ì‹œê°„ ì¶œë ¥
                stdout_lines.append(line)
        
        process.stdout.close()
        return_code = process.wait(timeout=1200)  # ìµœëŒ€ 20ë¶„ ëŒ€ê¸°
        
        # subprocess.runê³¼ ìœ ì‚¬í•œ ê²°ê³¼ ê°ì²´ ìƒì„±
        result = type('Result', (), {
            'returncode': return_code,
            'stdout': '\n'.join(stdout_lines),
            'stderr': ''  # stderrëŠ” stdoutìœ¼ë¡œ í•©ì³¤ìŒ
        })()

        if result.returncode == 0:
            # stdoutì´ ì—†ì–´ë„ ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ ì²´í¬
            scraped_count = 0
            if "scraped" in result.stdout.lower():
                # stdoutì—ì„œ ìŠ¤í¬ë˜í•‘ ê°œìˆ˜ ì¶”ì¶œ ì‹œë„
                import re

                match = re.search(
                    r"(\d+)\s*(?:items?|announcements?|ê³µê³ )", result.stdout
                )
                if match:
                    scraped_count = int(match.group(1))

            elapsed_time = time.time() - start_time
            return {
                "site_code": site_code,
                "status": "success",
                "output_dir": str(expected_output_dir),
                "scraped_count": scraped_count,
                "elapsed_time": elapsed_time,
                "stdout": (
                    result.stdout[-500:] if len(result.stdout) > 500 else result.stdout
                ),
            }
        else:
            elapsed_time = time.time() - start_time
            return {
                "site_code": site_code,
                "status": "failed",
                "returncode": result.returncode,
                "error": result.stderr if result.stderr else result.stdout,
                "elapsed_time": elapsed_time,
                "stdout": (
                    result.stdout[-500:] if len(result.stdout) > 500 else result.stdout
                ),
            }

    except subprocess.TimeoutExpired:
        elapsed_time = time.time() - start_time
        return {
            "site_code": site_code,
            "status": "timeout",
            "error": "15ë¶„ íƒ€ì„ì•„ì›ƒ ì´ˆê³¼",
            "elapsed_time": elapsed_time,
        }
    except Exception as e:
        elapsed_time = time.time() - start_time
        return {
            "site_code": site_code,
            "status": "error",
            "error": str(e),
            "elapsed_time": elapsed_time,
        }


def retry_failed_scrapers(failed_results, from_dates_map, max_retry=2):
    """
    ì‹¤íŒ¨í•œ ìŠ¤í¬ë˜í¼ë¥¼ ì¬ì‹œë„í•©ë‹ˆë‹¤.

    Args:
        failed_results: ì‹¤íŒ¨í•œ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ (failed, timeout, error ìƒíƒœ)
        from_dates_map: {site_code: from_date} ë§¤í•‘
        max_retry: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ (ê¸°ë³¸ê°’: 2)

    Returns:
        dict: {"success": [], "failed": []} í˜•íƒœì˜ ì¬ì‹œë„ ê²°ê³¼
    """
    if not failed_results:
        return {"success": [], "failed": []}

    print("\n" + "=" * 80)
    print("ì‹¤íŒ¨í•œ ìŠ¤í¬ë˜í¼ ì¬ì‹œë„ ì‹œì‘")
    print(f"ëŒ€ìƒ: {len(failed_results)}ê°œ")
    print("=" * 80)

    retry_results = {"success": [], "failed": []}

    for idx, failed_item in enumerate(failed_results, 1):
        site_code = failed_item["site_code"]
        from_date = from_dates_map.get(site_code)

        if not from_date:
            print(f"\n[ì¬ì‹œë„ {idx}/{len(failed_results)}] {site_code} - from_date ì—†ìŒ, ìŠ¤í‚µ")
            retry_results["failed"].append(failed_item)
            continue

        print(f"\n{'='*80}")
        print(f"[ì¬ì‹œë„ {idx}/{len(failed_results)}] {site_code}")
        print(f"ì›ë˜ ìƒíƒœ: {failed_item['status']}")
        print(f"{'='*80}")

        success = False
        last_result = None

        # ìµœëŒ€ max_retryë²ˆ ì¬ì‹œë„
        for retry_attempt in range(1, max_retry + 1):
            # ì§€ìˆ˜ ë°±ì˜¤í”„: 30ì´ˆ, 60ì´ˆ
            wait_time = 30 * retry_attempt

            print(f"\n  ì¬ì‹œë„ {retry_attempt}/{max_retry} - {wait_time}ì´ˆ ëŒ€ê¸° ì¤‘...")
            time.sleep(wait_time)

            print(f"  ì¬ì‹œë„ {retry_attempt}/{max_retry} ì‹¤í–‰ ì¤‘...")
            result = run_scraper(site_code, from_date)
            last_result = result

            if result["status"] == "success":
                print(f"  âœ“ ì¬ì‹œë„ ì„±ê³µ! (ì‹œë„ íšŸìˆ˜: {retry_attempt})")

                # DBì— ì„±ê³µ ë¡œê·¸ ì €ì¥ (ì¬ì‹œë„ ì„±ê³µì„ì„ í‘œì‹œ)
                elapsed = result.get("elapsed_time", 0)
                save_scraper_log(
                    site_code=site_code,
                    status=f"success_retry_{retry_attempt}",
                    elapsed_time=elapsed,
                    error_message=f"ì¬ì‹œë„ {retry_attempt}íšŒ ë§Œì— ì„±ê³µ",
                    scraper_file=f"{site_code}_scraper.js",
                    from_date=from_date,
                    output_dir=result.get("output_dir"),
                    scraped_count=result.get("scraped_count", 0),
                )

                # DB ì—…ë°ì´íŠ¸
                update_latest_announcement_date(site_code, result["output_dir"])

                retry_results["success"].append({
                    **result,
                    "retry_attempt": retry_attempt,
                    "original_status": failed_item["status"]
                })
                success = True
                break
            else:
                print(f"  âœ— ì¬ì‹œë„ {retry_attempt} ì‹¤íŒ¨: {result['status']} - {result.get('error', 'N/A')[:100]}")

                # ì¬ì‹œë„ ì‹¤íŒ¨ ë¡œê·¸ ì €ì¥
                elapsed = result.get("elapsed_time", 0)
                save_scraper_log(
                    site_code=site_code,
                    status=f"retry_{retry_attempt}_failed",
                    elapsed_time=elapsed,
                    error_message=result.get("error"),
                    scraper_file=f"{site_code}_scraper.js",
                    from_date=from_date,
                )

        # ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨
        if not success:
            print(f"\n  âœ— ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨ ({max_retry}íšŒ)")
            retry_results["failed"].append({
                **last_result,
                "retry_attempts": max_retry,
                "original_status": failed_item["status"]
            })

    print("\n" + "=" * 80)
    print("ì¬ì‹œë„ ê²°ê³¼ ìš”ì•½")
    print("=" * 80)
    print(f"ì¬ì‹œë„ ì„±ê³µ: {len(retry_results['success'])}ê°œ")
    print(f"ì¬ì‹œë„ ì‹¤íŒ¨: {len(retry_results['failed'])}ê°œ")
    print("=" * 80)

    return retry_results


def main():
    # ì»¤ë§¨ë“œë¼ì¸ ì¸ì íŒŒì‹±
    parser = argparse.ArgumentParser(description='í™ˆí˜ì´ì§€ ê³ ì‹œ/ê³µê³  ì ì§„ì  ìŠ¤í¬ë˜í•‘ v2')
    parser.add_argument('--site-code', type=str, help='íŠ¹ì • ì‚¬ì´íŠ¸ ì½”ë“œë§Œ ì²˜ë¦¬ (ì˜ˆ: --site-code acci)')
    args = parser.parse_args()
    
    total_start_time = time.time()

    print("=" * 80)
    print("í™ˆí˜ì´ì§€ ê³ ì‹œ/ê³µê³  ì ì§„ì  ìŠ¤í¬ë˜í•‘ v2")
    print(f"ì‹¤í–‰ ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    if args.site_code:
        print(f"ëŒ€ìƒ ì‚¬ì´íŠ¸: {args.site_code}")
    print("=" * 80)

    sites = get_sites_to_scrape(args.site_code)
    
    if not sites:
        if args.site_code:
            print(f"\nâŒ ì‚¬ì´íŠ¸ ì½”ë“œ '{args.site_code}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            print("\nâŒ ì²˜ë¦¬í•  ì‚¬ì´íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"\nì´ {len(sites)}ê°œ ì‚¬ì´íŠ¸ ëŒ€ìƒ")

    BASE_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

    results = {"success": [], "failed": [], "skipped": [], "timeout": [], "error": []}
    from_dates_map = {}  # ì¬ì‹œë„ë¥¼ ìœ„í•œ from_date ë§¤í•‘

    for idx, site in enumerate(sites, 1):
        site_code = site["site_code"]
        from_date = site["latest_announcement_date"]
        from_dates_map[site_code] = from_date  # from_date ì €ì¥

        print(f"\n{'='*80}")
        print(f"[{idx}/{len(sites)}] {site_code}")
        print(f"{'='*80}")

        result = run_scraper(site_code, from_date)
        status = result["status"]
        results[status].append(result)

        elapsed = result.get("elapsed_time", 0)

        # ëª¨ë“  ìƒíƒœì— ëŒ€í•´ ë¡œê·¸ ì €ì¥
        scraper_file = f"{site_code}_scraper.js"

        if status == "success":
            print(f"  âœ“ ì„±ê³µ: {result['output_dir']} (ì†Œìš”ì‹œê°„: {elapsed:.1f}ì´ˆ)")
            # ìŠ¤í¬ë˜í•‘ ì„±ê³µ ì‹œ DB ì—…ë°ì´íŠ¸
            update_latest_announcement_date(site_code, result["output_dir"])
            # ì„±ê³µ ë¡œê·¸ ì €ì¥
            save_scraper_log(
                site_code=site_code,
                status="success",
                elapsed_time=elapsed,
                scraper_file=scraper_file,
                from_date=from_date,
                output_dir=result.get("output_dir"),
                scraped_count=result.get("scraped_count", 0),
            )
        elif status == "skipped":
            print(f"  âŠ˜ ìŠ¤í‚µ: {result['reason']} (ì†Œìš”ì‹œê°„: {elapsed:.1f}ì´ˆ)")
            # ìŠ¤í‚µ ë¡œê·¸ ì €ì¥ (ì„ íƒì )
            save_scraper_log(
                site_code=site_code,
                status="skipped",
                elapsed_time=elapsed,
                error_message=result.get("reason"),
                scraper_file=scraper_file,
                from_date=from_date,
            )
        elif status == "failed":
            print(f"  âœ— ì‹¤íŒ¨: {result['error'][:200]} (ì†Œìš”ì‹œê°„: {elapsed:.1f}ì´ˆ)")
            # ì‹¤íŒ¨ ë¡œê·¸ ì €ì¥
            log_id = save_scraper_log(
                site_code=site_code,
                status="failed",
                elapsed_time=elapsed,
                error_message=result.get("error"),
                scraper_file=scraper_file,
                from_date=from_date,
            )
            # ì´ë©”ì¼ ì•Œë¦¼ ë°œì†¡ (ì„ íƒì )
            if ALERT_ENABLED:
                if send_alert_email(
                    site_code, "failed", result.get("error"), elapsed, from_date
                ):
                    save_alert_history(
                        log_id,
                        site_code,
                        "failure",
                        result.get("error"),
                        ALERT_RECIPIENTS,
                    )
        elif status == "timeout":
            print(f"  â± íƒ€ì„ì•„ì›ƒ: {result['error']} (ì†Œìš”ì‹œê°„: {elapsed:.1f}ì´ˆ)")
            # íƒ€ì„ì•„ì›ƒ ë¡œê·¸ ì €ì¥
            log_id = save_scraper_log(
                site_code=site_code,
                status="timeout",
                elapsed_time=elapsed,
                error_message=result.get("error"),
                scraper_file=scraper_file,
                from_date=from_date,
            )
            # íƒ€ì„ì•„ì›ƒì€ í•­ìƒ ì´ë©”ì¼ ì•Œë¦¼ ë°œì†¡
            if send_alert_email(
                site_code, "timeout", result.get("error"), elapsed, from_date
            ):
                save_alert_history(
                    log_id, site_code, "timeout", result.get("error"), ALERT_RECIPIENTS
                )
        elif status == "error":
            print(f"  âš  ì˜¤ë¥˜: {result['error'][:200]} (ì†Œìš”ì‹œê°„: {elapsed:.1f}ì´ˆ)")
            # ì˜¤ë¥˜ ë¡œê·¸ ì €ì¥
            log_id = save_scraper_log(
                site_code=site_code,
                status="error",
                elapsed_time=elapsed,
                error_message=result.get("error"),
                scraper_file=scraper_file,
                from_date=from_date,
            )
            # ì˜¤ë¥˜ë„ ì´ë©”ì¼ ì•Œë¦¼ ë°œì†¡
            if send_alert_email(
                site_code, "error", result.get("error"), elapsed, from_date
            ):
                save_alert_history(
                    log_id, site_code, "error", result.get("error"), ALERT_RECIPIENTS
                )

    # ====================================================================
    # ì¬ì‹œë„ ë¡œì§: ì‹¤íŒ¨/íƒ€ì„ì•„ì›ƒ/ì˜¤ë¥˜ ì‚¬ì´íŠ¸ ì¬ì‹œë„
    # ====================================================================
    failed_to_retry = results["failed"] + results["timeout"] + results["error"]

    if failed_to_retry:
        retry_results = retry_failed_scrapers(failed_to_retry, from_dates_map, max_retry=2)

        # ì¬ì‹œë„ ì„±ê³µí•œ í•­ëª©ì„ successë¡œ ì´ë™
        results["success"].extend(retry_results["success"])

        # ì›ë˜ ì‹¤íŒ¨ ë¦¬ìŠ¤íŠ¸ì—ì„œ ì¬ì‹œë„ ì„±ê³µí•œ í•­ëª© ì œê±°
        retry_success_site_codes = {r["site_code"] for r in retry_results["success"]}
        results["failed"] = [r for r in results["failed"] if r["site_code"] not in retry_success_site_codes]
        results["timeout"] = [r for r in results["timeout"] if r["site_code"] not in retry_success_site_codes]
        results["error"] = [r for r in results["error"] if r["site_code"] not in retry_success_site_codes]

    total_elapsed = time.time() - total_start_time

    print("\n" + "=" * 80)
    print("ìµœì¢… ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½")
    print("=" * 80)
    print(f"ì„±ê³µ: {len(results['success'])}ê°œ")
    print(f"ì‹¤íŒ¨: {len(results['failed'])}ê°œ")
    print(f"ìŠ¤í‚µ: {len(results['skipped'])}ê°œ")
    print(f"íƒ€ì„ì•„ì›ƒ: {len(results['timeout'])}ê°œ")
    print(f"ì˜¤ë¥˜: {len(results['error'])}ê°œ")

    if results["skipped"]:
        print(f"\nìŠ¤í‚µëœ ì‚¬ì´íŠ¸ ({len(results['skipped'])}ê°œ):")
        for r in results["skipped"][:10]:
            print(f"  - {r['site_code']}")
        if len(results["skipped"]) > 10:
            print(f"  ... ì™¸ {len(results['skipped']) - 10}ê°œ")

    print("\n" + "=" * 80)
    print(f"ì´ ì‹¤í–‰ ì‹œê°„: {total_elapsed:.1f}ì´ˆ ({total_elapsed/60:.1f}ë¶„)")
    print(f"ì¢…ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)

    # íƒ€ì„ì•„ì›ƒì´ë‚˜ ì˜¤ë¥˜ê°€ ìˆì—ˆë‹¤ë©´ ì¢…í•© ë³´ê³ ì„œ ì´ë©”ì¼ ë°œì†¡
    if (results["timeout"] or results["error"] or results["failed"]) and ALERT_ENABLED:
        send_summary_email(results, total_elapsed)


if __name__ == "__main__":
    main()
