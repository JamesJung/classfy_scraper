# ìŠ¤í¬ë˜í¼ URL ì¶”ì¶œ ë° ê²€ì¦ ì‹œìŠ¤í…œ

## ğŸ“‹ ê°œìš”

ìŠ¤í¬ë˜í•‘ ì „ì— ìƒì„¸ URLë§Œ ë¨¼ì € ì¶”ì¶œí•˜ì—¬ DBì— ì €ì¥í•˜ê³ , ì‹¤ì œ ìŠ¤í¬ë˜í•‘ ì‹œ ì¤‘ë³µ ì²´í¬ë¥¼ ìˆ˜í–‰í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

### ì£¼ìš” ê¸°ëŠ¥

1. **URLë§Œ ì¶”ì¶œ** - ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œë‚˜ content.md ìƒì„± ì—†ì´ URLë§Œ ì¶”ì¶œ
2. **URL ì •ê·œí™”** - page ê´€ë ¨ íŒŒë¼ë¯¸í„° ì œê±°í•˜ì—¬ ì¤‘ë³µ ë°©ì§€
3. **ì¤‘ë³µ ì²´í¬** - ì •ê·œí™”ëœ URLì˜ í•´ì‹œê°’ìœ¼ë¡œ ì¤‘ë³µ í™•ì¸
4. **ì§„í–‰ ìƒí™© ì¶”ì ** - ìŠ¤í¬ë˜í•‘ ì™„ë£Œ ì—¬ë¶€ ê¸°ë¡

---

## ğŸ—‚ï¸ êµ¬ì„± ìš”ì†Œ

### 1. DB í…Œì´ë¸”

```sql
CREATE TABLE scraper_detail_urls (
    id INT PRIMARY KEY AUTO_INCREMENT,
    batch_date DATE NOT NULL,
    site_code VARCHAR(50) NOT NULL,
    title VARCHAR(500) NULL,
    list_url TEXT NULL,
    detail_url TEXT NOT NULL,
    normalized_url TEXT NOT NULL,
    url_hash VARCHAR(64) NOT NULL,
    list_date VARCHAR(50) NULL,
    scraped TINYINT(1) DEFAULT 0,
    scraped_at TIMESTAMP NULL,
    UNIQUE KEY uk_site_url_hash (site_code, url_hash, batch_date)
);
```

**ìƒì„± ë°©ë²•:**
```bash
mysql -h localhost -P 3306 -u [ì‚¬ìš©ì] -p [ë°ì´í„°ë² ì´ìŠ¤] < create_detail_urls_table.sql
```

### 2. í•µì‹¬ ëª¨ë“ˆ

#### `url_manager.js`
URL ì •ê·œí™”, ì €ì¥, ì¤‘ë³µ ì²´í¬ë¥¼ ë‹´ë‹¹í•˜ëŠ” í•µì‹¬ ëª¨ë“ˆ

**ì£¼ìš” ë©”ì†Œë“œ:**
- `normalizeUrl(url)` - URL ì •ê·œí™” (page íŒŒë¼ë¯¸í„° ì œê±°)
- `hashUrl(url)` - URLì˜ SHA256 í•´ì‹œ ìƒì„±
- `saveDetailUrl(data)` - URL DB ì €ì¥
- `isDuplicate(site_code, detail_url, batch_date)` - ì¤‘ë³µ ì²´í¬
- `markAsScraped(site_code, detail_url, batch_date)` - ìŠ¤í¬ë˜í•‘ ì™„ë£Œ í‘œì‹œ
- `getUnscrapedUrls(site_code, batch_date, limit)` - ë¯¸ìŠ¤í¬ë˜í•‘ URL ì¡°íšŒ
- `getStats(site_code, batch_date)` - í†µê³„ ì¡°íšŒ

#### `andong_scraper.js` (í™•ì¥ë¨)
ìƒˆë¡œìš´ ë©”ì†Œë“œ ì¶”ê°€:
- `extractAndSaveUrls(batchDate)` - URLë§Œ ì¶”ì¶œí•˜ì—¬ DBì— ì €ì¥

---

## ğŸš€ ì‚¬ìš© ë°©ë²•

### Step 1: URL ì •ê·œí™” í…ŒìŠ¤íŠ¸

page íŒŒë¼ë¯¸í„°ê°€ ì˜¬ë°”ë¥´ê²Œ ì œê±°ë˜ëŠ”ì§€ í™•ì¸:

```bash
node test_url_normalization.js
```

**ì˜ˆìƒ ê²°ê³¼:**
```
âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼
âœ… page íŒŒë¼ë¯¸í„°ê°€ ë‹¤ë¥¸ URLë“¤ì˜ í•´ì‹œê°€ ë™ì¼
âœ… ê³ ìœ  ì‹ë³„ìê°€ ë‹¤ë¥¸ URLë“¤ì˜ í•´ì‹œê°€ ë‹¤ë¦„
```

### Step 2: URL ì¶”ì¶œ ë° ì €ì¥

íŠ¹ì • ì‚¬ì´íŠ¸ì˜ ìƒì„¸ URLì„ ì¶”ì¶œí•˜ì—¬ DBì— ì €ì¥:

```bash
# andong ì‚¬ì´íŠ¸ì˜ 2024ë…„ ê³µê³  URL ì¶”ì¶œ
node extract_urls.js --site andong --year 2024

# íŠ¹ì • ë‚ ì§œ ì´í›„ ê³µê³ ë§Œ ì¶”ì¶œ
node extract_urls.js --site andong --date 20240101

# ë°°ì¹˜ ë‚ ì§œ ì§€ì •
node extract_urls.js --site andong --year 2024 --batch-date 2025-01-15
```

**CLI ì˜µì…˜:**
- `--site, -s` : ì‚¬ì´íŠ¸ ì½”ë“œ (í•„ìˆ˜)
- `--url, -u` : ê¸°ë³¸ URL (í•„ìˆ˜)
- `--year, -y` : ëŒ€ìƒ ì—°ë„ (ê¸°ë³¸: í˜„ì¬ ì—°ë„)
- `--date, -d` : ëŒ€ìƒ ë‚ ì§œ YYYYMMDD (ì„ íƒ)
- `--batch-date, -b` : ë°°ì¹˜ ë‚ ì§œ YYYY-MM-DD (ê¸°ë³¸: ì˜¤ëŠ˜)
- `--list-selector` : ë¦¬ìŠ¤íŠ¸ ì„ íƒì
- `--title-selector` : ì œëª© ì„ íƒì
- `--date-selector` : ë‚ ì§œ ì„ íƒì

### Step 3: DB í™•ì¸

ì¶”ì¶œëœ URL í™•ì¸:

```sql
-- ì¶”ì¶œëœ URL í™•ì¸
SELECT
    site_code,
    title,
    detail_url,
    normalized_url,
    scraped,
    created_at
FROM scraper_detail_urls
WHERE site_code = 'andong'
  AND batch_date = '2025-01-15'
ORDER BY created_at DESC
LIMIT 10;

-- í†µê³„ í™•ì¸
SELECT
    site_code,
    batch_date,
    COUNT(*) as total,
    SUM(CASE WHEN scraped = 1 THEN 1 ELSE 0 END) as scraped,
    SUM(CASE WHEN scraped = 0 THEN 1 ELSE 0 END) as unscraped
FROM scraper_detail_urls
GROUP BY site_code, batch_date
ORDER BY batch_date DESC;

-- ì¤‘ë³µ í™•ì¸ (ê°™ì€ url_hashê°€ ì—¬ëŸ¬ ê°œ ìˆëŠ”ì§€)
SELECT
    url_hash,
    COUNT(*) as count,
    GROUP_CONCAT(title SEPARATOR ' | ') as titles
FROM scraper_detail_urls
WHERE site_code = 'andong'
  AND batch_date = '2025-01-15'
GROUP BY url_hash
HAVING count > 1;
```

### Step 4: ì‹¤ì œ ìŠ¤í¬ë˜í•‘ (í–¥í›„)

ì‹¤ì œ ìŠ¤í¬ë˜í•‘ ì‹œ ì¤‘ë³µ ì²´í¬ í™œìš©:

```javascript
const UrlManager = require('./node/scraper/url_manager');

// ìŠ¤í¬ë˜í•‘ ì „ ì¤‘ë³µ ì²´í¬
const isDuplicate = await UrlManager.isDuplicate('andong', detailUrl, batchDate);
if (isDuplicate) {
    console.log('ì´ë¯¸ ì¶”ì¶œëœ URL, ìŠ¤í‚µ');
    return;
}

// ìŠ¤í¬ë˜í•‘ í›„ ì™„ë£Œ í‘œì‹œ
await UrlManager.markAsScraped('andong', detailUrl, batchDate);
```

---

## ğŸ” URL ì •ê·œí™” ë¡œì§

### ì œê±°ë˜ëŠ” íŒŒë¼ë¯¸í„°

- **í˜ì´ì§€ ê´€ë ¨:** page, pageNum, pageNo, pageIndex, pageNumber, startPage, currentPage, p, pg, pn
- **ì˜¤í”„ì…‹ ê´€ë ¨:** offset, start, from
- **í¬ê¸° ê´€ë ¨:** pageSize, pagesize, size, limit
- **ìƒíƒœ ê´€ë ¨:** isManager, isCharge

### ì˜ˆì‹œ

```javascript
// ì›ë³¸ URL 1
https://www.gg.go.kr/bbs/boardView.do?bIdx=201528470&bsIdx=469&bcIdx=0&menuId=1547&isManager=false&isCharge=false&page=10

// ì›ë³¸ URL 2
https://www.gg.go.kr/bbs/boardView.do?bIdx=201528470&bsIdx=469&bcIdx=0&menuId=1547&isManager=false&isCharge=false&page=99

// ì •ê·œí™” ê²°ê³¼ (ë™ì¼)
https://www.gg.go.kr/bbs/boardView.do?bIdx=201528470&bsIdx=469&bcIdx=0&menuId=1547

// í•´ì‹œ (ë™ì¼)
d39d7e0e1521e495...
```

---

## ğŸ“Š ì›Œí¬í”Œë¡œìš°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. URL ì¶”ì¶œ            â”‚
â”‚  extract_urls.js        â”‚
â”‚  - ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ ìˆœíšŒ    â”‚
â”‚  - ìƒì„¸ URL ìƒì„±        â”‚
â”‚  - URL ì •ê·œí™”           â”‚
â”‚  - DB ì €ì¥              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  scraper_detail_urls    â”‚
â”‚  - detail_url           â”‚
â”‚  - normalized_url       â”‚
â”‚  - url_hash (SHA256)    â”‚
â”‚  - scraped = 0          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. ì‹¤ì œ ìŠ¤í¬ë˜í•‘        â”‚
â”‚  (í–¥í›„ êµ¬í˜„)             â”‚
â”‚  - ì¤‘ë³µ ì²´í¬             â”‚
â”‚  - ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ     â”‚
â”‚  - content.md ìƒì„±      â”‚
â”‚  - scraped = 1 ì—…ë°ì´íŠ¸ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§ª í…ŒìŠ¤íŠ¸

### URL ì •ê·œí™” í…ŒìŠ¤íŠ¸

```bash
node test_url_normalization.js
```

**í…ŒìŠ¤íŠ¸ í•­ëª©:**
- âœ… page íŒŒë¼ë¯¸í„° ì œê±°
- âœ… pageNum, pageIndex ë“± ë‹¤ì–‘í•œ ë³€í˜• ì œê±°
- âœ… ì¤‘ë³µ URL í•´ì‹œ ë™ì¼ í™•ì¸
- âœ… ë‹¤ë¥¸ URL í•´ì‹œ ë‹¤ë¦„ í™•ì¸

### ì‹¤ì œ URL ì¶”ì¶œ í…ŒìŠ¤íŠ¸ (ì†ŒëŸ‰)

```bash
# ìµœì‹  1í˜ì´ì§€ë§Œ ì¶”ì¶œ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)
node extract_urls.js --site andong --date 20250101
```

---

## ğŸ“ˆ í†µê³„ ë° ëª¨ë‹ˆí„°ë§

### ì¶”ì¶œ í˜„í™© ì¡°íšŒ

```javascript
const UrlManager = require('./node/scraper/url_manager');

const stats = await UrlManager.getStats('andong', '2025-01-15');
console.log(`ì „ì²´: ${stats.total}ê°œ`);
console.log(`ì™„ë£Œ: ${stats.scraped}ê°œ`);
console.log(`ëŒ€ê¸°: ${stats.unscraped}ê°œ`);
```

### ë¯¸ìŠ¤í¬ë˜í•‘ URL ì¡°íšŒ

```javascript
const unscraped = await UrlManager.getUnscrapedUrls('andong', '2025-01-15', 100);
console.log(`ë¯¸ìŠ¤í¬ë˜í•‘ URL ${unscraped.length}ê°œ:`);
unscraped.forEach(item => {
    console.log(`  - ${item.title}`);
    console.log(`    ${item.detail_url}`);
});
```

---

## ğŸ”§ ë‹¤ë¥¸ ì‚¬ì´íŠ¸ ì½”ë“œë¡œ í™•ì¥

### 1. ì‚¬ì´íŠ¸ë³„ ì„¤ì • ì¤€ë¹„

```bash
# ê²½ê¸°ë„ URL ì¶”ì¶œ
node extract_urls.js \
  --site gg \
  --url "https://www.gg.go.kr/bbs/boardList.do?bsIdx=469&menuId=1547" \
  --list-selector "table tbody tr" \
  --title-selector "td.subject a" \
  --date-selector "td.date" \
  --year 2024

# ê±°ì œì‹œ URL ì¶”ì¶œ
node extract_urls.js \
  --site geoje \
  --url "https://www.gjcity.go.kr/portal/saeol/gosi/list.do?mId=0202010000" \
  --list-selector "table.bod_list tbody tr" \
  --title-selector "td:nth-child(2) a" \
  --date-selector "td:nth-child(5)" \
  --year 2024
```

### 2. ë°°ì¹˜ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±

ì—¬ëŸ¬ ì‚¬ì´íŠ¸ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸:

```bash
#!/bin/bash
# extract_all_sites.sh

SITES=("andong" "gg" "geoje")
YEAR=2024
BATCH_DATE=$(date +%Y-%m-%d)

for SITE in "${SITES[@]}"; do
    echo "=== $SITE ì‚¬ì´íŠ¸ URL ì¶”ì¶œ ì‹œì‘ ==="
    node extract_urls.js --site $SITE --year $YEAR --batch-date $BATCH_DATE
    echo ""
done

echo "=== ì „ì²´ í†µê³„ ==="
mysql -h localhost -P 3306 -u root -p -e "
    SELECT
        site_code,
        COUNT(*) as total,
        SUM(CASE WHEN scraped = 1 THEN 1 ELSE 0 END) as scraped,
        SUM(CASE WHEN scraped = 0 THEN 1 ELSE 0 END) as unscraped
    FROM classfy.scraper_detail_urls
    WHERE batch_date = '$BATCH_DATE'
    GROUP BY site_code;
"
```

---

## ğŸ› íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ë¬¸ì œ 1: URL ì •ê·œí™” í›„ì—ë„ ì¤‘ë³µ ë°œìƒ

**ì›ì¸:** URL íŒŒë¼ë¯¸í„° ìˆœì„œê°€ ë‹¤ë¦„

**í•´ê²°:** UrlManagerì˜ `normalizeUrl()` ë©”ì†Œë“œê°€ URLSearchParamsë¥¼ ì‚¬ìš©í•˜ì—¬ ìë™ìœ¼ë¡œ ì •ë ¬í•©ë‹ˆë‹¤.

### ë¬¸ì œ 2: UNIQUE ì œì•½ì¡°ê±´ ìœ„ë°˜

**ì›ì¸:** ê°™ì€ URLì„ ì¬ì¶”ì¶œ

**í•´ê²°:** `ON DUPLICATE KEY UPDATE`ë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ì¡´ ë ˆì½”ë“œë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.

### ë¬¸ì œ 3: page ì™¸ ë‹¤ë¥¸ íŒŒë¼ë¯¸í„°ë„ ì œê±° í•„ìš”

**í•´ê²°:** `url_manager.js`ì˜ `pageParams` ë°°ì—´ì— ì¶”ê°€:

```javascript
const pageParams = [
    'page', 'pageNum', // ê¸°ì¡´
    'yourParam' // ì¶”ê°€
];
```

---

## ğŸ“ TODO

- [ ] ì‹¤ì œ ìŠ¤í¬ë˜í•‘ ì‹œ ì¤‘ë³µ ì²´í¬ í†µí•©
- [ ] ë¯¸ìŠ¤í¬ë˜í•‘ URL ì¬ì‹œë„ ë¡œì§
- [ ] URL ì¶”ì¶œ ì‹¤íŒ¨ ë¡œê·¸ ìˆ˜ì§‘
- [ ] ì‚¬ì´íŠ¸ë³„ URL íŒ¨í„´ ìë™ ê°ì§€
- [ ] ëŒ€ëŸ‰ ì‚¬ì´íŠ¸ ë³‘ë ¬ ì²˜ë¦¬

---

## ğŸ“š ê´€ë ¨ ë¬¸ì„œ

- [SCRAPER_FAILURE_RETRY_README.md](./SCRAPER_FAILURE_RETRY_README.md) - ì‹¤íŒ¨ ê³µê³  ì¬ì‹œë„ ì‹œìŠ¤í…œ
- [create_detail_urls_table.sql](./create_detail_urls_table.sql) - DB í…Œì´ë¸” ìƒì„±
- [test_url_normalization.js](./test_url_normalization.js) - URL ì •ê·œí™” í…ŒìŠ¤íŠ¸

---

## âš™ï¸ í™˜ê²½ ì„¤ì •

`.env` íŒŒì¼ì— DB ì„¤ì • í•„ìš”:

```env
DB_HOST=localhost
DB_PORT=3306
DB_USER=root
DB_PASSWORD=your_password
DB_NAME=subvention
```
