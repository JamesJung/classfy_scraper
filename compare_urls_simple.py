#!/usr/bin/env python3

def load_db_urls(file_path):
    """DBì—ì„œ ê°€ì ¸ì˜¨ URL íŒŒì¼ ì½ê¸°"""
    db_dict = {}
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
            # ì²« ë²ˆì§¸ ì¤„ì€ í—¤ë”ì´ë¯€ë¡œ ê±´ë„ˆë›°ê¸°
            for line in lines[1:]:
                line = line.strip()
                if not line:
                    continue
                parts = line.split('\t')
                if len(parts) >= 2:
                    site_code = parts[0]
                    announcement_url = parts[1]
                    db_dict[site_code] = announcement_url

        print(f"âœ… DBì—ì„œ {len(db_dict)}ê°œ ë ˆì½”ë“œ ë¡œë“œ")
        return db_dict
    except Exception as e:
        print(f"âŒ DB íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
        return {}

def load_site_url_file(file_path):
    """site_url.txt íŒŒì¼ ì½ê¸°"""
    site_url_dict = {}
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                parts = line.split('\t')
                if len(parts) >= 2:
                    site_code = parts[0]
                    list_url = parts[1]
                    site_url_dict[site_code] = list_url

        print(f"âœ… site_url.txtì—ì„œ {len(site_url_dict)}ê°œ ë ˆì½”ë“œ ë¡œë“œ")
        return site_url_dict
    except Exception as e:
        print(f"âŒ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
        return {}

def normalize_url(url):
    """URL ì •ê·œí™” (ë¹„êµë¥¼ ìœ„í•´)"""
    if not url:
        return ""
    # ëì˜ ìŠ¬ë˜ì‹œ ì œê±°
    url = url.rstrip('/')
    # httpì™€ https í†µì¼
    url = url.replace('http://', 'https://')
    return url

def compare_urls(db_data, file_data):
    """URL ë¹„êµ"""
    print("\n" + "="*120)
    print("URL ë¹„êµ ê²°ê³¼")
    print("="*120)

    # 1. DBì—ë§Œ ìˆëŠ” ì‚¬ì´íŠ¸
    db_only = set(db_data.keys()) - set(file_data.keys())
    print(f"\nğŸ“Š DBì—ë§Œ ìˆëŠ” ì‚¬ì´íŠ¸ ({len(db_only)}ê°œ):")
    if len(db_only) > 0:
        for site_code in sorted(db_only):
            print(f"  - {site_code}: {db_data[site_code]}")
    else:
        print("  (ì—†ìŒ)")

    # 2. íŒŒì¼ì—ë§Œ ìˆëŠ” ì‚¬ì´íŠ¸
    file_only = set(file_data.keys()) - set(db_data.keys())
    print(f"\nğŸ“Š site_url.txtì—ë§Œ ìˆëŠ” ì‚¬ì´íŠ¸ ({len(file_only)}ê°œ):")
    if len(file_only) > 0:
        for site_code in sorted(file_only):
            print(f"  - {site_code}: {file_data[site_code]}")
    else:
        print("  (ì—†ìŒ)")

    # 3. ì–‘ìª½ ëª¨ë‘ ìˆëŠ” ì‚¬ì´íŠ¸
    common_sites = set(db_data.keys()) & set(file_data.keys())
    different_urls = []
    same_urls = []

    for site_code in common_sites:
        db_url = normalize_url(db_data[site_code])
        file_url = normalize_url(file_data[site_code])

        if db_url != file_url:
            different_urls.append((site_code, db_data[site_code], file_data[site_code]))
        else:
            same_urls.append(site_code)

    print(f"\nğŸ“Š ì–‘ìª½ ëª¨ë‘ ìˆëŠ” ì‚¬ì´íŠ¸ ì¤‘ URLì´ ë‹¤ë¥¸ ê²½ìš° ({len(different_urls)}ê°œ):")
    if len(different_urls) > 0:
        for site_code, db_url, file_url in sorted(different_urls):
            print(f"\n  ì‚¬ì´íŠ¸: {site_code}")
            print(f"    DB:   {db_url}")
            print(f"    File: {file_url}")
    else:
        print("  (ì—†ìŒ)")

    print(f"\nâœ… ì–‘ìª½ ëª¨ë‘ ìˆê³  URLì´ ë™ì¼í•œ ì‚¬ì´íŠ¸: {len(same_urls)}ê°œ")

    # ìš”ì•½
    print("\n" + "="*120)
    print("ğŸ“Š ìš”ì•½")
    print("="*120)
    print(f"DBì—ë§Œ ìˆìŒ:                 {len(db_only):>4}ê°œ")
    print(f"íŒŒì¼ì—ë§Œ ìˆìŒ:               {len(file_only):>4}ê°œ")
    print(f"URL ë‹¤ë¦„:                    {len(different_urls):>4}ê°œ")
    print(f"URL ë™ì¼:                    {len(same_urls):>4}ê°œ")
    print(f"ì´ ê³µí†µ ì‚¬ì´íŠ¸:              {len(common_sites):>4}ê°œ")
    print("-"*120)
    print(f"ì´ DB ë ˆì½”ë“œ:                {len(db_data):>4}ê°œ")
    print(f"ì´ íŒŒì¼ ë ˆì½”ë“œ:              {len(file_data):>4}ê°œ")
    print("="*120)

    # ìƒì„¸ í†µê³„
    print("\n" + "="*120)
    print("ğŸ“Š ìƒì„¸ í†µê³„")
    print("="*120)
    total_unique_sites = len(set(db_data.keys()) | set(file_data.keys()))
    print(f"ì´ ê³ ìœ  ì‚¬ì´íŠ¸ ìˆ˜:           {total_unique_sites:>4}ê°œ")
    print(f"DB ì»¤ë²„ë¦¬ì§€:                 {len(db_data)/total_unique_sites*100:>5.1f}%")
    print(f"íŒŒì¼ ì»¤ë²„ë¦¬ì§€:               {len(file_data)/total_unique_sites*100:>5.1f}%")
    if len(common_sites) > 0:
        print(f"URL ì¼ì¹˜ìœ¨ (ê³µí†µ ì‚¬ì´íŠ¸):    {len(same_urls)/len(common_sites)*100:>5.1f}%")
    print("="*120)

def main():
    # DB ë°ì´í„° ë¡œë“œ
    db_data = load_db_urls('/tmp/db_urls.txt')

    # site_url.txt íŒŒì¼ ë¡œë“œ
    file_data = load_site_url_file('/Users/jin/classfy_scraper/site_url.txt')

    # URL ë¹„êµ
    compare_urls(db_data, file_data)

if __name__ == '__main__':
    main()
