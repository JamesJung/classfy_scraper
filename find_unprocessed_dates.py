#!/usr/bin/env python3
"""
incremental ë””ë ‰í† ë¦¬ì—ì„œ ë¯¸ì²˜ë¦¬ëœ ë‚ ì§œ í´ë”ë¥¼ ì°¾ëŠ” ìŠ¤í¬ë¦½íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ”:
1. /home/zium/moabojo/incremental/btp/, eminwon/, homepage/ ë””ë ‰í† ë¦¬ë¥¼ ìŠ¤ìº”
2. ê° ë‚ ì§œ í´ë”ì˜ ê³µê³  ê°œìˆ˜ë¥¼ ê³„ì‚°
3. DBì˜ announcement_pre_processing í…Œì´ë¸”ê³¼ ë¹„êµ
4. ë¯¸ë“±ë¡ëœ ë°ì´í„°ê°€ ìˆëŠ” ë‚ ì§œ í´ë”ë¥¼ ë¦¬í¬íŠ¸

ì‚¬ìš©ë²•:
  python3 find_unprocessed_dates.py [--days N] [--source all|btp|eminwon|homepage]

ì˜µì…˜:
  --days N    : ìµœê·¼ Nì¼ ì´ë‚´ì˜ í´ë”ë§Œ ê²€ì‚¬ (ê¸°ë³¸: 30ì¼)
  --source    : ê²€ì‚¬í•  ì†ŒìŠ¤ (ê¸°ë³¸: all)
  --report    : ìƒì„¸ ë¦¬í¬íŠ¸ ì¶œë ¥
"""

import os
import sys
import argparse
import mysql.connector
from pathlib import Path
from datetime import datetime, timedelta
from collections import defaultdict
import json
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()


class UnprocessedDataFinder:
    def __init__(self, days=30, source='all', report=False):
        self.days = days
        self.source = source
        self.report = report

        # DB ì—°ê²°
        self.conn = mysql.connector.connect(
            host=os.getenv('DB_HOST'),
            port=int(os.getenv('DB_PORT')),
            user=os.getenv('DB_USER'),
            password=os.getenv('DB_PASSWORD'),
            database=os.getenv('DB_NAME')
        )
        self.cursor = self.conn.cursor(dictionary=True)

        # ì†ŒìŠ¤ë³„ ê¸°ë³¸ ê²½ë¡œ
        self.base_paths = {
            'btp': Path('/home/zium/moabojo/incremental/btp'),
            'eminwon': Path('/home/zium/moabojo/incremental/eminwon'),
            'homepage': Path('/home/zium/moabojo/incremental/homepage')
        }

        # ê²°ê³¼ ì €ì¥
        self.results = {
            'btp': [],
            'eminwon': [],
            'homepage': []
        }

    def get_date_folders(self, source_name):
        """íŠ¹ì • ì†ŒìŠ¤ì˜ ë‚ ì§œ í´ë” ëª©ë¡ ë°˜í™˜"""
        base_path = self.base_paths[source_name]

        if not base_path.exists():
            print(f"âš ï¸  ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {base_path}")
            return []

        date_folders = []
        cutoff_date = datetime.now() - timedelta(days=self.days)

        for item in base_path.iterdir():
            if not item.is_dir():
                continue

            # ë‚ ì§œ í´ë” í˜•ì‹ íŒŒì‹± (YYYY-MM-DD ë˜ëŠ” YYYYMMDD)
            folder_name = item.name
            try:
                if '-' in folder_name:
                    # YYYY-MM-DD í˜•ì‹
                    folder_date = datetime.strptime(folder_name, '%Y-%m-%d')
                elif len(folder_name) == 8 and folder_name.isdigit():
                    # YYYYMMDD í˜•ì‹
                    folder_date = datetime.strptime(folder_name, '%Y%m%d')
                else:
                    continue

                # ìµœê·¼ Nì¼ ì´ë‚´ì¸ì§€ í™•ì¸
                if folder_date >= cutoff_date:
                    date_folders.append({
                        'path': item,
                        'date_str': folder_name,
                        'date': folder_date
                    })
            except ValueError:
                continue

        return sorted(date_folders, key=lambda x: x['date'], reverse=True)

    def count_folder_announcements(self, date_folder_path):
        """ë‚ ì§œ í´ë” ë‚´ì˜ ê³µê³  ê°œìˆ˜ ê³„ì‚°"""
        total = 0
        site_counts = {}

        if not date_folder_path.exists():
            return total, site_counts

        for site_folder in date_folder_path.iterdir():
            if not site_folder.is_dir() or site_folder.name.startswith('.'):
                continue

            # ì‚¬ì´íŠ¸ í´ë” ë‚´ì˜ ê³µê³  í´ë” ê°œìˆ˜ (ìˆ«ìë¡œ ì‹œì‘í•˜ëŠ” í´ë”)
            announcement_folders = [
                d for d in site_folder.iterdir()
                if d.is_dir() and not d.name.startswith('.')
            ]

            count = len(announcement_folders)
            if count > 0:
                site_counts[site_folder.name] = count
                total += count

        return total, site_counts

    def count_db_announcements(self, date_str, site_code=None):
        """
        DBì— ë“±ë¡ëœ ê³µê³  ê°œìˆ˜ ê³„ì‚°

        content_md IS NOT NULL ì¡°ê±´ì„ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œë¡œ ì²˜ë¦¬ ì™„ë£Œëœ ê³µê³ ë§Œ ì¹´ìš´íŠ¸í•©ë‹ˆë‹¤.
        created_at ë‚ ì§œ ê¸°ì¤€ì€ ì¬ì²˜ë¦¬ ì‹œ ë¶€ì •í™•í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ,
        ì¶”ê°€ë¡œ folder_name íŒ¨í„´ë„ í™•ì¸í•©ë‹ˆë‹¤.

        Args:
            date_str: ë‚ ì§œ ë¬¸ìì—´ (YYYY-MM-DD ë˜ëŠ” YYYYMMDD)
            site_code: ì‚¬ì´íŠ¸ ì½”ë“œ (Noneì´ë©´ ëª¨ë“  ì‚¬ì´íŠ¸)

        Returns:
            DBì— ë“±ë¡ëœ ê³µê³  ê°œìˆ˜
        """
        # ë‚ ì§œ í˜•ì‹ ë³€í™˜ (YYYYMMDD -> YYYY-MM-DD)
        if '-' in date_str:
            db_date = date_str
            folder_date = date_str.replace('-', '')  # YYYYMMDD for folder pattern
        else:
            db_date = f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:]}"
            folder_date = date_str

        if site_code:
            # content_mdê°€ ìˆëŠ” ê³µê³ ë§Œ ì¹´ìš´íŠ¸ (ì‹¤ì œ ì²˜ë¦¬ ì™„ë£Œ)
            # created_at ë‚ ì§œ ê¸°ì¤€ OR folder_name íŒ¨í„´ ê¸°ì¤€ (ì¬ì²˜ë¦¬ ëŒ€ì‘)
            query = """
                SELECT COUNT(*) as count
                FROM announcement_pre_processing
                WHERE content_md IS NOT NULL
                    AND site_code = %s
                    AND (
                        DATE(created_at) = %s
                        OR folder_name LIKE CONCAT(%s, '_%')
                    )
            """
            self.cursor.execute(query, (site_code, db_date, folder_date))
        else:
            query = """
                SELECT COUNT(*) as count
                FROM announcement_pre_processing
                WHERE content_md IS NOT NULL
                    AND (
                        DATE(created_at) = %s
                        OR folder_name LIKE CONCAT(%s, '_%')
                    )
            """
            self.cursor.execute(query, (db_date, folder_date))

        result = self.cursor.fetchone()
        return result['count'] if result else 0

    def check_source(self, source_name):
        """íŠ¹ì • ì†ŒìŠ¤ì˜ ë¯¸ì²˜ë¦¬ ë°ì´í„° í™•ì¸"""
        print(f"\n{'='*80}")
        print(f"ã€{source_name.upper()}ã€‘ ë¯¸ì²˜ë¦¬ ë°ì´í„° ê²€ì‚¬")
        print(f"{'='*80}")

        date_folders = self.get_date_folders(source_name)

        if not date_folders:
            print(f"  ê²€ì‚¬í•  ë‚ ì§œ í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        print(f"  ìµœê·¼ {self.days}ì¼ ì´ë‚´ ë‚ ì§œ í´ë”: {len(date_folders)}ê°œ\n")

        unprocessed = []

        for folder_info in date_folders:
            folder_path = folder_info['path']
            date_str = folder_info['date_str']

            # í´ë” ë‚´ ê³µê³  ê°œìˆ˜ ê³„ì‚°
            folder_count, site_counts = self.count_folder_announcements(folder_path)

            # DB ë‚´ ê³µê³  ê°œìˆ˜ ê³„ì‚°
            db_count = self.count_db_announcements(date_str)

            # ì°¨ì´ ê³„ì‚°
            diff = folder_count - db_count

            if diff > 0 or self.report:
                status = "âŒ ë¯¸ë“±ë¡" if diff > 0 else "âœ… ì™„ë£Œ"
                print(f"  {date_str}: {status}")
                print(f"    í´ë”: {folder_count:4d}ê°œ | DB: {db_count:4d}ê°œ | ì°¨ì´: {diff:4d}ê°œ")

                if diff > 0:
                    unprocessed.append({
                        'date': date_str,
                        'folder_count': folder_count,
                        'db_count': db_count,
                        'diff': diff,
                        'site_counts': site_counts
                    })

                    if self.report and site_counts:
                        print(f"    ì‚¬ì´íŠ¸ë³„ ìƒì„¸:")
                        for site, count in sorted(site_counts.items(), key=lambda x: -x[1])[:10]:
                            site_db_count = self.count_db_announcements(date_str, site)
                            site_diff = count - site_db_count
                            if site_diff > 0:
                                print(f"      - {site:<20}: í´ë” {count:3d}ê°œ | DB {site_db_count:3d}ê°œ | ì°¨ì´ {site_diff:3d}ê°œ")

        self.results[source_name] = unprocessed

        if unprocessed:
            print(f"\n  âš ï¸  ë¯¸ì²˜ë¦¬ ë‚ ì§œ: {len(unprocessed)}ê°œ")
            print(f"  ğŸ“Š ë¯¸ë“±ë¡ ê³µê³ : {sum(d['diff'] for d in unprocessed):,}ê°œ")
        else:
            print(f"\n  âœ… ëª¨ë“  ë°ì´í„°ê°€ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def generate_report(self):
        """ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±"""
        print(f"\n{'='*80}")
        print("ã€ìµœì¢… ìš”ì•½ã€‘")
        print(f"{'='*80}\n")

        total_unprocessed_dates = 0
        total_unprocessed_announcements = 0

        for source_name, unprocessed in self.results.items():
            if unprocessed:
                count = len(unprocessed)
                announcements = sum(d['diff'] for d in unprocessed)
                total_unprocessed_dates += count
                total_unprocessed_announcements += announcements

                print(f"  {source_name.upper():<10}: {count}ê°œ ë‚ ì§œ, {announcements:,}ê°œ ê³µê³  ë¯¸ë“±ë¡")

        print(f"\n  {'='*76}")
        print(f"  ì´ê³„        : {total_unprocessed_dates}ê°œ ë‚ ì§œ, {total_unprocessed_announcements:,}ê°œ ê³µê³  ë¯¸ë“±ë¡")
        print(f"  {'='*76}")

        if total_unprocessed_announcements > 0:
            print(f"\n  âš ï¸  DBì— ë“±ë¡ë˜ì§€ ì•Šì€ ê³µê³ ê°€ {total_unprocessed_announcements:,}ê°œ ìˆìŠµë‹ˆë‹¤!")
            print(f"\n  ğŸ’¡ ì¬ì²˜ë¦¬ ëª…ë ¹ì–´:")
            print(f"     python3 batch_reprocess_dates.py --auto")
        else:
            print(f"\n  âœ… ëª¨ë“  ë°ì´í„°ê°€ DBì— ë“±ë¡ë˜ì—ˆìŠµë‹ˆë‹¤!")

    def export_json(self, output_file='unprocessed_dates.json'):
        """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        output = {
            'scan_date': datetime.now().isoformat(),
            'days_scanned': self.days,
            'results': self.results
        }

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(output, f, indent=2, ensure_ascii=False, default=str)

        print(f"\n  ğŸ“„ ê²°ê³¼ ì €ì¥: {output_file}")

    def run(self):
        """ë©”ì¸ ì‹¤í–‰"""
        print(f"{'='*80}")
        print(f"ë¯¸ì²˜ë¦¬ ë°ì´í„° ê²€ì‚¬ ì‹œì‘")
        print(f"{'='*80}")
        print(f"  ê²€ì‚¬ ê¸°ê°„: ìµœê·¼ {self.days}ì¼")
        print(f"  ê²€ì‚¬ ëŒ€ìƒ: {self.source}")

        sources = ['btp', 'eminwon', 'homepage'] if self.source == 'all' else [self.source]

        for source_name in sources:
            self.check_source(source_name)

        self.generate_report()
        self.export_json()

        self.cursor.close()
        self.conn.close()


def main():
    parser = argparse.ArgumentParser(
        description='incremental ë””ë ‰í† ë¦¬ì˜ ë¯¸ì²˜ë¦¬ ë°ì´í„° ê²€ì‚¬',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )

    parser.add_argument(
        '--days',
        type=int,
        default=30,
        help='ê²€ì‚¬í•  ê¸°ê°„ (ì¼ ë‹¨ìœ„, ê¸°ë³¸: 30ì¼)'
    )

    parser.add_argument(
        '--source',
        choices=['all', 'btp', 'eminwon', 'homepage'],
        default='all',
        help='ê²€ì‚¬í•  ì†ŒìŠ¤ (ê¸°ë³¸: all)'
    )

    parser.add_argument(
        '--report',
        action='store_true',
        help='ìƒì„¸ ë¦¬í¬íŠ¸ ì¶œë ¥'
    )

    args = parser.parse_args()

    finder = UnprocessedDataFinder(
        days=args.days,
        source=args.source,
        report=args.report
    )

    finder.run()


if __name__ == '__main__':
    main()
