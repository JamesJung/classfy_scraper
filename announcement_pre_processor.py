#!/usr/bin/env python3
"""
Í≥µÍ≥† ÏÇ¨Ï†Ñ Ï≤òÎ¶¨ ÌîÑÎ°úÍ∑∏Îû®

ÏÇ¨Ïö©Î≤ï:
    python announcement_pre_processor.py -d [ÎîîÎ†âÌÜ†Î¶¨Î™Ö] --site-code [ÏÇ¨Ïù¥Ìä∏ÏΩîÎìú]

ÏòàÏãú:
    python announcement_pre_processor.py -d scraped_data --site-code site001
    python announcement_pre_processor.py -d eminwon_data --site-code emw001
"""

import sys
import argparse
import json
import os
import re
import sys
import time
import unicodedata
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional

# ÌîÑÎ°úÏ†ùÌä∏ Î£®Ìä∏Î•º Python pathÏóê Ï∂îÍ∞Ä
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.config.config import ConfigManager

from src.config.logConfig import setup_logging

from src.models.announcementPrvDatabase import AnnouncementPrvDatabaseManager

logger = setup_logging(__name__)

config = ConfigManager().get_config()


class AnnouncementPreProcessor:
    """Í≥µÍ≥† ÏÇ¨Ï†Ñ Ï≤òÎ¶¨ Î©îÏù∏ ÌÅ¥ÎûòÏä§"""

    def __init__(self, site_type: str, attach_force: bool = False, site_code: str = None, lazy_init: bool = False):
        # lazy_init ÏòµÏÖòÏù¥ TrueÎ©¥ AttachmentProcessorÎ•º ÎÇòÏ§ëÏóê Ï¥àÍ∏∞Ìôî
        self._lazy_init = lazy_init
        self._attachment_processor = None

        if not lazy_init:
            # AttachmentProcessorÎ•º ÏßÄÏó∞ import
            from src.utils.attachmentProcessor import AttachmentProcessor
            self._attachment_processor = AttachmentProcessor()

        self.db_manager = AnnouncementPrvDatabaseManager()
        self.attach_force = attach_force
        self.site_type = site_type
        self.site_code = site_code  # site_codeÎ•º Ïù∏Ïä§ÌÑ¥Ïä§ Î≥ÄÏàòÎ°ú Ï†ÄÏû•

        # Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÌÖåÏù¥Î∏î ÏÉùÏÑ± (ÏóÜÎäî Í≤ΩÏö∞)
        self._ensure_database_tables()

        # Ï†úÏô∏ ÌÇ§ÏõåÎìú Î°úÎìú
        self.exclusion_keywords = self._load_exclusion_keywords()

    @property
    def attachment_processor(self):
        """ÏßÄÏó∞ Ï¥àÍ∏∞ÌôîÎ•º ÏúÑÌïú property"""
        if self._lazy_init and self._attachment_processor is None:
            logger.info("ÏßÄÏó∞ Ï¥àÍ∏∞Ìôî: AttachmentProcessor ÏÉùÏÑ±")
            try:
                # ÏßÄÏó∞ import
                from src.utils.attachmentProcessor import AttachmentProcessor
                self._attachment_processor = AttachmentProcessor()
            except Exception as e:
                logger.error(f"AttachmentProcessor Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
                # Ïã§Ìå® Ïãú None Î∞òÌôòÌïòÏó¨ Ìò∏Ï∂úÏûêÍ∞Ä Ï≤òÎ¶¨ÌïòÎèÑÎ°ù Ìï®
                return None
        return self._attachment_processor

    def _ensure_database_tables(self):
        """Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÌÖåÏù¥Î∏îÏù¥ Ï°¥Ïû¨ÌïòÎäîÏßÄ ÌôïÏù∏ÌïòÍ≥† ÏÉùÏÑ±Ìï©ÎãàÎã§."""
        try:
            if self.db_manager.test_connection():
                # announcement_pre_processing ÌÖåÏù¥Î∏î ÏÉùÏÑ±
                from sqlalchemy import text

                with self.db_manager.SessionLocal() as session:
                    session.execute(
                        text(
                            """
                        CREATE TABLE IF NOT EXISTS announcement_pre_processing (
                            id INT PRIMARY KEY AUTO_INCREMENT,
                            folder_name VARCHAR(255) UNIQUE,
                            site_type VARCHAR(50),
                            site_code VARCHAR(50),
                            content_md LONGTEXT,
                            combined_content LONGTEXT,
                            attachment_filenames TEXT,
                            attachment_files_list JSON,
                            exclusion_keyword TEXT,
                            exclusion_reason TEXT,
                            title VARCHAR(500),
                            origin_url VARCHAR(1000),
                            announcement_date VARCHAR(50),
                            processing_status VARCHAR(50),
                            error_message TEXT,
                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
                            INDEX idx_site_code (site_code),
                            INDEX idx_processing_status (processing_status),
                            INDEX idx_origin_url (origin_url)
                        ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci
                    """
                        )
                    )
                    session.commit()
                logger.info("Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÌÖåÏù¥Î∏î ÌôïÏù∏/ÏÉùÏÑ± ÏôÑÎ£å")
            else:
                logger.warning(
                    "Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïó∞Í≤∞ Ïã§Ìå® - Í≥ÑÏÜç ÏßÑÌñâÌï©ÎãàÎã§ (DB Ï†ÄÏû• Î∂àÍ∞Ä)"
                )
        except Exception as e:
            logger.warning(
                f"Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e} - Í≥ÑÏÜç ÏßÑÌñâÌï©ÎãàÎã§ (DB Ï†ÄÏû• Î∂àÍ∞Ä)"
            )

    def _load_exclusion_keywords(self) -> List[Dict[str, Any]]:
        """Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ÏóêÏÑú Ï†úÏô∏ ÌÇ§ÏõåÎìúÎ•º Î°úÎìúÌï©ÎãàÎã§."""
        try:
            from sqlalchemy import text

            with self.db_manager.SessionLocal() as session:
                result = session.execute(
                    text(
                        """
                    SELECT EXCLUSION_ID, KEYWORD, DESCRIPTION
                    FROM EXCLUSION_KEYWORDS
                    WHERE IS_ACTIVE = TRUE
                    ORDER BY EXCLUSION_ID
                """
                    )
                )

                keywords = []
                for row in result:
                    keywords.append(
                        {"id": row[0], "keyword": row[1], "description": row[2]}
                    )

                logger.info(f"Ï†úÏô∏ ÌÇ§ÏõåÎìú Î°úÎìú ÏôÑÎ£å: {len(keywords)}Í∞ú")
                return keywords

        except Exception as e:
            logger.warning(f"Ï†úÏô∏ ÌÇ§ÏõåÎìú Î°úÎìú Ïã§Ìå®: {e}")
            return []

    def process_directory(self, directory_path: Path, site_code: str) -> bool:
        """
        Îã®Ïùº ÎîîÎ†âÌÜ†Î¶¨Î•º Ï≤òÎ¶¨Ìï©ÎãàÎã§.

        Args:
            directory_path: Ï≤òÎ¶¨Ìï† ÎîîÎ†âÌÜ†Î¶¨ Í≤ΩÎ°ú
            site_code: ÏÇ¨Ïù¥Ìä∏ ÏΩîÎìú

        Returns:
            Ï≤òÎ¶¨ ÏÑ±Í≥µ Ïó¨Î∂Ä
        """
        folder_name = directory_path.name
        return self.process_directory_with_custom_name(
            directory_path, site_code, folder_name
        )

    def _find_target_directories(
        self, base_dir: Path, site_code: str, force: bool = False
    ) -> List[Path]:
        """
        Ï≤òÎ¶¨Ìï† ÎåÄÏÉÅ ÎîîÎ†âÌÜ†Î¶¨Îì§ÏùÑ Ï∞æÏäµÎãàÎã§.

        Args:
            base_dir: Í∏∞Î≥∏ ÎîîÎ†âÌÜ†Î¶¨
            site_code: ÏÇ¨Ïù¥Ìä∏ ÏΩîÎìú
            force: Ïù¥ÎØ∏ Ï≤òÎ¶¨Îêú Ìï≠Î™©ÎèÑ Îã§Ïãú Ï≤òÎ¶¨Ìï†ÏßÄ Ïó¨Î∂Ä

        Returns:
            Ï≤òÎ¶¨ ÎåÄÏÉÅ ÎîîÎ†âÌÜ†Î¶¨ Î™©Î°ù
        """
        site_dir = base_dir / site_code

        if not site_dir.exists():
            logger.error(f"ÏÇ¨Ïù¥Ìä∏ ÎîîÎ†âÌÜ†Î¶¨Í∞Ä ÏóÜÏùå: {site_dir}")
            return []

        target_directories = []

        # Î™®Îì† ÌïòÏúÑ ÎîîÎ†âÌÜ†Î¶¨ÏóêÏÑú content.md, JSON ÌååÏùº ÎòêÎäî attachments Ìè¥ÎçîÍ∞Ä ÏûàÎäî ÎîîÎ†âÌÜ†Î¶¨ Ï∞æÍ∏∞
        logger.info(f"ÎîîÎ†âÌÜ†Î¶¨ Í≤ÄÏÉâ ÏãúÏûë: {site_dir}")

        # bizInfo, sme, kStartUpÏùÄ ÌîåÎû´ Íµ¨Ï°∞ (ÏßÅÏ†ë ÌïòÏúÑ ÎîîÎ†âÌÜ†Î¶¨Îßå Í≤ÄÏÉâ)
        if site_code in ["bizInfo", "sme", "kStartUp"]:
            # ÏßÅÏ†ë ÌïòÏúÑ ÎîîÎ†âÌÜ†Î¶¨Îßå Í≤ÄÏÉâ (Îçî Îπ†Î¶Ñ)
            # Î™®Îì† API ÏÇ¨Ïù¥Ìä∏Îäî content.mdÍ∞Ä Î∞òÎìúÏãú ÏûàÏñ¥Ïïº Ìï®
            for root_path in site_dir.iterdir():
                if root_path.is_dir():
                    has_content_md = (root_path / "content.md").exists()

                    if has_content_md:
                        # content.mdÍ∞Ä ÏûàÎäî ÎîîÎ†âÌÜ†Î¶¨Îßå Ï≤òÎ¶¨
                        target_directories.append(root_path)
                        logger.debug(
                            f"ÎåÄÏÉÅ ÎîîÎ†âÌÜ†Î¶¨ Î∞úÍ≤¨: {root_path.relative_to(site_dir)}"
                        )
                    else:
                        # content.mdÍ∞Ä ÏóÜÎäî ÎîîÎ†âÌÜ†Î¶¨Îäî Í±¥ÎÑàÎõ∞Í∏∞
                        logger.debug(
                            f"{site_code} ÎîîÎ†âÌÜ†Î¶¨ Í±¥ÎÑàÎõ∞Í∏∞ (content.md ÏóÜÏùå): {root_path.relative_to(site_dir)}"
                        )
        else:
            # Îã§Î•∏ ÏÇ¨Ïù¥Ìä∏Îäî Ïû¨Í∑ÄÏ†ÅÏúºÎ°ú Í≤ÄÏÉâ (Ï§ëÏ≤© Íµ¨Ï°∞ Í∞ÄÎä•)
            for root_path in site_dir.rglob("*"):
                if root_path.is_dir():
                    # content.md ÌååÏùºÏù¥ ÏûàÍ±∞ÎÇò attachments Ìè¥ÎçîÍ∞Ä ÏûàÍ±∞ÎÇò JSON ÌååÏùºÏù¥ ÏûàÎäî ÎîîÎ†âÌÜ†Î¶¨Îßå ÎåÄÏÉÅÏúºÎ°ú Ìï®
                    has_content_md = (root_path / "content.md").exists()
                    has_json = bool(list(root_path.glob("*.json")))
                    # attachments Ìè¥Îçî ÌôïÏù∏ ÏµúÏ†ÅÌôî
                    attachments_dir = root_path / "attachments"
                    has_attachments = False
                    if attachments_dir.exists():
                        # Ï≤´ Î≤àÏß∏ ÌååÏùºÎßå ÌôïÏù∏ (Ï†ÑÏ≤¥ ÎîîÎ†âÌÜ†Î¶¨ ÏàúÌöå Î∞©ÏßÄ)
                        try:
                            next(attachments_dir.iterdir())
                            has_attachments = True
                        except StopIteration:
                            has_attachments = False

                    if has_content_md or has_attachments or has_json:
                        target_directories.append(root_path)
                        logger.debug(
                            f"ÎåÄÏÉÅ ÎîîÎ†âÌÜ†Î¶¨ Î∞úÍ≤¨: {root_path.relative_to(site_dir)}"
                        )

        # Ìè¥ÎçîÎ™ÖÏúºÎ°ú Ï†ïÎ†¨
        target_directories = sorted(target_directories, key=self._natural_sort_key)

        logger.info(f"Î∞úÍ≤¨Îêú Ï†ÑÏ≤¥ ÎîîÎ†âÌÜ†Î¶¨: {len(target_directories)}Í∞ú")

        # Ï≤òÏùå Î™á Í∞ú Ìè¥ÎçîÎ™Ö Î°úÍπÖ
        if target_directories:
            logger.info(f"Ï≤´ 5Í∞ú Ìè¥Îçî: {[d.name for d in target_directories[:5]]}")

        # force ÏòµÏÖòÏù¥ ÏóÜÏùÑ ÎïåÎßå Ïù¥ÎØ∏ Ï≤òÎ¶¨Îêú Ìè¥Îçî Ï†úÏô∏
        if not force:
            processed_folders = set(self._get_processed_folders(site_code))

            filtered_directories = []
            for directory in target_directories:
                # ÏÇ¨Ïù¥Ìä∏ ÎîîÎ†âÌÜ†Î¶¨Î°úÎ∂ÄÌÑ∞Ïùò ÏÉÅÎåÄ Í≤ΩÎ°úÎ•º Ìè¥ÎçîÎ™ÖÏúºÎ°ú ÏÇ¨Ïö©
                relative_path = directory.relative_to(site_dir)
                folder_name = self._normalize_korean_text(
                    str(relative_path).replace("/", "_")
                )

                if folder_name not in processed_folders:
                    filtered_directories.append(directory)
                else:
                    logger.debug(f"Ïù¥ÎØ∏ Ï≤òÎ¶¨Îêú Ìè¥Îçî Í±¥ÎÑàÎúÄ: {folder_name}")

            logger.info(f"Ï†ÑÏ≤¥ Î∞úÍ≤¨Îêú ÎîîÎ†âÌÜ†Î¶¨: {len(target_directories)}Í∞ú")
            logger.info(f"Ï≤òÎ¶¨ ÎåÄÏÉÅ ÎîîÎ†âÌÜ†Î¶¨: {len(filtered_directories)}Í∞ú")
            logger.info(f"Ïù¥ÎØ∏ Ï≤òÎ¶¨Îêú Ìè¥Îçî: {len(processed_folders)}Í∞ú")

            return filtered_directories
        else:
            # force ÏòµÏÖòÏù¥ ÏûàÏúºÎ©¥ Î™®Îì† ÎîîÎ†âÌÜ†Î¶¨ Î∞òÌôò
            logger.info(
                f"--force ÏòµÏÖò: Î™®Îì† ÎîîÎ†âÌÜ†Î¶¨ Ï≤òÎ¶¨ ({len(target_directories)}Í∞ú)"
            )
            return target_directories

    def _get_processed_folders(self, site_code: str) -> List[str]:
        """Ïù¥ÎØ∏ Ï≤òÎ¶¨Îêú Ìè¥Îçî Î™©Î°ùÏùÑ Í∞ÄÏ†∏ÏòµÎãàÎã§."""
        try:
            from sqlalchemy import text

            with self.db_manager.SessionLocal() as session:
                result = session.execute(
                    text(
                        """
                    SELECT folder_name 
                    FROM announcement_pre_processing 
                    WHERE site_code = :site_code
                """
                    ),
                    {"site_code": site_code},
                )

                return [row[0] for row in result]

        except Exception as e:
            logger.error(f"Ï≤òÎ¶¨Îêú Ìè¥Îçî Î™©Î°ù Ï°∞Ìöå Ïã§Ìå®: {e}")
            return []

    def process_site_directories(
        self, base_dir: Path, site_code: str, force: bool = False
    ) -> Dict[str, int]:
        """
        ÌäπÏ†ï ÏÇ¨Ïù¥Ìä∏Ïùò Î™®Îì† ÎîîÎ†âÌÜ†Î¶¨Î•º Ï≤òÎ¶¨Ìï©ÎãàÎã§.

        Args:
            base_dir: Í∏∞Î≥∏ ÎîîÎ†âÌÜ†Î¶¨
            site_code: ÏÇ¨Ïù¥Ìä∏ ÏΩîÎìú
            force: Ïù¥ÎØ∏ Ï≤òÎ¶¨Îêú Ìï≠Î™©ÎèÑ Îã§Ïãú Ï≤òÎ¶¨Ìï†ÏßÄ Ïó¨Î∂Ä

        Returns:
            Ï≤òÎ¶¨ Í≤∞Í≥º ÌÜµÍ≥Ñ
        """

        # Ï≤òÎ¶¨Ìï† ÎîîÎ†âÌÜ†Î¶¨ Î™©Î°ù Ï∞æÍ∏∞
        target_directories = self._find_target_directories(base_dir, site_code, force)

        if not target_directories:
            logger.warning("Ï≤òÎ¶¨Ìï† ÎîîÎ†âÌÜ†Î¶¨Í∞Ä ÏóÜÏäµÎãàÎã§.")
            return {"total": 0, "success": 0, "failed": 0, "skipped": 0}

        total_count = len(target_directories)
        results = {"total": total_count, "success": 0, "failed": 0, "skipped": 0}
        site_dir = base_dir / site_code

        print(f"\n{'='*60}")
        print(
            f"Í≥µÍ≥† Ï≤òÎ¶¨ ÏãúÏûë: {site_code} (Site Type: {self.site_type}) ({total_count}Í∞ú Ìè¥Îçî)"
        )
        print(f"{'='*60}")

        # ÏãúÏûë ÏãúÍ∞Ñ Í∏∞Î°ù
        start_time = time.time()

        for i, directory in enumerate(target_directories, 1):
            try:
                # Í∞úÎ≥Ñ Ìï≠Î™© ÏãúÏûë ÏãúÍ∞Ñ
                item_start_time = time.time()

                # ÏÇ¨Ïù¥Ìä∏ ÎîîÎ†âÌÜ†Î¶¨Î°úÎ∂ÄÌÑ∞Ïùò ÏÉÅÎåÄ Í≤ΩÎ°úÎ•º Ìè¥ÎçîÎ™ÖÏúºÎ°ú ÏÇ¨Ïö©
                relative_path = directory.relative_to(site_dir)
                folder_name = self._normalize_korean_text(
                    str(relative_path).replace("/", "_")
                )

                progress_pct = (i / total_count) * 100
                print(f"\n[{i}/{total_count} : {progress_pct:.1f}%] {folder_name}")

                # Ïù¥ÎØ∏ Ï≤òÎ¶¨Îêú Ìï≠Î™© ÌôïÏù∏ (force ÏòµÏÖòÏù¥ ÏóÜÏùÑ ÎïåÎßå)
                if not force and self._is_already_processed(folder_name, site_code):
                    skip_elapsed = time.time() - item_start_time
                    print(f"  ‚úì Ïù¥ÎØ∏ Ï≤òÎ¶¨Îê®, Í±¥ÎÑàÎúÄ ({skip_elapsed:.1f}Ï¥à)")
                    results["skipped"] += 1
                    continue
                elif force and self._is_already_processed(folder_name, site_code):
                    print("  üîÑ Ïù¥ÎØ∏ Ï≤òÎ¶¨Îê®, --force ÏòµÏÖòÏúºÎ°ú Ïû¨Ï≤òÎ¶¨")

                success = self.process_directory_with_custom_name(
                    directory, site_code, folder_name, force
                )

                # Í∞úÎ≥Ñ Ìï≠Î™© Ï≤òÎ¶¨ ÏãúÍ∞Ñ Í≥ÑÏÇ∞
                item_elapsed = time.time() - item_start_time

                if success:
                    results["success"] += 1
                    print(f"  ‚úì Ï≤òÎ¶¨ ÏôÑÎ£å ({item_elapsed:.1f}Ï¥à)")
                else:
                    results["failed"] += 1
                    print(f"  ‚úó Ï≤òÎ¶¨ Ïã§Ìå® ({item_elapsed:.1f}Ï¥à)")

            except Exception as e:
                error_elapsed = time.time() - item_start_time
                results["failed"] += 1
                print(f"  ‚úó ÏòàÏô∏ Î∞úÏÉù: {str(e)[:100]}... ({error_elapsed:.1f}Ï¥à)")
                logger.error(f"Ï≤òÎ¶¨ Ï§ë Ïò§Î•ò ({directory}): {e}")

        # Ï¢ÖÎ£å ÏãúÍ∞Ñ Î∞è ÌÜµÍ≥Ñ Í≥ÑÏÇ∞
        end_time = time.time()
        total_elapsed = end_time - start_time
        processed_count = results["success"] + results["failed"]

        print(f"\n{'='*60}")
        print(
            f"Ï≤òÎ¶¨ ÏôÑÎ£å: {results['success']}/{total_count} ÏÑ±Í≥µ ({(results['success']/total_count)*100:.1f}%)"
        )
        print(f"Í±¥ÎÑàÎúÄ: {results['skipped']}, Ïã§Ìå®: {results['failed']}")
        print(f"")
        print(f"üìä Ï≤òÎ¶¨ ÏãúÍ∞Ñ ÌÜµÍ≥Ñ:")
        print(f"   Ï¥ù ÏÜåÏöî ÏãúÍ∞Ñ: {total_elapsed:.1f}Ï¥à ({total_elapsed/60:.1f}Î∂Ñ)")

        if processed_count > 0:
            avg_time_per_item = total_elapsed / processed_count
            print(f"   Ï≤òÎ¶¨Ìïú Ìï≠Î™©Îãπ ÌèâÍ∑† ÏãúÍ∞Ñ: {avg_time_per_item:.1f}Ï¥à")

        if results["success"] > 0:
            avg_time_per_success = total_elapsed / results["success"]
            print(f"   ÏÑ±Í≥µÌïú Ìï≠Î™©Îãπ ÌèâÍ∑† ÏãúÍ∞Ñ: {avg_time_per_success:.1f}Ï¥à")

        print(f"{'='*60}")

        logger.info(
            f"Ï≤òÎ¶¨ ÏôÑÎ£å - Ï†ÑÏ≤¥: {results['total']}, ÏÑ±Í≥µ: {results['success']}, Ïã§Ìå®: {results['failed']}, Í±¥ÎÑàÎúÄ: {results['skipped']}"
        )

        return results

    def process_directory_with_custom_name(
        self,
        directory_path: Path,
        site_code: str,
        folder_name: str,
        force: bool = False,
    ) -> bool:
        """
        ÏÇ¨Ïö©Ïûê Ï†ïÏùò Ìè¥ÎçîÎ™ÖÏúºÎ°ú ÎîîÎ†âÌÜ†Î¶¨Î•º Ï≤òÎ¶¨Ìï©ÎãàÎã§.

        Args:
            directory_path: Ï≤òÎ¶¨Ìï† ÎîîÎ†âÌÜ†Î¶¨ Í≤ΩÎ°ú
            site_code: ÏÇ¨Ïù¥Ìä∏ ÏΩîÎìú
            folder_name: Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§Ïóê Ï†ÄÏû•Ìï† Ìè¥ÎçîÎ™Ö
            force: Ïù¥ÎØ∏ Ï≤òÎ¶¨Îêú Ìï≠Î™©ÎèÑ Îã§Ïãú Ï≤òÎ¶¨Ìï†ÏßÄ Ïó¨Î∂Ä

        Returns:
            Ï≤òÎ¶¨ ÏÑ±Í≥µ Ïó¨Î∂Ä
        """
        logger.info(f"ÎîîÎ†âÌÜ†Î¶¨ Ï≤òÎ¶¨ ÏãúÏûë: {folder_name} (site_code: {site_code})")

        try:
            # 0. folder_name Ï§ëÎ≥µ Ï≤¥ÌÅ¨ (force ÏòµÏÖòÏù¥ ÏóÜÏùÑ ÎïåÎßå)
            if not force:
                if self._check_folder_name_exists(folder_name, site_code):
                    logger.info(f"Ïù¥ÎØ∏ Ï≤òÎ¶¨Îêú Ìè¥Îçî Í±¥ÎÑàÎúÄ: {folder_name}")
                    return True  # ÏÑ±Í≥µÏúºÎ°ú Ï≤òÎ¶¨ (Ïù¥ÎØ∏ Ï≤òÎ¶¨Îê®)

            # 1. Ï†úÏô∏ ÌÇ§ÏõåÎìú Ï≤¥ÌÅ¨
            excluded_keywords = []
            excluded_keywords = self._check_exclusion_keywords(folder_name)

            # 2. ÌäπÏàò ÏÇ¨Ïù¥Ìä∏ Ï≤òÎ¶¨ (Î™®Îëê content.md ÏùΩÍ∏∞)
            content_md = ""
            title = "Ï†ïÎ≥¥ ÏóÜÏùå"
            origin_url = "Ï†ïÎ≥¥ ÏóÜÏùå"
            announcement_date = "Ï†ïÎ≥¥ ÏóÜÏùå"

            if site_code in ["kStartUp", "bizInfo", "sme"]:
                # kStartUp, bizInfo, smeÎäî content.mdÎ•º ÏùΩÍ≥†, JSONÏóêÏÑú ÎÇ†Ïßú Ï†ïÎ≥¥Îßå Î≥¥ÏôÑ
                content_md_path = directory_path / "content.md"
                if content_md_path.exists():
                    try:
                        with open(content_md_path, "r", encoding="utf-8") as f:
                            content_md = f.read()
                        logger.info(f"content.md ÏùΩÍ∏∞ ÏôÑÎ£å: {len(content_md)} Î¨∏Ïûê")

                        # content.mdÏóêÏÑú Í∏∞Î≥∏ Ï†ïÎ≥¥ Ï∂îÏ∂ú
                        title = self._extract_title_from_content(content_md) or "Ï†ïÎ≥¥ ÏóÜÏùå"
                        origin_url = self._extract_origin_url_from_content(content_md) or "Ï†ïÎ≥¥ ÏóÜÏùå"

                        # JSON ÌååÏùºÏóêÏÑú announcement_date Î≥¥ÏôÑ
                        json_files = list(directory_path.glob("*.json"))
                        if json_files:
                            try:
                                with open(json_files[0], "r", encoding="utf-8") as f:
                                    json_data = json.load(f)
                                announcement_date_raw = json_data.get("announcementDate", "")
                                if announcement_date_raw:
                                    announcement_date = self._convert_to_yyyymmdd(announcement_date_raw)
                                else:
                                    # JSONÏóê ÏóÜÏúºÎ©¥ content.mdÏóêÏÑú Ï∂îÏ∂ú
                                    announcement_date = self._extract_announcement_date_from_content(content_md) or "Ï†ïÎ≥¥ ÏóÜÏùå"
                            except Exception as e:
                                logger.warning(f"{site_code} JSON ÎÇ†Ïßú Ï∂îÏ∂ú Ïã§Ìå®, content.md ÏÇ¨Ïö©: {e}")
                                announcement_date = self._extract_announcement_date_from_content(content_md) or "Ï†ïÎ≥¥ ÏóÜÏùå"
                        else:
                            # JSON ÌååÏùºÏù¥ ÏóÜÏúºÎ©¥ content.mdÏóêÏÑú Ï∂îÏ∂ú
                            announcement_date = self._extract_announcement_date_from_content(content_md) or "Ï†ïÎ≥¥ ÏóÜÏùå"

                    except Exception as e:
                        logger.error(f"content.md ÏùΩÍ∏∞ Ïã§Ìå®: {e}")
                        return self._save_processing_result(
                            folder_name,
                            site_code,
                            content_md,
                            "",
                            status="error",
                            error_message=f"content.md ÏùΩÍ∏∞ Ïã§Ìå®: {e}",
                        )
                else:
                    logger.warning(f"content.md ÌååÏùºÏù¥ ÏóÜÏùå: {content_md_path}")

            else:
                # ÏùºÎ∞ò ÏÇ¨Ïù¥Ìä∏ Ï≤òÎ¶¨ (Í∏∞Ï°¥ Î°úÏßÅ)
                content_md_path = directory_path / "content.md"
                if content_md_path.exists():
                    try:
                        with open(content_md_path, "r", encoding="utf-8") as f:
                            content_md = f.read()
                        logger.info(f"content.md ÏùΩÍ∏∞ ÏôÑÎ£å: {len(content_md)} Î¨∏Ïûê")
                    except Exception as e:
                        logger.error(f"content.md ÏùΩÍ∏∞ Ïã§Ìå®: {e}")
                        return self._save_processing_result(
                            folder_name,
                            site_code,
                            content_md,
                            "",
                            status="error",
                            error_message=f"content.md ÏùΩÍ∏∞ Ïã§Ìå®: {e}",
                        )
                else:
                    logger.warning(f"content.md ÌååÏùºÏù¥ ÏóÜÏùå: {content_md_path}")

            # 3. content.mdÎßåÏúºÎ°ú Í∏∞Î≥∏ Í≤ÄÏ¶ù
            if not content_md.strip():
                logger.warning("content.md ÎÇ¥Ïö©Ïù¥ ÏóÜÏùå")
                return self._save_processing_result(
                    folder_name,
                    site_code,
                    content_md,
                    "",
                    attachment_filenames=[],
                    status="error",
                    error_message="content.md ÎÇ¥Ïö©Ïù¥ ÏóÜÏùå",
                )

            # ÏùºÎ∞ò ÏÇ¨Ïù¥Ìä∏Ïùò Í≤ΩÏö∞Îßå content.mdÏóêÏÑú Ï†ïÎ≥¥ Ï∂îÏ∂ú (API ÏÇ¨Ïù¥Ìä∏Îäî Ïù¥ÎØ∏ Ï∂îÏ∂úÌï®)
            if site_code not in ["kStartUp", "bizInfo", "sme"]:
                title = self._extract_title_from_content(content_md) or "Ï†ïÎ≥¥ ÏóÜÏùå"
                origin_url = self._extract_origin_url_from_content(content_md) or "Ï†ïÎ≥¥ ÏóÜÏùå"
                announcement_date = self._extract_announcement_date_from_content(content_md) or "Ï†ïÎ≥¥ ÏóÜÏùå"

            # 3.5. origin_url Ï§ëÎ≥µ Ï≤¥ÌÅ¨
            is_duplicate_url = False
            if origin_url and origin_url != "Ï†ïÎ≥¥ ÏóÜÏùå":
                is_duplicate_url = self._check_origin_url_exists(origin_url, site_code)

            # 4. Ï≤®Î∂ÄÌååÏùº Ï≤òÎ¶¨ (content.mdÏôÄ Î∂ÑÎ¶¨)
            combined_content = ""
            attachment_filenames = []
            attachment_files_info = []
            attachment_error = None
            
            try:
                combined_content, attachment_filenames, attachment_files_info = (
                    self._process_attachments_separately(directory_path)
                )
                logger.info(
                    f"Ï≤®Î∂ÄÌååÏùº ÎÇ¥Ïö© Ï≤òÎ¶¨ ÏôÑÎ£å: {len(combined_content)} Î¨∏Ïûê, ÌååÏùº {len(attachment_filenames)}Í∞ú"
                )
            except Exception as e:
                # Ï≤®Î∂ÄÌååÏùº Ï≤òÎ¶¨ Ïã§Ìå®Î•º Í∏∞Î°ùÌïòÏßÄÎßå Í≥ÑÏÜç ÏßÑÌñâ
                attachment_error = str(e)
                logger.error(f"Ï≤®Î∂ÄÌååÏùº Ï≤òÎ¶¨ Ï§ë ÏòàÏô∏ Î∞úÏÉù (Í≥ÑÏÜç ÏßÑÌñâ): {e}")
                # Îπà Í∞íÏúºÎ°ú ÏÑ§Ï†ïÌïòÍ≥† Í≥ÑÏÜç ÏßÑÌñâ
                combined_content = ""
                attachment_filenames = []
                attachment_files_info = []
                
            # content.mdÏôÄ combined_content Î™®Îëê ÏóÜÎäî Í≤ΩÏö∞ÏóêÎßå ÏóêÎü¨ Ï≤òÎ¶¨
            if not content_md.strip() and not combined_content.strip():
                logger.warning("Ï≤òÎ¶¨Ìï† ÎÇ¥Ïö©Ïù¥ ÏóÜÏùå")
                error_msg = "Ï≤òÎ¶¨Ìï† ÎÇ¥Ïö©Ïù¥ ÏóÜÏùå"
                if attachment_error:
                    error_msg += f" (Ï≤®Î∂ÄÌååÏùº Ïò§Î•ò: {attachment_error})"
                return self._save_processing_result(
                    folder_name,
                    site_code,
                    content_md,
                    combined_content,
                    attachment_filenames=attachment_filenames,
                    attachment_files_info=attachment_files_info,
                    status="error",
                    error_message=error_msg,
                )

            # 5. Ï†úÏô∏ ÌÇ§ÏõåÎìúÍ∞Ä ÏûàÎäî Í≤ΩÏö∞ Ï†úÏô∏ Ï≤òÎ¶¨
            if excluded_keywords:
                exclusion_msg = (
                    f"Ï†úÏô∏ ÌÇ§ÏõåÎìúÍ∞Ä ÏûÖÎ†•ÎêòÏñ¥ ÏûàÏäµÎãàÎã§: {', '.join(excluded_keywords)}"
                )
                logger.info(f"Ï†úÏô∏ Ï≤òÎ¶¨: {folder_name} - {exclusion_msg}")

                return self._save_processing_result(
                    folder_name,
                    site_code,
                    content_md,
                    combined_content,
                    attachment_filenames=attachment_filenames,
                    attachment_files_info=attachment_files_info,
                    status="Ï†úÏô∏",
                    title=title,
                    announcement_date=announcement_date,
                    origin_url=origin_url,
                    exclusion_keywords=excluded_keywords,
                    exclusion_reason=exclusion_msg,
                )

            # 6. Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§Ïóê Ï†ÄÏû• (Ï§ëÎ≥µ URL Ïó¨Î∂ÄÏóê Îî∞Îùº ÏÉÅÌÉú Í≤∞Ï†ï)
            final_status = "Ï§ëÎ≥µ" if is_duplicate_url else "ÏÑ±Í≥µ"

            record_id = self._save_processing_result(
                folder_name,
                site_code,
                content_md,
                combined_content,
                attachment_filenames=attachment_filenames,
                attachment_files_info=attachment_files_info,
                title=title,
                announcement_date=announcement_date,
                origin_url=origin_url,
                status=final_status,
                force=force,
            )

            if is_duplicate_url:
                logger.info(f"origin_url Ï§ëÎ≥µÏúºÎ°ú 'Ï§ëÎ≥µ' ÏÉÅÌÉúÎ°ú Ï†ÄÏû•: {folder_name}")

            if record_id:
                logger.info(f"ÎîîÎ†âÌÜ†Î¶¨ Ï≤òÎ¶¨ ÏôÑÎ£å: {folder_name}")
                return True
            else:
                logger.error(f"ÎîîÎ†âÌÜ†Î¶¨ Ï≤òÎ¶¨ Ïã§Ìå®: {folder_name}")
                return False

        except Exception as e:
            logger.error(f"ÎîîÎ†âÌÜ†Î¶¨ Ï≤òÎ¶¨ Ï§ë ÏòàÏÉÅÏπò Î™ªÌïú Ïò§Î•ò: {e}")
            result = self._save_processing_result(
                folder_name,
                site_code,
                "",
                "",
                status="error",
                error_message=f"ÏòàÏÉÅÏπò Î™ªÌïú Ïò§Î•ò: {e}",
            )
            return result is not None

    def _check_exclusion_keywords(self, folder_name: str) -> List[str]:
        """Ìè¥ÎçîÎ™ÖÏóêÏÑú Ï†úÏô∏ ÌÇ§ÏõåÎìúÎ•º Ï≤¥ÌÅ¨Ìï©ÎãàÎã§."""
        matched_keywords = []

        for keyword_info in self.exclusion_keywords:
            keyword = keyword_info["keyword"].lower()
            if keyword in folder_name.lower():
                matched_keywords.append(keyword_info["keyword"])
                logger.debug(f"Ï†úÏô∏ ÌÇ§ÏõåÎìú Îß§Ïπ≠: '{keyword}' in '{folder_name}'")

        return matched_keywords

    def _check_folder_name_exists(self, folder_name: str, site_code: str) -> bool:
        """folder_nameÏù¥ Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§Ïóê Ïù¥ÎØ∏ Ï°¥Ïû¨ÌïòÎäîÏßÄ ÌôïÏù∏Ìï©ÎãàÎã§."""
        try:
            from sqlalchemy import text

            with self.db_manager.SessionLocal() as session:
                result = session.execute(
                    text(
                        """
                    SELECT COUNT(*) FROM announcement_pre_processing 
                    WHERE folder_name = :folder_name AND site_code = :site_code
                """
                    ),
                    {"folder_name": folder_name, "site_code": site_code},
                )

                count = result.scalar()
                exists = count > 0

                if exists:
                    logger.debug(f"folder_name Ï§ëÎ≥µ Î∞úÍ≤¨: {folder_name}")

                return exists

        except Exception as e:
            logger.error(f"folder_name Ï§ëÎ≥µ Ï≤¥ÌÅ¨ Ïã§Ìå®: {e}")
            return False

    def _check_origin_url_exists(self, origin_url: str, site_code: str) -> bool:
        """origin_urlÏù¥ Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§Ïóê Ïù¥ÎØ∏ Ï°¥Ïû¨ÌïòÎäîÏßÄ ÌôïÏù∏Ìï©ÎãàÎã§."""
        try:
            from sqlalchemy import text

            with self.db_manager.SessionLocal() as session:
                result = session.execute(
                    text(
                        """
                    SELECT COUNT(*) FROM announcement_pre_processing 
                    WHERE origin_url = :origin_url AND site_code = :site_code
                """
                    ),
                    {"origin_url": origin_url, "site_code": site_code},
                )

                count = result.scalar()
                exists = count > 0

                if exists:
                    logger.debug(f"origin_url Ï§ëÎ≥µ Î∞úÍ≤¨: {origin_url}")

                return exists

        except Exception as e:
            logger.error(f"origin_url Ï§ëÎ≥µ Ï≤¥ÌÅ¨ Ïã§Ìå®: {e}")
            return False

    def _is_already_processed(self, folder_name: str, site_code: str) -> bool:
        """Ìè¥ÎçîÍ∞Ä Ïù¥ÎØ∏ Ï≤òÎ¶¨ÎêòÏóàÎäîÏßÄ ÌôïÏù∏Ìï©ÎãàÎã§."""
        return self._check_folder_name_exists(folder_name, site_code)

    def _extract_title_from_content(self, content_md: str) -> str:
        """content.mdÏóêÏÑú Ï†úÎ™©ÏùÑ Ï∂îÏ∂úÌï©ÎãàÎã§."""
        if not content_md:
            return ""

        lines = content_md.split("\n")

        # Ï≤´ Î≤àÏß∏ ÎπÑÏñ¥ÏûàÏßÄ ÏïäÏùÄ Ï§ÑÏùÑ Ï∞æÍ∏∞
        for line in lines[:10]:  # ÏÉÅÏúÑ 10Ï§ÑÎßå ÌôïÏù∏
            line = line.strip()
            if line:
                # # ÎßàÌÅ¨Îã§Ïö¥ Ìó§Îçî Ï†úÍ±∞
                if line.startswith("#"):
                    title = line.lstrip("#").strip()
                    logger.debug(f"ÎßàÌÅ¨Îã§Ïö¥ Ìó§ÎçîÏóêÏÑú Ï†úÎ™© Ï∂îÏ∂ú: {title}")
                    return title

                # **Ï†úÎ™©**: Ìå®ÌÑ¥ ÌôïÏù∏ (ÎßàÌÅ¨Îã§Ïö¥ Î≥ºÎìú)
                if line.startswith("**Ï†úÎ™©**:"):
                    title = line.replace("**Ï†úÎ™©**:", "").strip()
                    logger.debug(f"**Ï†úÎ™©** Ìå®ÌÑ¥ÏóêÏÑú Ï†úÎ™© Ï∂îÏ∂ú: {title}")
                    return title

                # Ï†úÎ™©:, Í≥µÍ≥†Î™Ö: Ìå®ÌÑ¥ ÌôïÏù∏
                for prefix in ["Ï†úÎ™©:", "Í≥µÍ≥†Î™Ö:", "Í≥µÍ≥† Ï†úÎ™©:", "Ï†úÎ™© :"]:
                    if line.lower().startswith(prefix.lower()):
                        title = line[len(prefix) :].strip()
                        logger.debug(f"{prefix} Ìå®ÌÑ¥ÏóêÏÑú Ï†úÎ™© Ï∂îÏ∂ú: {title}")
                        return title

                # ÏùºÎ∞ò ÌÖçÏä§Ìä∏Ïù∏ Í≤ΩÏö∞ Í∑∏ÎåÄÎ°ú Ï†úÎ™©ÏúºÎ°ú ÏÇ¨Ïö© (Ï≤´ Î≤àÏß∏ Ï§Ñ)
                logger.debug(f"Ï≤´ Î≤àÏß∏ Ï§ÑÏùÑ Ï†úÎ™©ÏúºÎ°ú ÏÇ¨Ïö©: {line}")
                return line

        return ""

    def _extract_origin_url_from_content(self, content_md: str) -> str:
        """content.mdÏóêÏÑú ÏõêÎ≥∏ URLÏùÑ Ï∂îÏ∂úÌï©ÎãàÎã§."""
        if not content_md:
            return ""

        # ÏõêÎ≥∏ URL Ìå®ÌÑ¥ Ï∞æÍ∏∞
        origin_patterns = [
            r"\*\*ÏõêÎ≥∏ URL\*\*[:\s]*(.+?)(?:\n|$)",
            r"ÏõêÎ≥∏ URL[:\s]*(.+?)(?:\n|$)",
            r"ÏõêÎ≥∏[:\s]*(.+?)(?:\n|$)",
            r"(https?://[^\s\)]+(?:\.go\.kr|\.or\.kr)[^\s\)]*)",
        ]

        for pattern in origin_patterns:
            matches = re.findall(pattern, content_md, re.IGNORECASE)
            if matches:
                url = matches[0].strip()
                if url and url.startswith("http"):
                    logger.debug(f"ÏõêÎ≥∏ URL Ï∂îÏ∂ú ÏÑ±Í≥µ: {url[:50]}...")
                    return url

        logger.debug("content.mdÏóêÏÑú ÏõêÎ≥∏ URLÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏùå")
        return ""

    def _extract_announcement_date_from_content(self, content_md: str) -> str:
        """content.mdÏóêÏÑú Í≥µÍ≥†ÏùºÏùÑ Î¨∏ÏûêÏó¥Î°ú Ï∂îÏ∂úÌï©ÎãàÎã§."""
        if not content_md:
            return ""

        # ÏûëÏÑ±Ïùº Ìå®ÌÑ¥ Ï∞æÍ∏∞ (ÎßàÌÅ¨Îã§Ïö¥ ÌòïÏãù)
        # ÏΩúÎ°†(:) Îí§Ïùò ÎÇ†ÏßúÎßå Ï†ïÌôïÌûà Ï∫°Ï≤ò
        date_patterns = [
            r"\*\*ÏûëÏÑ±Ïùº\*\*:\s*([^\n]+)",  # **ÏûëÏÑ±Ïùº**: ÎÇ†Ïßú
            r"\*\*ÏûëÏÑ±Ïùº\*\*:\*\*\s*([^\n]+)",  # **ÏûëÏÑ±Ïùº:**: ÎÇ†Ïßú
            r"ÏûëÏÑ±Ïùº:\s*([^\n]+)",  # ÏûëÏÑ±Ïùº: ÎÇ†Ïßú
            r"\*\*Îì±Î°ùÏùº\*\*:\s*([^\n]+)",  # **Îì±Î°ùÏùº**: ÎÇ†Ïßú
            r"\*\*Îì±Î°ùÏùº\*\*:\*\*\s*([^\n]+)",  # **Îì±Î°ùÏùº:**: ÎÇ†Ïßú
            r"Îì±Î°ùÏùº:\s*([^\n]+)",  # Îì±Î°ùÏùº: ÎÇ†Ïßú
            r"\*\*Í≥µÍ≥†Ïùº\*\*:\s*([^\n]+)",  # **Í≥µÍ≥†Ïùº**: ÎÇ†Ïßú
            r"\*\*Í≥µÍ≥†Ïùº\*\*:\*\*\s*([^\n]+)",  # **Í≥µÍ≥†Ïùº:**: ÎÇ†Ïßú
            r"Í≥µÍ≥†Ïùº:\s*([^\n]+)",  # Í≥µÍ≥†Ïùº: ÎÇ†Ïßú
        ]

        for pattern in date_patterns:
            matches = re.findall(pattern, content_md, re.IGNORECASE)
            if matches:
                date_str = matches[0].strip()
                if date_str:
                    # ÎßàÌÅ¨Îã§Ïö¥ Î≥ºÎìú(**) Ï†úÍ±∞
                    date_str = re.sub(r'\*+', '', date_str).strip()
                    # Ï∂îÍ∞ÄÏ†ÅÏù∏ Ï†ïÎ¶¨ (Í≥µÎ∞± Ï†úÍ±∞ Îì±)
                    date_str = date_str.strip()

                    # ÎÇ†Ïßú ÌòïÏãù Í≤ÄÏ¶ù (ÏµúÏÜåÌïú Ïó∞ÎèÑÍ∞Ä Ìè¨Ìï®ÎêòÏñ¥Ïïº Ìï®)
                    if re.search(r'\d{4}', date_str):
                        logger.debug(f"Í≥µÍ≥†Ïùº Ï∂îÏ∂ú ÏÑ±Í≥µ: {date_str}")
                        return date_str

        logger.debug("content.mdÏóêÏÑú Í≥µÍ≥†ÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏùå")
        return ""

    def _extract_attachment_urls_from_content(
        self, directory_path: Path
    ) -> Dict[str, str]:
        """content.mdÏóêÏÑú Ï≤®Î∂ÄÌååÏùº Îã§Ïö¥Î°úÎìú URLÏùÑ Ï∂îÏ∂úÌï©ÎãàÎã§."""
        content_md_path = directory_path / "content.md"
        attachment_urls = {}

        if not content_md_path.exists():
            return attachment_urls

        try:
            with open(content_md_path, "r", encoding="utf-8") as f:
                content = f.read()

            # **Ï≤®Î∂ÄÌååÏùº**: ÏÑπÏÖò Ï∞æÍ∏∞
            attachments_section = re.search(
                r"\*\*Ï≤®Î∂ÄÌååÏùº\*\*:\s*\n+(.*?)(?=\n\*\*|$)",
                content,
                re.DOTALL | re.MULTILINE,
            )

            if attachments_section:
                attachments_text = attachments_section.group(1)

                # Î™®Îì† Ï§ÑÏùÑ Ï≤òÎ¶¨ÌïòÏó¨ Î≤àÌò∏. ÌååÏùºÎ™Ö:URL Ìå®ÌÑ¥ Ï∞æÍ∏∞
                lines = attachments_text.strip().split("\n")
                for line in lines:
                    line = line.strip()
                    if not line:
                        continue

                    # Î≤àÌò∏. ÌååÏùºÎ™Ö:URL Ìå®ÌÑ¥
                    match = re.match(r"^\d+\.\s*(.+?):(https?://[^\s]+)", line)
                    if match:
                        filename = match.group(1).strip()
                        url = match.group(2).strip()
                        attachment_urls[filename] = url
                        logger.debug(f"Ï≤®Î∂ÄÌååÏùº URL Îß§Ìïë: {filename} -> {url[:50]}...")

            logger.info(
                f"Ï≤®Î∂ÄÌååÏùº URL Ï∂îÏ∂ú ÏôÑÎ£å: {len(attachment_urls)}Í∞ú, ÌÇ§: {list(attachment_urls.keys())}"
            )

        except Exception as e:
            logger.error(f"Ï≤®Î∂ÄÌååÏùº URL Ï∂îÏ∂ú Ïã§Ìå®: {e}")

        return attachment_urls

    def _normalize_korean_text(self, text: str) -> str:
        """ÌïúÍ∏Ä ÌÖçÏä§Ìä∏Î•º NFC(Composed) ÌòïÌÉúÎ°ú Ï†ïÍ∑úÌôîÌï©ÎãàÎã§."""
        return unicodedata.normalize("NFC", text)

    def _natural_sort_key(self, path: Path) -> tuple:
        """Ìè¥ÎçîÎ™ÖÏùò Ïà´Ïûê Î∂ÄÎ∂ÑÏùÑ Í∏∞Ï§ÄÏúºÎ°ú ÏûêÏó∞ Ï†ïÎ†¨ÏùÑ ÏúÑÌïú ÌÇ§Î•º ÏÉùÏÑ±Ìï©ÎãàÎã§."""
        import re

        folder_name = path.name
        # Ïà´Ïûê_Ï†úÎ™© Ìå®ÌÑ¥ÏóêÏÑú Ïà´Ïûê Î∂ÄÎ∂Ñ Ï∂îÏ∂ú
        match = re.match(r"^(\d+)_(.*)$", folder_name)
        if match:
            number = int(match.group(1))
            title = match.group(2)
            return (number, title)
        else:
            # Ïà´ÏûêÎ°ú ÏãúÏûëÌïòÏßÄ ÏïäÎäî Í≤ΩÏö∞Îäî Îß® Îí§Î°ú
            return (float("inf"), folder_name)

    def _process_attachments_separately(
        self, directory_path: Path
    ) -> tuple[str, List[str], List[Dict[str, Any]]]:
        """Ï≤®Î∂ÄÌååÏùºÎì§ÏùÑ Ï≤òÎ¶¨ÌïòÏó¨ ÎÇ¥Ïö©ÏùÑ Í≤∞Ìï©ÌïòÍ≥† ÌååÏùºÎ™Ö Î™©Î°ùÏùÑ Î∞òÌúòÌï©ÎãàÎã§."""
        attachments_dir = directory_path / "attachments"

        if not attachments_dir.exists():
            return "", [], []

        combined_content = ""
        attachment_filenames = []
        attachment_files_info = []

        # content.mdÏóêÏÑú ÌååÏùº Îã§Ïö¥Î°úÎìú URL Ï∂îÏ∂ú
        attachment_urls = self._extract_attachment_urls_from_content(directory_path)

        # Ï≤òÎ¶¨ Í∞ÄÎä•Ìïú ÌôïÏû•Ïûê Ï†ïÏùò (Excel ÌååÏùº Ï†úÏô∏)
        supported_extensions = {
            ".pdf",
            ".hwp",
            ".hwpx",
            ".png",
            ".jpg",
            ".jpeg",
            ".gif",
            ".bmp",
            ".webp",
            ".pptx",
            ".docx",
            ".md",
            ".zip",  # ZIP ÌååÏùº ÏßÄÏõê Ï∂îÍ∞Ä
        }

        target_keywords = ["ÏñëÏãù", "ÏÑúÎ•ò", "Ïã†Ï≤≠ÏÑú", "ÎèôÏùòÏÑú"]
        
        # ÌååÏùºÎì§ÏùÑ Ïö∞ÏÑ†ÏàúÏúÑÏóê Îî∞Îùº Î∂ÑÎ•ò
        priority_files = []  # ÏßÄÏõê/Í≥µÍ≥† ÌÇ§ÏõåÎìúÍ∞Ä ÏûàÎäî ÌååÏùºÎì§
        normal_files = []    # ÏùºÎ∞ò ÌååÏùºÎì§
        
        # Î™®Îì† ÌååÏùºÏùÑ Î®ºÏ†Ä Í≤ÄÏÇ¨ÌïòÏó¨ Î∂ÑÎ•ò
        for file_path in attachments_dir.iterdir():
            if file_path.is_file():
                file_extension = file_path.suffix.lower()
                filename = file_path.stem
                lowercase_filename = filename.lower()
                
                # ÏßÄÏõêÌïòÎäî ÌôïÏû•ÏûêÎßå Ï≤òÎ¶¨
                if file_extension and file_extension in supported_extensions:
                    # ÏñëÏãù, ÏÑúÎ•ò Îì± Ï†úÏô∏ ÌÇ§ÏõåÎìú Ï≤¥ÌÅ¨
                    if any(keyword in lowercase_filename for keyword in target_keywords):
                        continue
                    
                    # ÏßÄÏõê/Í≥µÍ≥† ÌÇ§ÏõåÎìúÍ∞Ä ÏûàÎäîÏßÄ ÌôïÏù∏
                    if "ÏßÄÏõê" in lowercase_filename or "Í≥µÍ≥†" in lowercase_filename:
                        priority_files.append(file_path)
                        logger.info(f"Ïö∞ÏÑ†ÏàúÏúÑ ÌååÏùº Î∞úÍ≤¨ (ÏßÄÏõê/Í≥µÍ≥† ÌÇ§ÏõåÎìú): {file_path.name}")
                    else:
                        normal_files.append(file_path)
        
        # Ïö∞ÏÑ†ÏàúÏúÑ ÌååÏùºÎì§ÏùÑ Î®ºÏ†Ä Ï≤òÎ¶¨, Í∑∏ Îã§Ïùå ÏùºÎ∞ò ÌååÏùºÎì§ Ï≤òÎ¶¨
        all_files_ordered = priority_files + normal_files
        
        for file_path in all_files_ordered:
            # Ïù¥ÎØ∏ ÏúÑÏóêÏÑú ÌïÑÌÑ∞ÎßÅ ÌñàÏúºÎØÄÎ°ú Î∞îÎ°ú Ï≤òÎ¶¨
            file_extension = file_path.suffix.lower()
            filename = file_path.stem

            logger.info(f"filename===={filename}{file_extension}")

            attachment_filenames.append(self._normalize_korean_text(file_path.name))
            logger.info(f"Ï≤®Î∂ÄÌååÏùº Ï≤òÎ¶¨ ÏãúÏûë: {file_path.name}")

            # ÌååÏùº Ï†ïÎ≥¥ ÏàòÏßë (Î™®Îì† ÌååÏùºÏóê ÎåÄÌï¥)
            # URL Îß§Ïπ≠ ÏãúÎèÑ - ÌååÏùºÎ™ÖÏúºÎ°ú Î®ºÏ†Ä ÏãúÎèÑ, ÏóÜÏúºÎ©¥ stemÏúºÎ°ú ÏãúÎèÑ
            download_url = attachment_urls.get(file_path.name, "")
            if not download_url:
                # ÌôïÏû•Ïûê ÏóÜÎäî Ïù¥Î¶ÑÏúºÎ°úÎèÑ ÏãúÎèÑ
                download_url = attachment_urls.get(file_path.stem, "")

            if download_url:
                logger.debug(
                    f"URL Îß§Ïπ≠ ÏÑ±Í≥µ: {file_path.name} -> {download_url[:50]}..."
                )
            else:
                logger.debug(
                    f"URL Îß§Ïπ≠ Ïã§Ìå®: {file_path.name}, Í∞ÄÎä•Ìïú ÌÇ§: {list(attachment_urls.keys())[:3]}"
                )

            file_info = {
                "filename": file_path.name,  # ÌôïÏû•Ïûê Ìè¨Ìï®Îêú Ï†ÑÏ≤¥ ÌååÏùºÎ™Ö
                "file_size": (
                    file_path.stat().st_size if file_path.exists() else 0
                ),
                "conversion_success": False,
                "conversion_method": self._guess_conversion_method(
                    file_extension
                ),
                "download_url": download_url,  # Îã§Ïö¥Î°úÎìú URL Ï∂îÍ∞Ä
            }

            # md ÌååÏùºÏù¥ ÏïÑÎãå Í≤ΩÏö∞Îßå attachment_files_infoÏóê Ï∂îÍ∞Ä
            if file_extension != ".md":
                attachment_files_info.append(file_info)

            # Ïù¥ÎØ∏ .md ÌååÏùºÏù∏ Í≤ΩÏö∞ ÏßÅÏ†ë ÏùΩÍ∏∞
            if file_extension == ".md":
                try:
                    with open(file_path, "r", encoding="utf-8") as f:
                        content = f.read()
                    if content.strip():
                        combined_content += f"\n\n=== {self._normalize_korean_text(file_path.name)} ===\n{content}"
                        logger.info(
                            f"Ï≤®Î∂ÄÌååÏùº .md ÏßÅÏ†ë ÏùΩÍ∏∞ ÏÑ±Í≥µ: {file_path.name} ({len(content)} Î¨∏Ïûê)"
                        )
                        file_info["conversion_success"] = True
                    else:
                        logger.warning(
                            f"Ï≤®Î∂ÄÌååÏùº .md ÎÇ¥Ïö©Ïù¥ ÎπÑÏñ¥ÏûàÏùå: {file_path.name}"
                        )
                except Exception as e:
                    logger.error(f"Ï≤®Î∂ÄÌååÏùº .md ÏßÅÏ†ë ÏùΩÍ∏∞ Ïã§Ìå®: {e}")
                continue

            # Ï≤®Î∂ÄÌååÏùºÎ™Ö.md ÌååÏùºÏù¥ ÏûàÎäîÏßÄ ÌôïÏù∏
            md_file_path = attachments_dir / f"{filename}.md"
            logger.debug(f"md_file_path: {md_file_path}")

            # attach_forceÍ∞Ä TrueÏù¥Î©¥ Í∏∞Ï°¥ .md ÌååÏùºÏùÑ Î¨¥ÏãúÌïòÍ≥† ÏõêÎ≥∏ÏóêÏÑú Ïû¨Î≥ÄÌôò
            if not self.attach_force and md_file_path.exists():
                # .md ÌååÏùºÏù¥ ÏûàÏúºÎ©¥ Í∑∏Í≤ÉÏùÑ ÏùΩÏùå
                try:
                    with open(md_file_path, "r", encoding="utf-8") as f:
                        content = f.read()
                    if content.strip():
                        combined_content += f"\n\n=== {self._normalize_korean_text(filename)}.md ===\n{content}"
                        logger.debug(
                            f"Ï≤®Î∂ÄÌååÏùº .md ÏùΩÍ∏∞ ÏÑ±Í≥µ: {filename}.md ({len(content)} Î¨∏Ïûê)"
                        )
                        file_info["conversion_success"] = True
                    else:
                        logger.warning(
                            f"Ï≤®Î∂ÄÌååÏùº .md ÎÇ¥Ïö©Ïù¥ ÎπÑÏñ¥ÏûàÏùå: {filename}.md"
                        )
                except Exception as e:
                    logger.error(f"Ï≤®Î∂ÄÌååÏùº .md ÏùΩÍ∏∞ Ïã§Ìå®: {e}")
            else:
                # .md ÌååÏùºÏù¥ ÏóÜÍ±∞ÎÇò attach_forceÍ∞Ä TrueÏù¥Î©¥ ÏõêÎ≥∏ ÌååÏùºÏùÑ Î≥ÄÌôò
                if self.attach_force and md_file_path.exists():
                    logger.info(
                        f"--attach-force: Í∏∞Ï°¥ .md ÌååÏùº Î¨¥ÏãúÌïòÍ≥† Ïû¨Î≥ÄÌôò: {file_path.name}"
                    )
                else:
                    logger.info(f"Ï≤®Î∂ÄÌååÏùº Î≥ÄÌôò ÏãúÏûë: {file_path.name}")

                try:
                    # attachment_processorÍ∞Ä NoneÏù∏ Í≤ΩÏö∞ Ï≤òÎ¶¨
                    if self.attachment_processor is None:
                        logger.warning(f"AttachmentProcessorÎ•º ÏÇ¨Ïö©Ìï† Ïàò ÏóÜÏñ¥ ÌååÏùº Í±¥ÎÑàÎúÄ: {file_path.name}")
                        continue
                        
                    content = self.attachment_processor.process_single_file(
                        file_path
                    )

                    if content and content.strip():
                        combined_content += f"\n\n=== {self._normalize_korean_text(file_path.name)} ===\n{content}"
                        logger.info(
                            f"Ï≤®Î∂ÄÌååÏùº Î≥ÄÌôò ÏÑ±Í≥µ: {file_path.name} ({len(content)} Î¨∏Ïûê)"
                        )
                        file_info["conversion_success"] = True

                        # Î≥ÄÌôòÎêú ÎÇ¥Ïö©ÏùÑ .md ÌååÏùºÎ°ú Ï†ÄÏû•
                        try:
                            with open(md_file_path, "w", encoding="utf-8") as f:
                                f.write(content)
                            logger.debug(
                                f"Î≥ÄÌôòÎêú ÎÇ¥Ïö©ÏùÑ .mdÎ°ú Ï†ÄÏû•: {md_file_path}"
                            )
                        except Exception as save_e:
                            logger.warning(f".md ÌååÏùº Ï†ÄÏû• Ïã§Ìå®: {save_e}")
                    else:
                        logger.warning(
                            f"Ï≤®Î∂ÄÌååÏùºÏóêÏÑú ÎÇ¥Ïö© Ï∂îÏ∂ú Ïã§Ìå®: {file_path.name}"
                        )

                except Exception as e:
                    error_msg = str(e)
                    if "Invalid code point" in error_msg or "PDFSyntaxError" in error_msg or "No /Root object" in error_msg:
                        logger.warning(f"ÏÜêÏÉÅÎêú PDF ÌååÏùº Í±¥ÎÑàÎõ∞Í∏∞: {file_path.name}")
                    elif "UnicodeDecodeError" in error_msg:
                        logger.warning(f"Ïù∏ÏΩîÎî© Î¨∏Ï†úÎ°ú ÌååÏùº Í±¥ÎÑàÎõ∞Í∏∞: {file_path.name}")
                    else:
                        logger.error(f"Ï≤®Î∂ÄÌååÏùº Î≥ÄÌôò Ïã§Ìå® ({file_path.name}): {e}")

                    # Î≥ÄÌôò Ïã§Ìå®Ìïú ÌååÏùº Ï†ïÎ≥¥ Í∏∞Î°ù
                    file_info["conversion_success"] = False
                    file_info["error_message"] = error_msg[:200]  # Ïò§Î•ò Î©îÏãúÏßÄ ÏùºÎ∂ÄÎßå Ï†ÄÏû•

        logger.info(
            f"Ï≤®Î∂ÄÌååÏùº Ï≤òÎ¶¨ ÏôÑÎ£å: {len(attachment_filenames)}Í∞ú ÌååÏùº, {len(combined_content)} Î¨∏Ïûê"
        )
        return combined_content.strip(), attachment_filenames, attachment_files_info

    def _guess_conversion_method(self, file_extension: str) -> str:
        """ÌååÏùº ÌôïÏû•ÏûêÏóê Îî∞Î•∏ Î≥ÄÌôò Î∞©Î≤ïÏùÑ Ï∂îÏ†ïÌï©ÎãàÎã§."""
        ext_lower = file_extension.lower()

        if ext_lower == ".pdf":
            return "pdf_docling"
        elif ext_lower in [".hwp", ".hwpx"]:
            return "hwp_markdown"
        elif ext_lower in [".png", ".jpg", ".jpeg", ".gif", ".bmp", ".webp"]:
            return "ocr"
        else:
            return "unknown"

    def _convert_to_yyyymmdd(self, date_str: str) -> str:
        """ÎÇ†Ïßú Î¨∏ÏûêÏó¥ÏùÑ YYYYMMDD Ìè¨Îß∑ÏúºÎ°ú Î≥ÄÌôòÌï©ÎãàÎã§."""
        try:
            # Îã§ÏñëÌïú ÎÇ†Ïßú Ìè¨Îß∑ ÏãúÎèÑ
            from datetime import datetime

            # Í∞ÄÎä•Ìïú ÎÇ†Ïßú Ìè¨Îß∑Îì§
            date_formats = [
                "%Y-%m-%d",
                "%Y.%m.%d",
                "%Y/%m/%d",
                "%Y%m%d",
                "%YÎÖÑ %mÏõî %dÏùº",
                "%Y-%m-%d %H:%M:%S",
                "%Y.%m.%d %H:%M:%S",
            ]

            for fmt in date_formats:
                try:
                    dt = datetime.strptime(date_str.strip(), fmt)
                    return dt.strftime("%Y%m%d")
                except ValueError:
                    continue

            # Î™®Îì† Ìè¨Îß∑ Ïã§Ìå®Ïãú ÏõêÎ≥∏ Î∞òÌôò
            logger.warning(f"ÎÇ†Ïßú Î≥ÄÌôò Ïã§Ìå®, ÏõêÎ≥∏ Î∞òÌôò: {date_str}")
            return date_str

        except Exception as e:
            logger.error(f"ÎÇ†Ïßú Î≥ÄÌôò Ï§ë Ïò§Î•ò: {e}")
            return date_str

    def _save_processing_result(
        self,
        folder_name: str,
        site_code: str,
        content_md: str,
        combined_content: str,
        attachment_filenames: List[str] = None,
        status: str = "ÏÑ±Í≥µ",
        exclusion_keywords: List[str] = None,
        exclusion_reason: str = None,
        error_message: str = None,
        force: bool = False,
        title: str = None,
        origin_url: str = None,
        announcement_date: str = None,
        attachment_files_info: List[Dict[str, Any]] = None,
    ) -> Optional[int]:
        """Ï≤òÎ¶¨ Í≤∞Í≥ºÎ•º Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§Ïóê Ï†ÄÏû•Ìï©ÎãàÎã§."""
        try:
            from sqlalchemy import text

            with self.db_manager.SessionLocal() as session:
                if force:
                    # UPSERT Î°úÏßÅ
                    sql = text(
                        """
                        INSERT INTO announcement_pre_processing (
                            folder_name, site_type, site_code, content_md, combined_content,
                            attachment_filenames, attachment_files_list, exclusion_keyword, exclusion_reason, 
                            title, origin_url, announcement_date,
                            processing_status, error_message, created_at, updated_at
                        ) VALUES (
                            :folder_name, :site_type, :site_code, :content_md, :combined_content,
                            :attachment_filenames, :attachment_files_list, :exclusion_keyword, :exclusion_reason, 
                            :title, :origin_url, :announcement_date,
                            :processing_status, :error_message, NOW(), NOW()
                        )
                        ON DUPLICATE KEY UPDATE
                            site_type = VALUES(site_type),
                            content_md = VALUES(content_md),
                            combined_content = VALUES(combined_content),
                            attachment_filenames = VALUES(attachment_filenames),
                            attachment_files_list = VALUES(attachment_files_list),
                            exclusion_keyword = VALUES(exclusion_keyword),
                            exclusion_reason = VALUES(exclusion_reason),
                            processing_status = VALUES(processing_status),
                            title = VALUES(title),
                            origin_url = VALUES(origin_url),
                            announcement_date = VALUES(announcement_date),
                            error_message = VALUES(error_message),
                            updated_at = NOW()
                    """
                    )
                else:
                    # ÏùºÎ∞ò INSERT
                    sql = text(
                        """
                        INSERT INTO announcement_pre_processing (
                            folder_name, site_type, site_code, content_md, combined_content,
                            attachment_filenames, attachment_files_list, exclusion_keyword, exclusion_reason, 
                            title, origin_url, announcement_date,
                            processing_status, error_message, created_at, updated_at
                        ) VALUES (
                            :folder_name, :site_type, :site_code, :content_md, :combined_content,
                            :attachment_filenames, :attachment_files_list, :exclusion_keyword, :exclusion_reason, 
                            :title, :origin_url, :announcement_date,
                            :processing_status, :error_message, NOW(), NOW()
                        )
                    """
                    )

                # JSONÏúºÎ°ú ÏßÅÎ†¨Ìôî
                attachment_files_json = (
                    json.dumps(attachment_files_info, ensure_ascii=False)
                    if attachment_files_info
                    else None
                )

                params = {
                    "folder_name": folder_name,
                    "site_type": self.site_type,
                    "site_code": site_code,
                    "content_md": content_md,
                    "combined_content": combined_content,
                    "attachment_filenames": (
                        ", ".join(attachment_filenames)
                        if attachment_filenames
                        else None
                    ),
                    "attachment_files_list": attachment_files_json,
                    "exclusion_keyword": (
                        ", ".join(exclusion_keywords) if exclusion_keywords else None
                    ),
                    "exclusion_reason": exclusion_reason,
                    "title": title,
                    "origin_url": origin_url,
                    "announcement_date": announcement_date,
                    "processing_status": status,
                    "error_message": error_message,
                }

                result = session.execute(sql, params)
                session.commit()

                record_id = result.lastrowid
                logger.info(f"Ï≤òÎ¶¨ Í≤∞Í≥º Ï†ÄÏû• ÏôÑÎ£å: ID {record_id}, ÏÉÅÌÉú: {status}")
                return record_id

        except Exception as e:
            logger.error(f"Ï≤òÎ¶¨ Í≤∞Í≥º Ï†ÄÏû• Ïã§Ìå®: {e}")
            return None


def determine_site_type(directory_name: str, site_code: str) -> str:
    """ÎîîÎ†âÌÜ†Î¶¨Î™ÖÍ≥º ÏÇ¨Ïù¥Ìä∏ ÏΩîÎìúÏóêÏÑú site_typeÏùÑ Í≤∞Ï†ïÌï©ÎãàÎã§."""
    # ÌäπÏàò API ÏÇ¨Ïù¥Ìä∏ Ï≤¥ÌÅ¨
    if site_code in ["kStartUp", "bizInfo", "sme"]:
        return "api_scrap"
    elif "scraped" in directory_name.lower():
        return "Homepage"
    elif "eminwon" in directory_name.lower():
        return "Eminwon"
    else:
        return "Unknown"


def main():
    """Î©îÏù∏ Ìï®Ïàò"""
    parser = argparse.ArgumentParser(
        description="Í≥µÍ≥† ÏÇ¨Ï†Ñ Ï≤òÎ¶¨ ÌîÑÎ°úÍ∑∏Îû®",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ÏòàÏãú:
  python announcement_pre_processor.py -d scraped_data --site-code site001
  python announcement_pre_processor.py -d eminwon_data --site-code emw001
  python announcement_pre_processor.py -d scraped_data --site-code site001 --force
  python announcement_pre_processor.py -d eminwon_data --site-code emw001 --attach-force
        """,
    )

    parser.add_argument(
        "-d", "--directory", type=str, required=True, help="Îç∞Ïù¥ÌÑ∞ ÎîîÎ†âÌÜ†Î¶¨Î™Ö (ÌïÑÏàò)"
    )

    parser.add_argument(
        "--site-code", type=str, required=True, help="ÏÇ¨Ïù¥Ìä∏ ÏΩîÎìú (ÌïÑÏàò)"
    )

    parser.add_argument(
        "--force", action="store_true", help="Ïù¥ÎØ∏ Ï≤òÎ¶¨Îêú Ìï≠Î™©ÎèÑ Îã§Ïãú Ï≤òÎ¶¨"
    )

    parser.add_argument(
        "--attach-force",
        action="store_true",
        help="Ï≤®Î∂ÄÌååÏùº Í∞ïÏ†ú Ïû¨Ï≤òÎ¶¨ (Í∏∞Ï°¥ .md ÌååÏùº Î¨¥ÏãúÌïòÍ≥† ÏõêÎ≥∏ ÌååÏùºÏóêÏÑú Îã§Ïãú Î≥ÄÌôò)",
    )

    args = parser.parse_args()

    try:
        # Í∏∞Î≥∏ ÎîîÎ†âÌÜ†Î¶¨ Í≤∞Ï†ï
        current_dir = Path.cwd()
        base_directory = current_dir / args.directory

        if not base_directory.exists():
            logger.error(f"ÎîîÎ†âÌÜ†Î¶¨Í∞Ä Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏäµÎãàÎã§: {base_directory}")
            sys.exit(1)

        # site_type Í≤∞Ï†ï
        site_type = determine_site_type(args.directory, args.site_code)

        logger.info(f"Í∏∞Î≥∏ ÎîîÎ†âÌÜ†Î¶¨: {base_directory}")
        logger.info(f"Site Type: {site_type}")
        logger.info(f"Site Code: {args.site_code}")

        # ÌîÑÎ°úÏÑ∏ÏÑú Ï¥àÍ∏∞Ìôî
        logger.info("Í≥µÍ≥† ÏÇ¨Ï†Ñ Ï≤òÎ¶¨ ÌîÑÎ°úÍ∑∏Îû® ÏãúÏûë")

        processor = AnnouncementPreProcessor(
            site_type=site_type,
            attach_force=args.attach_force,
            site_code=args.site_code,
            lazy_init=False
        )

        # ÏÇ¨Ïù¥Ìä∏ ÎîîÎ†âÌÜ†Î¶¨ Ï≤òÎ¶¨ Ïã§Ìñâ
        results = processor.process_site_directories(
            base_directory, args.site_code, args.force
        )

        # Í≤∞Í≥º Ï∂úÎ†•
        print(f"\n=== ÏµúÏ¢Ö ÏöîÏïΩ ===")
        print(f"Ï†ÑÏ≤¥ ÎåÄÏÉÅ: {results['total']}Í∞ú")
        print(f"Ï≤òÎ¶¨ ÏÑ±Í≥µ: {results['success']}Í∞ú")
        print(f"Ï≤òÎ¶¨ Ïã§Ìå®: {results['failed']}Í∞ú")
        print(f"Í±¥ÎÑàÎõ¥ Ìï≠Î™©: {results['skipped']}Í∞ú")

        if results["failed"] > 0:
            print(
                f"\nÏã§Ìå®Ìïú Ìï≠Î™©Ïù¥ {results['failed']}Í∞ú ÏûàÏäµÎãàÎã§. Î°úÍ∑∏Î•º ÌôïÏù∏Ìï¥Ï£ºÏÑ∏Ïöî."
            )
            sys.exit(1)
        else:
            print("\nÎ™®Îì† Ï≤òÎ¶¨Í∞Ä ÏôÑÎ£åÎêòÏóàÏäµÎãàÎã§!")
            sys.exit(0)

    except KeyboardInterrupt:
        logger.info("ÏÇ¨Ïö©ÏûêÏóê ÏùòÌï¥ Ï§ëÎã®Îê®")
        sys.exit(1)
    except Exception as e:
        logger.error(f"ÌîÑÎ°úÍ∑∏Îû® Ïã§Ìñâ Ï§ë Ïò§Î•ò: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
