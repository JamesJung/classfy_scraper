# announcement_duplicate_log í…Œì´ë¸” ìƒì„¸ ì„¤ê³„ ë° êµ¬í˜„ ê°€ì´ë“œ

## ğŸ“‹ ëª©ì°¨

1. [í•µì‹¬ ì›ì¹™](#í•µì‹¬-ì›ì¹™)
2. [í…Œì´ë¸” ì„¤ê³„](#í…Œì´ë¸”-ì„¤ê³„)
3. [ì¤‘ë³µ ì²´í¬ ë¡œì§](#ì¤‘ë³µ-ì²´í¬-ë¡œì§)
4. [ë¡œê·¸ ê¸°ë¡ ì‹œë‚˜ë¦¬ì˜¤](#ë¡œê·¸-ê¸°ë¡-ì‹œë‚˜ë¦¬ì˜¤)
5. [ì½”ë“œ êµ¬í˜„](#ì½”ë“œ-êµ¬í˜„)
6. [í™œìš© ë°©ì•ˆ](#í™œìš©-ë°©ì•ˆ)
7. [í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤](#í…ŒìŠ¤íŠ¸-ì‹œë‚˜ë¦¬ì˜¤)

---

## ğŸ¯ í•µì‹¬ ì›ì¹™

### ë¡œê·¸ ê¸°ë¡ ì›ì¹™

**ê¸°ë¡ ëŒ€ìƒ**: `url_key_hash` ê¸°ë°˜ ì¤‘ë³µ ì²´í¬ ì‹œ ë°œìƒí•˜ëŠ” ëª¨ë“  ì‹œë„

**ê¸°ë¡ ì‹œì **: `announcement_pre_processing` í…Œì´ë¸” ì €ì¥ ì§í›„

**ê¸°ë¡ ë²”ìœ„**:
- âœ… **ë„ë©”ì¸ ë¯¸ì„¤ì •**: domain_key_configì— ì„¤ì •ì´ ì—†ì–´ì„œ url_keyê°€ ìƒì„±ë˜ì§€ ì•Šì€ ê²½ìš°
- âœ… **ì‹ ê·œ ì‚½ì…**: url_key_hash ì¤‘ë³µì´ ì—†ì–´ì„œ ìƒˆë¡œ ì €ì¥ëœ ê²½ìš°
- âœ… **ì¤‘ë³µ ë°œê²¬ - êµì²´**: ìš°ì„ ìˆœìœ„ê°€ ë†’ì•„ì„œ ê¸°ì¡´ ë°ì´í„°ë¥¼ êµì²´í•œ ê²½ìš° (UPSERT UPDATE)
- âœ… **ì¤‘ë³µ ë°œê²¬ - ìœ ì§€**: ìš°ì„ ìˆœìœ„ê°€ ë‚®ì•„ì„œ ê¸°ì¡´ ë°ì´í„°ë¥¼ ìœ ì§€í•œ ê²½ìš°
- âœ… **ì¤‘ë³µ ë°œê²¬ - ë™ì¼**: ìš°ì„ ìˆœìœ„ê°€ ê°™ì•„ì„œ ìµœì‹  ë°ì´í„°ë¡œ ì—…ë°ì´íŠ¸í•œ ê²½ìš° (UPSERT UPDATE)
- âœ… **ì˜¤ë¥˜**: ì˜ˆìƒì¹˜ ëª»í•œ ì²˜ë¦¬ ì˜¤ë¥˜ ë°œìƒ ì‹œ

---

## ğŸ“Š í…Œì´ë¸” ì„¤ê³„

### ê¸°ì¡´ ìŠ¤í‚¤ë§ˆ (create_announcement_duplicate_log.sql)

```sql
CREATE TABLE IF NOT EXISTS announcement_duplicate_log (
    -- ê¸°ë³¸ ì •ë³´
    id BIGINT PRIMARY KEY AUTO_INCREMENT,

    -- ë©”ì¸ í…Œì´ë¸” ì°¸ì¡°
    preprocessing_id INT NOT NULL COMMENT 'ìµœì¢… ì €ì¥ëœ announcement_pre_processing.id',
    existing_preprocessing_id INT COMMENT 'ê¸°ì¡´ ë ˆì½”ë“œ ID (ì¤‘ë³µ ë°œìƒì‹œ)',

    -- ì¤‘ë³µ ë°œìƒ ì •ë³´
    duplicate_type VARCHAR(50) NOT NULL COMMENT 'ì¤‘ë³µ ìœ í˜•',
    /*
        - 'unconfigured_domain': domain_key_configì— ì„¤ì • ì—†ìŒ (url_key ìƒì„± ì‹¤íŒ¨)
        - 'new_inserted': ì‹ ê·œ ì‚½ì… (ì¤‘ë³µ ì•„ë‹˜)
        - 'replaced': ê¸°ì¡´ ë°ì´í„° êµì²´ë¨ (ìƒˆ ë°ì´í„° ìš°ì„ ìˆœìœ„ ë†’ìŒ, UPSERT UPDATE)
        - 'kept_existing': ê¸°ì¡´ ë°ì´í„° ìœ ì§€ë¨ (ìƒˆ ë°ì´í„° ìš°ì„ ìˆœìœ„ ë‚®ìŒ)
        - 'same_type_duplicate': ê°™ì€ íƒ€ì… ì¤‘ë³µ (ìš°ì„ ìˆœìœ„ ë™ì¼, UPSERT UPDATE)
        - 'error': ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ
        - 'unknown': ì˜ˆìƒì¹˜ ëª»í•œ ìƒíƒœ
    */

    -- â­ í•µì‹¬: URL ì‹ë³„ì (ì¤‘ë³µ ì²´í¬ ê¸°ì¤€)
    url_key_hash CHAR(32) COMMENT 'URL í‚¤ í•´ì‹œ (MD5)',

    -- íƒ€ì… ì •ë³´
    new_site_type VARCHAR(50) NOT NULL,
    new_site_code VARCHAR(50) NOT NULL,
    existing_site_type VARCHAR(50),
    existing_site_code VARCHAR(50),

    -- ìš°ì„ ìˆœìœ„ ì •ë³´
    new_priority TINYINT,
    existing_priority TINYINT,

    -- ì¤‘ë³µ ìƒì„¸ ì •ë³´
    duplicate_detail JSON,
    new_folder_name VARCHAR(255),
    error_message TEXT,

    -- íƒ€ì„ìŠ¤íƒ¬í”„
    created_at DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP,

    -- â­ í•µì‹¬ ì¸ë±ìŠ¤: url_key_hashë¡œ ê²€ìƒ‰
    INDEX idx_url_key_hash (url_key_hash),
    INDEX idx_preprocessing_id (preprocessing_id),
    INDEX idx_existing_id (existing_preprocessing_id),
    INDEX idx_duplicate_type (duplicate_type),
    INDEX idx_created_at (created_at),
    INDEX idx_new_site_code (new_site_code),

    -- ì™¸ë˜í‚¤
    CONSTRAINT fk_preprocessing_id
        FOREIGN KEY (preprocessing_id)
        REFERENCES announcement_pre_processing(id)
        ON DELETE CASCADE
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
```

### ì„¤ê³„ í¬ì¸íŠ¸

#### 1. url_key_hashê°€ í•µì‹¬

```
ì¤‘ë³µ ì²´í¬ íë¦„:
1. origin_url ì¶”ì¶œ
2. url_key ìƒì„± (URL ì •ê·œí™”)
3. url_key_hash ê³„ì‚° (MD5)
4. announcement_pre_processingì—ì„œ url_key_hashë¡œ ê¸°ì¡´ ë ˆì½”ë“œ ì¡°íšŒ
5. ì¡°íšŒ ê²°ê³¼ì— ë”°ë¼ duplicate_type ê²°ì •
6. announcement_duplicate_log ê¸°ë¡
```

#### 2. ì¤‘ë³µ ìœ í˜• (duplicate_type)

| duplicate_type | ë°œìƒ ì¡°ê±´ | ì„¤ëª… |
|---------------|----------|------|
| `unconfigured_domain` | domain_key_configì— ì„¤ì • ì—†ìŒ | url_key ìƒì„± ì‹¤íŒ¨ (Fallback ë¹„í™œì„±í™”) |
| `new_inserted` | url_key_hashê°€ DBì— ì—†ìŒ | ì™„ì „íˆ ìƒˆë¡œìš´ ê³µê³  (UPSERT INSERT) |
| `replaced` | url_key_hash ì¤‘ë³µ + ìƒˆ ìš°ì„ ìˆœìœ„ > ê¸°ì¡´ ìš°ì„ ìˆœìœ„ | ì§€ìì²´ê°€ API êµì²´ (UPSERT UPDATE) |
| `kept_existing` | url_key_hash ì¤‘ë³µ + ìƒˆ ìš°ì„ ìˆœìœ„ < ê¸°ì¡´ ìš°ì„ ìˆœìœ„ | APIê°€ ì§€ìì²´ ìœ ì§€ (UPSERTì—ì„œ ì¡°ê±´ ë¯¸ì¶©ì¡±) |
| `same_type_duplicate` | url_key_hash ì¤‘ë³µ + ìƒˆ ìš°ì„ ìˆœìœ„ = ê¸°ì¡´ ìš°ì„ ìˆœìœ„ | ê°™ì€ íƒ€ì… ì¬ìˆ˜ì§‘ (UPSERT UPDATE) |
| `error` | ì˜ˆìƒì¹˜ ëª»í•œ ì²˜ë¦¬ ì˜¤ë¥˜ | UPSERT affected_rows ì´ìƒê°’ ë“± |
| `unknown` | ë§¤í•‘ë˜ì§€ ì•Šì€ ìƒíƒœ | ì˜ˆìƒì¹˜ ëª»í•œ ìƒíƒœ (ë²„ê·¸ ê°€ëŠ¥ì„±) |

#### 3. ìš°ì„ ìˆœìœ„ ì²´ê³„

```python
def get_priority(site_type: str) -> int:
    """
    ìš°ì„ ìˆœìœ„ ë§¤í•‘ (ë†’ì„ìˆ˜ë¡ ìš°ì„ )
    """
    priority_map = {
        'Eminwon': 3,    # ì§€ìì²´ ë¯¼ì›
        'Homepage': 3,   # ì§€ìì²´ í™ˆí˜ì´ì§€
        'Scraper': 3,    # ì§€ìì²´ ìŠ¤í¬ë ˆì´í¼
        'api_scrap': 1,  # API ìˆ˜ì§‘
        'Unknown': 0,    # ì•Œ ìˆ˜ ì—†ìŒ
    }
    return priority_map.get(site_type, 0)
```

**ìš°ì„ ìˆœìœ„ ê·œì¹™**:
- ì§€ìì²´ ë°ì´í„° (Eminwon/Homepage/Scraper) = 3
- API ë°ì´í„° (api_scrap) = 1
- **ì§€ìì²´ > API**: ì§€ìì²´ ë°ì´í„°ê°€ API ë°ì´í„°ë¥¼ ë®ì–´ì”€
- **API < ì§€ìì²´**: API ë°ì´í„°ê°€ ì§€ìì²´ ë°ì´í„°ë¥¼ ìœ ì§€í•¨

---

## ğŸ” ì¤‘ë³µ ì²´í¬ ë¡œì§

### ì „ì²´ íë¦„ë„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. content.md ì½ê¸° â†’ origin_url ì¶”ì¶œ                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. URL ì •ê·œí™”: origin_url â†’ url_key                         â”‚
â”‚    - domain_key_config ì¡°íšŒ                                  â”‚
â”‚    - ì„¤ì • ì—†ìœ¼ë©´ url_key = NULL (Fallback ë¹„í™œì„±í™”)         â”‚
â”‚    - ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì •ë ¬ ë“±                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
                    url_keyê°€ ì—†ìŒ?
                    â†™ YES        â†˜ NO
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ duplicate_type  â”‚    â”‚ 3. UPSERT ì‹¤í–‰             â”‚
         â”‚ = 'unconfigured_â”‚    â”‚    INSERT ... ON DUPLICATEâ”‚
         â”‚ domain'         â”‚    â”‚    KEY UPDATE ... WHERE   â”‚
         â”‚                 â”‚    â”‚    ìš°ì„ ìˆœìœ„ ì¡°ê±´          â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“                          â†“
                  â†“                   affected_rows?
                  â†“              â†™ 1           â†“ 2          â†˜ ê¸°íƒ€
                  â†“    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â†“    â”‚new_inserted  â”‚ â”‚duplicate_  â”‚ â”‚error     â”‚
                  â†“    â”‚(INSERTë¨)    â”‚ â”‚updated     â”‚ â”‚(ì˜ˆì™¸ê°’)  â”‚
                  â†“    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚(UPDATEë¨)  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“                          â†“
                  â†“                     ìš°ì„ ìˆœìœ„ ë¹„êµ
                  â†“                  â†™ >      â†“ =      â†˜ <
                  â†“         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â†“         â”‚replaced  â”‚ â”‚same_type_ â”‚ â”‚kept_existing â”‚
                  â†“         â”‚          â”‚ â”‚duplicate  â”‚ â”‚(ì¡°ê±´ ë¯¸ì¶©ì¡±) â”‚
                  â†“         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“                          â†“
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. url_key_hash ì¡°íšŒ (GENERATED COLUMN)                     â”‚
â”‚    - DBì—ì„œ ìë™ ìƒì„±ëœ url_key_hash ì¡°íšŒ                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. announcement_duplicate_log ê¸°ë¡ â­                        â”‚
â”‚    - preprocessing_id: ì €ì¥ëœ ë ˆì½”ë“œ ID                     â”‚
â”‚    - existing_preprocessing_id: ê¸°ì¡´ ë ˆì½”ë“œ ID (ì¤‘ë³µ ì‹œ)    â”‚
â”‚    - url_key_hash: DB ìƒì„± í•´ì‹œ (GENERATED COLUMN)          â”‚
â”‚    - duplicate_type: ì¤‘ë³µ ìœ í˜•                               â”‚
â”‚    - new/existing_site_type/code: íƒ€ì… ì •ë³´                 â”‚
â”‚    - new/existing_priority: ìš°ì„ ìˆœìœ„                        â”‚
â”‚    - duplicate_detail: ìƒì„¸ ì •ë³´ (JSON, domain í¬í•¨)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ ë¡œê·¸ ê¸°ë¡ ì‹œë‚˜ë¦¬ì˜¤

### ì‹œë‚˜ë¦¬ì˜¤ 1: ì‹ ê·œ ì‚½ì… (new_inserted)

**ìƒí™©**: bizInfoì—ì„œ ìƒˆë¡œìš´ ê³µê³  ìˆ˜ì§‘

**ë°ì´í„°**:
```python
origin_url = "https://www.bizinfo.go.kr/web/lay1/bbs/S1T122C128/AS/74/view.do?pblancId=PBLN_000000000100001"
url_key = "bizinfo.go.kr/web/lay1/bbs/S1T122C128/AS/74/view.do?pblancId=PBLN_000000000100001"
url_key_hash = MD5(url_key) = "abc123def456..."
site_type = "api_scrap"
site_code = "bizInfo"
```

**ì¤‘ë³µ ì²´í¬**:
```sql
SELECT id, site_type, site_code
FROM announcement_pre_processing
WHERE url_key_hash = 'abc123def456...'
-- ê²°ê³¼: ì—†ìŒ (ì‹ ê·œ)
```

**announcement_duplicate_log ê¸°ë¡**:
```sql
INSERT INTO announcement_duplicate_log (
    preprocessing_id,
    existing_preprocessing_id,
    duplicate_type,
    url_key_hash,
    new_site_type,
    new_site_code,
    existing_site_type,
    existing_site_code,
    new_priority,
    existing_priority,
    new_folder_name,
    duplicate_detail,
    error_message
) VALUES (
    1001,                    -- ë°©ê¸ˆ ì €ì¥ëœ ID
    NULL,                    -- ê¸°ì¡´ ë ˆì½”ë“œ ì—†ìŒ
    'new_inserted',
    'abc123def456...',
    'api_scrap',
    'bizInfo',
    NULL,
    NULL,
    1,                       -- api_scrap ìš°ì„ ìˆœìœ„
    NULL,
    '2025-11-01_PBLN_000000000100001',
    NULL,
    NULL
);
```

---

### ì‹œë‚˜ë¦¬ì˜¤ 2: êµì²´ (replaced)

**ìƒí™©**: ê°™ì€ ê³µê³ ë¥¼ seoul ì§€ìì²´ í™ˆí˜ì´ì§€ì—ì„œ ë‹¤ì‹œ ìˆ˜ì§‘

**ë°ì´í„°**:
```python
origin_url = "https://www.seoul.go.kr/support/announce/view.do?id=12345"  # ê°™ì€ ê³µê³ 
url_key = "bizinfo.go.kr/web/lay1/bbs/S1T122C128/AS/74/view.do?pblancId=PBLN_000000000100001"  # ì •ê·œí™” í›„ ë™ì¼
url_key_hash = MD5(url_key) = "abc123def456..."  # ë™ì¼
site_type = "Homepage"
site_code = "seoul"
```

**ì¤‘ë³µ ì²´í¬**:
```sql
SELECT id, site_type, site_code
FROM announcement_pre_processing
WHERE url_key_hash = 'abc123def456...'
-- ê²°ê³¼: id=1001, site_type='api_scrap', site_code='bizInfo' (ê¸°ì¡´ ë ˆì½”ë“œ)
```

**ìš°ì„ ìˆœìœ„ ë¹„êµ**:
```python
new_priority = get_priority('Homepage') = 3
existing_priority = get_priority('api_scrap') = 1
# 3 > 1 â†’ êµì²´
```

**announcement_pre_processing ì—…ë°ì´íŠ¸**:
```sql
UPDATE announcement_pre_processing
SET
    site_type = 'Homepage',
    site_code = 'prv_seoul',  -- prv_ ì ‘ë‘ì‚¬ ì¶”ê°€
    content_md = 'ìƒˆ ë‚´ìš©',
    ...
WHERE id = 1001;
```

**announcement_duplicate_log ê¸°ë¡**:
```sql
INSERT INTO announcement_duplicate_log (
    preprocessing_id,
    existing_preprocessing_id,
    duplicate_type,
    url_key_hash,
    new_site_type,
    new_site_code,
    existing_site_type,
    existing_site_code,
    new_priority,
    existing_priority,
    new_folder_name,
    duplicate_detail,
    error_message
) VALUES (
    1001,                    -- ë™ì¼ ID (ì—…ë°ì´íŠ¸ë¨)
    1001,                    -- ê¸°ì¡´ ë ˆì½”ë“œ ID
    'replaced',
    'abc123def456...',
    'Homepage',
    'seoul',
    'api_scrap',
    'bizInfo',
    3,
    1,
    'seoul_20251101_12345',
    JSON_OBJECT(
        'decision', 'ê¸°ì¡´ ë°ì´í„° êµì²´',
        'reason', 'ìš°ì„ ìˆœìœ„ ë†’ìŒ: Homepage(3) > api_scrap(1)',
        'existing_folder', '2025-11-01_PBLN_000000000100001',
        'priority_comparison', '3 > 1'
    ),
    NULL
);
```

---

### ì‹œë‚˜ë¦¬ì˜¤ 3: ìœ ì§€ (kept_existing)

**ìƒí™©**: seoul ì§€ìì²´ ë°ì´í„°ê°€ ì´ë¯¸ ìˆëŠ”ë°, bizInfoì—ì„œ ë‹¤ì‹œ ìˆ˜ì§‘

**ë°ì´í„°**:
```python
origin_url = "https://www.bizinfo.go.kr/web/lay1/bbs/S1T122C128/AS/74/view.do?pblancId=PBLN_000000000100001"
url_key = "bizinfo.go.kr/web/lay1/bbs/S1T122C128/AS/74/view.do?pblancId=PBLN_000000000100001"
url_key_hash = "abc123def456..."
site_type = "api_scrap"
site_code = "bizInfo"
```

**ì¤‘ë³µ ì²´í¬**:
```sql
SELECT id, site_type, site_code
FROM announcement_pre_processing
WHERE url_key_hash = 'abc123def456...'
-- ê²°ê³¼: id=1001, site_type='Homepage', site_code='prv_seoul' (ê¸°ì¡´ ë ˆì½”ë“œ)
```

**ìš°ì„ ìˆœìœ„ ë¹„êµ**:
```python
new_priority = get_priority('api_scrap') = 1
existing_priority = get_priority('Homepage') = 3
# 1 < 3 â†’ ìœ ì§€ (ì—…ë°ì´íŠ¸ ì•ˆí•¨)
```

**announcement_pre_processing**: ë³€ê²½ ì—†ìŒ

**announcement_duplicate_log ê¸°ë¡**:
```sql
INSERT INTO announcement_duplicate_log (
    preprocessing_id,
    existing_preprocessing_id,
    duplicate_type,
    url_key_hash,
    new_site_type,
    new_site_code,
    existing_site_type,
    existing_site_code,
    new_priority,
    existing_priority,
    new_folder_name,
    duplicate_detail,
    error_message
) VALUES (
    1001,                    -- ê¸°ì¡´ ID (ë³€ê²½ ì—†ìŒ)
    1001,                    -- ê¸°ì¡´ ë ˆì½”ë“œ ID
    'kept_existing',
    'abc123def456...',
    'api_scrap',
    'bizInfo',
    'Homepage',
    'prv_seoul',
    1,
    3,
    '2025-11-01_PBLN_000000000100001',
    JSON_OBJECT(
        'decision', 'ê¸°ì¡´ ë°ì´í„° ìœ ì§€',
        'reason', 'ìš°ì„ ìˆœìœ„ ë‚®ìŒ: api_scrap(1) < Homepage(3)',
        'existing_folder', 'seoul_20251101_12345',
        'priority_comparison', '1 < 3'
    ),
    NULL
);
```

---

### ì‹œë‚˜ë¦¬ì˜¤ 4: ë™ì¼ íƒ€ì… ì¤‘ë³µ (same_type_duplicate)

**ìƒí™©**: bizInfoì—ì„œ ê°™ì€ ê³µê³ ë¥¼ ë‹¤ì‹œ ìˆ˜ì§‘ (ì¬ìˆ˜ì§‘)

**ë°ì´í„°**:
```python
url_key_hash = "abc123def456..."  # ë™ì¼
site_type = "api_scrap"
site_code = "bizInfo"
```

**ì¤‘ë³µ ì²´í¬**:
```sql
SELECT id, site_type, site_code
FROM announcement_pre_processing
WHERE url_key_hash = 'abc123def456...'
-- ê²°ê³¼: id=1001, site_type='api_scrap', site_code='bizInfo' (ë™ì¼ íƒ€ì…)
```

**ìš°ì„ ìˆœìœ„ ë¹„êµ**:
```python
new_priority = get_priority('api_scrap') = 1
existing_priority = get_priority('api_scrap') = 1
# 1 = 1 â†’ ìµœì‹  ë°ì´í„°ë¡œ ì—…ë°ì´íŠ¸
```

**announcement_pre_processing ì—…ë°ì´íŠ¸**: ìµœì‹  ë‚´ìš©ìœ¼ë¡œ ê°±ì‹ 

**announcement_duplicate_log ê¸°ë¡**:
```sql
INSERT INTO announcement_duplicate_log (
    preprocessing_id,
    existing_preprocessing_id,
    duplicate_type,
    url_key_hash,
    new_site_type,
    new_site_code,
    existing_site_type,
    existing_site_code,
    new_priority,
    existing_priority,
    new_folder_name,
    duplicate_detail,
    error_message
) VALUES (
    1001,
    1001,
    'same_type_duplicate',
    'abc123def456...',
    'api_scrap',
    'bizInfo',
    'api_scrap',
    'bizInfo',
    1,
    1,
    '2025-11-02_PBLN_000000000100001',
    JSON_OBJECT(
        'decision', 'ìµœì‹  ë°ì´í„°ë¡œ ì—…ë°ì´íŠ¸',
        'reason', 'ìš°ì„ ìˆœìœ„ ë™ì¼: api_scrap(1) = api_scrap(1)',
        'existing_folder', '2025-11-01_PBLN_000000000100001',
        'priority_comparison', '1 = 1',
        'update_type', 'refresh'
    ),
    NULL
);
```

---

### ì‹œë‚˜ë¦¬ì˜¤ 5: ì˜¤ë¥˜ (error)

**ìƒí™©**: URL ì •ê·œí™” ì‹¤íŒ¨

**ë°ì´í„°**:
```python
origin_url = "invalid-url"
url_key = None  # ì •ê·œí™” ì‹¤íŒ¨
url_key_hash = None
site_type = "api_scrap"
site_code = "bizInfo"
```

**announcement_duplicate_log ê¸°ë¡**:
```sql
INSERT INTO announcement_duplicate_log (
    preprocessing_id,
    existing_preprocessing_id,
    duplicate_type,
    url_key_hash,
    new_site_type,
    new_site_code,
    existing_site_type,
    existing_site_code,
    new_priority,
    existing_priority,
    new_folder_name,
    duplicate_detail,
    error_message
) VALUES (
    1002,
    NULL,
    'error',
    NULL,
    'api_scrap',
    'bizInfo',
    NULL,
    NULL,
    1,
    NULL,
    '2025-11-01_INVALID',
    NULL,
    'URL ì •ê·œí™” ì‹¤íŒ¨: origin_urlì´ ìœ íš¨í•˜ì§€ ì•ŠìŒ'
);
```

---

## ğŸ’» ì½”ë“œ êµ¬í˜„

### 1ï¸âƒ£ _save_processing_result() í•¨ìˆ˜ ìˆ˜ì •

**íŒŒì¼**: `announcement_pre_processor.py`

**í•µì‹¬ ë³€ê²½ì‚¬í•­**:
1. url_key_hash ê¸°ë°˜ ì¤‘ë³µ ì²´í¬
2. ìš°ì„ ìˆœìœ„ ë¹„êµ
3. announcement_duplicate_log ê¸°ë¡

```python
def _save_processing_result(
    self,
    folder_name: str,
    site_code: str,
    content_md: str,
    combined_content: str,
    attachment_filenames: List[str] = None,
    status: str = "ì„±ê³µ",
    exclusion_keywords: List[str] = None,
    exclusion_reason: str = None,
    error_message: str = None,
    force: bool = False,
    title: str = None,
    origin_url: str = None,
    url_key: str = None,
    scraping_url: str = None,
    announcement_date: str = None,
    attachment_files_info: List[Dict[str, Any]] = None,
) -> Optional[int]:
    """ì²˜ë¦¬ ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤."""
    try:
        from sqlalchemy import text
        import hashlib

        with self.db_manager.SessionLocal() as session:
            # ================================================
            # 1ë‹¨ê³„: url_key_hash ê¸°ë°˜ ì¤‘ë³µ ì²´í¬
            # ================================================
            url_key_hash = None
            existing_record = None

            if url_key:
                # url_key_hash ê³„ì‚°
                url_key_hash = hashlib.md5(url_key.encode('utf-8')).hexdigest()
                logger.debug(f"url_key_hash ìƒì„±: {url_key_hash[:16]}...")

                # ê¸°ì¡´ ë ˆì½”ë“œ ì¡°íšŒ
                try:
                    existing_record_result = session.execute(
                        text("""
                            SELECT id, site_type, site_code, folder_name, url_key
                            FROM announcement_pre_processing
                            WHERE url_key_hash = :url_key_hash
                            LIMIT 1
                        """),
                        {"url_key_hash": url_key_hash}
                    ).fetchone()

                    if existing_record_result:
                        existing_record = {
                            'id': existing_record_result.id,
                            'site_type': existing_record_result.site_type,
                            'site_code': existing_record_result.site_code,
                            'folder_name': existing_record_result.folder_name,
                            'url_key': existing_record_result.url_key
                        }
                        logger.info(
                            f"âš ï¸  ì¤‘ë³µ ë°œê²¬: url_key_hash={url_key_hash[:16]}... "
                            f"ê¸°ì¡´ ë ˆì½”ë“œ ID={existing_record['id']}, "
                            f"site_type={existing_record['site_type']}, "
                            f"folder_name={existing_record['folder_name']}"
                        )
                except Exception as e:
                    logger.warning(f"ê¸°ì¡´ ë ˆì½”ë“œ ì¡°íšŒ ì‹¤íŒ¨ (ë¬´ì‹œí•˜ê³  ê³„ì†): {e}")

            # ================================================
            # 2ë‹¨ê³„: ìš°ì„ ìˆœìœ„ ë¹„êµ ë° ì²˜ë¦¬ ê²°ì •
            # ================================================
            duplicate_type = None
            should_update = False

            if not url_key:
                # URL ì •ê·œí™” ì‹¤íŒ¨
                duplicate_type = 'error'
                should_update = True  # ì—ëŸ¬ë¼ë„ ì¼ë‹¨ ì €ì¥
                logger.warning("URL ì •ê·œí™” ì‹¤íŒ¨ - ì¤‘ë³µ ì²´í¬ ë¶ˆê°€")

            elif existing_record:
                # ì¤‘ë³µ ë°œê²¬ - ìš°ì„ ìˆœìœ„ ë¹„êµ
                new_priority = self.get_priority(self.site_type)
                existing_priority = self.get_priority(existing_record['site_type'])

                logger.info(
                    f"ìš°ì„ ìˆœìœ„ ë¹„êµ: ìƒˆ={self.site_type}({new_priority}) vs "
                    f"ê¸°ì¡´={existing_record['site_type']}({existing_priority})"
                )

                if new_priority > existing_priority:
                    # ìƒˆ ë°ì´í„° ìš°ì„ ìˆœìœ„ê°€ ë†’ìŒ â†’ êµì²´
                    duplicate_type = 'replaced'
                    should_update = True
                    logger.info(f"âœ… ê¸°ì¡´ ë°ì´í„° êµì²´: {new_priority} > {existing_priority}")

                elif new_priority < existing_priority:
                    # ê¸°ì¡´ ë°ì´í„° ìš°ì„ ìˆœìœ„ê°€ ë†’ìŒ â†’ ìœ ì§€
                    duplicate_type = 'kept_existing'
                    should_update = False
                    logger.info(f"â­ï¸  ê¸°ì¡´ ë°ì´í„° ìœ ì§€: {new_priority} < {existing_priority}")

                else:
                    # ìš°ì„ ìˆœìœ„ ë™ì¼ â†’ ìµœì‹  ë°ì´í„°ë¡œ ì—…ë°ì´íŠ¸
                    duplicate_type = 'same_type_duplicate'
                    should_update = True
                    logger.info(f"ğŸ”„ ë™ì¼ ìš°ì„ ìˆœìœ„ ì—…ë°ì´íŠ¸: {new_priority} = {existing_priority}")

            else:
                # ì‹ ê·œ ì‚½ì…
                duplicate_type = 'new_inserted'
                should_update = True
                logger.info("âœ¨ ì‹ ê·œ ê³µê³  ì‚½ì…")

            # ================================================
            # 3ë‹¨ê³„: announcement_pre_processing ì €ì¥
            # ================================================
            record_id = None

            if should_update:
                if existing_record and duplicate_type in ['replaced', 'same_type_duplicate']:
                    # ê¸°ì¡´ ë ˆì½”ë“œ ì—…ë°ì´íŠ¸
                    # ... (UPDATE ì¿¼ë¦¬ ì‹¤í–‰)
                    record_id = existing_record['id']
                    logger.info(f"ê¸°ì¡´ ë ˆì½”ë“œ ì—…ë°ì´íŠ¸ ì™„ë£Œ: ID={record_id}")
                else:
                    # ì‹ ê·œ INSERT
                    # ... (INSERT ì¿¼ë¦¬ ì‹¤í–‰)
                    result = session.execute(insert_sql, params)
                    record_id = result.lastrowid
                    logger.info(f"ì‹ ê·œ ë ˆì½”ë“œ ì €ì¥ ì™„ë£Œ: ID={record_id}")

                session.commit()
            else:
                # ê¸°ì¡´ ë°ì´í„° ìœ ì§€ (kept_existing)
                record_id = existing_record['id']
                logger.info(f"ê¸°ì¡´ ë°ì´í„° ìœ ì§€: ID={record_id}")

            # ================================================
            # 4ë‹¨ê³„: announcement_duplicate_log ê¸°ë¡ â­
            # ================================================
            self._log_announcement_duplicate(
                session=session,
                preprocessing_id=record_id,
                url_key_hash=url_key_hash,
                duplicate_type=duplicate_type,
                site_code=site_code,
                folder_name=folder_name,
                existing_record=existing_record,
                error_message=error_message if duplicate_type == 'error' else None
            )

            return record_id

    except Exception as e:
        logger.error(f"ì²˜ë¦¬ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
        return None
```

---

### 2ï¸âƒ£ _log_announcement_duplicate() í•¨ìˆ˜ (ì‹ ê·œ)

**íŒŒì¼**: `announcement_pre_processor.py`

```python
def _log_announcement_duplicate(
    self,
    session,
    preprocessing_id: int,
    url_key_hash: str,
    duplicate_type: str,
    site_code: str,
    folder_name: str,
    domain: str = None,
    domain_configured: bool = False,
    existing_record: dict = None,
    error_message: str = None
) -> bool:
    """
    announcement_duplicate_log í…Œì´ë¸”ì— ì¤‘ë³µ ì²˜ë¦¬ ë¡œê·¸ë¥¼ ê¸°ë¡í•©ë‹ˆë‹¤.

    Args:
        session: SQLAlchemy ì„¸ì…˜
        preprocessing_id: ì €ì¥/ì—…ë°ì´íŠ¸ëœ ë ˆì½”ë“œ ID
        url_key_hash: URL í‚¤ í•´ì‹œ (MD5) - domain_key_config ì—†ìœ¼ë©´ NULL
        duplicate_type: ì¤‘ë³µ ìœ í˜•
            - 'unconfigured_domain': domain_key_configì— ì„¤ì • ì—†ìŒ
            - 'new_inserted': ì‹ ê·œ ì‚½ì… (domain_key_config ìˆê³  ì¤‘ë³µ ì—†ìŒ)
            - 'replaced': ê¸°ì¡´ ë°ì´í„° êµì²´ (ìš°ì„ ìˆœìœ„ ë†’ìŒ)
            - 'kept_existing': ê¸°ì¡´ ë°ì´í„° ìœ ì§€ (ìš°ì„ ìˆœìœ„ ë‚®ìŒ)
            - 'same_type_duplicate': ë™ì¼ íƒ€ì… ì¬ìˆ˜ì§‘ (ìš°ì„ ìˆœìœ„ ë™ì¼)
            - 'error': ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜
        site_code: ì‚¬ì´íŠ¸ ì½”ë“œ
        folder_name: í´ë”ëª…
        domain: ë„ë©”ì¸ëª…
        domain_configured: domain_key_configì— ì„¤ì • ì—¬ë¶€
        existing_record: ê¸°ì¡´ ë ˆì½”ë“œ ì •ë³´ (ì¤‘ë³µ ì‹œ)
        error_message: ì—ëŸ¬ ë©”ì‹œì§€ (ì˜¤ë¥˜ ì‹œ)

    Returns:
        ë¡œê·¸ ê¸°ë¡ ì„±ê³µ ì—¬ë¶€
    """
    try:
        from sqlalchemy import text
        import json

        # ìš°ì„ ìˆœìœ„ ê³„ì‚°
        new_priority = self.get_priority(self.site_type)
        existing_priority = None
        existing_preprocessing_id = None
        existing_site_type = None
        existing_site_code = None
        duplicate_detail = None

        # ê¸°ì¡´ ë ˆì½”ë“œ ì •ë³´ ì¶”ì¶œ
        if existing_record:
            existing_preprocessing_id = existing_record['id']
            existing_site_type = existing_record['site_type']
            existing_site_code = existing_record['site_code']
            existing_priority = self.get_priority(existing_site_type)

            # ìƒì„¸ ì •ë³´ JSON ìƒì„±
            if duplicate_type == 'replaced':
                decision = 'ê¸°ì¡´ ë°ì´í„° êµì²´'
                reason = f'ìš°ì„ ìˆœìœ„ ë†’ìŒ: {self.site_type}({new_priority}) > {existing_site_type}({existing_priority})'
            elif duplicate_type == 'kept_existing':
                decision = 'ê¸°ì¡´ ë°ì´í„° ìœ ì§€'
                reason = f'ìš°ì„ ìˆœìœ„ ë‚®ìŒ: {self.site_type}({new_priority}) < {existing_site_type}({existing_priority})'
            elif duplicate_type == 'same_type_duplicate':
                decision = 'ìµœì‹  ë°ì´í„°ë¡œ ì—…ë°ì´íŠ¸'
                reason = f'ìš°ì„ ìˆœìœ„ ë™ì¼: {self.site_type}({new_priority}) = {existing_site_type}({existing_priority})'
            else:
                decision = 'ì•Œ ìˆ˜ ì—†ìŒ'
                reason = f'duplicate_type={duplicate_type}'

            duplicate_detail = {
                'decision': decision,
                'reason': reason,
                'existing_folder': existing_record.get('folder_name'),
                'existing_url_key': existing_record.get('url_key'),
                'priority_comparison': f'{new_priority} vs {existing_priority}',
                'domain': domain,
                'domain_configured': domain_configured,
                'timestamp': datetime.now().isoformat()
            }

        elif duplicate_type == 'unconfigured_domain':
            # domain_key_configì— ì—†ëŠ” ê²½ìš°
            duplicate_detail = {
                'decision': 'ì‹ ê·œ ë“±ë¡ (domain_key_config ì—†ìŒ)',
                'reason': 'domain_key_config í…Œì´ë¸”ì— ì„¤ì •ì´ ì—†ì–´ì„œ ì¤‘ë³µ ì²´í¬ ìƒëµ',
                'domain': domain,
                'domain_configured': False,
                'timestamp': datetime.now().isoformat()
            }

        elif duplicate_type == 'new_inserted':
            # domain_key_configì— ìˆì§€ë§Œ url_key_hash ì¤‘ë³µ ì—†ìŒ
            duplicate_detail = {
                'decision': 'ì‹ ê·œ ë“±ë¡',
                'reason': 'url_key_hash ì¤‘ë³µ ì—†ìŒ',
                'domain': domain,
                'domain_configured': domain_configured,
                'timestamp': datetime.now().isoformat()
            }

        # announcement_duplicate_log INSERT
        sql = text("""
            INSERT INTO announcement_duplicate_log (
                preprocessing_id,
                existing_preprocessing_id,
                duplicate_type,
                url_key_hash,
                new_site_type,
                new_site_code,
                existing_site_type,
                existing_site_code,
                new_priority,
                existing_priority,
                new_folder_name,
                duplicate_detail,
                error_message
            ) VALUES (
                :preprocessing_id,
                :existing_preprocessing_id,
                :duplicate_type,
                :url_key_hash,
                :new_site_type,
                :new_site_code,
                :existing_site_type,
                :existing_site_code,
                :new_priority,
                :existing_priority,
                :new_folder_name,
                :duplicate_detail,
                :error_message
            )
        """)

        # JSON ì§ë ¬í™”
        duplicate_detail_json = None
        if duplicate_detail:
            duplicate_detail_json = json.dumps(duplicate_detail, ensure_ascii=False)

        # íŒŒë¼ë¯¸í„° ë°”ì¸ë”©
        params = {
            'preprocessing_id': preprocessing_id,
            'existing_preprocessing_id': existing_preprocessing_id,
            'duplicate_type': duplicate_type,
            'url_key_hash': url_key_hash,
            'new_site_type': self.site_type,
            'new_site_code': site_code,
            'existing_site_type': existing_site_type,
            'existing_site_code': existing_site_code,
            'new_priority': new_priority,
            'existing_priority': existing_priority,
            'new_folder_name': folder_name,
            'duplicate_detail': duplicate_detail_json,
            'error_message': error_message
        }

        # ì‹¤í–‰
        session.execute(sql, params)
        session.commit()

        logger.debug(
            f"ì¤‘ë³µ ë¡œê·¸ ê¸°ë¡ ì™„ë£Œ: {duplicate_type} - "
            f"preprocessing_id={preprocessing_id}, "
            f"url_key_hash={url_key_hash[:16] if url_key_hash else 'None'}..."
        )

        return True

    except Exception as e:
        logger.error(f"ì¤‘ë³µ ë¡œê·¸ ê¸°ë¡ ì‹¤íŒ¨: {e}")
        # ë¡œê·¸ ê¸°ë¡ ì‹¤íŒ¨í•´ë„ ë©”ì¸ ì²˜ë¦¬ëŠ” ê³„ì† ì§„í–‰
        return False
```

---

### 3ï¸âƒ£ get_priority() í•¨ìˆ˜

**íŒŒì¼**: `announcement_pre_processor.py`

```python
def get_priority(self, site_type: str) -> int:
    """
    site_typeë³„ ìš°ì„ ìˆœìœ„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

    ìš°ì„ ìˆœìœ„ ê·œì¹™:
    - ì§€ìì²´ ë°ì´í„° (Eminwon/Homepage/Scraper) = 3 (ë†’ìŒ)
    - API ë°ì´í„° (api_scrap) = 1 (ë‚®ìŒ)
    - Unknown = 0 (ìµœí•˜)

    Args:
        site_type: ì‚¬ì´íŠ¸ íƒ€ì…

    Returns:
        ìš°ì„ ìˆœìœ„ (0-3)
    """
    priority_map = {
        'Eminwon': 3,
        'Homepage': 3,
        'Scraper': 3,
        'api_scrap': 1,
        'Unknown': 0,
    }
    return priority_map.get(site_type, 0)
```

---

## ğŸ“Š í™œìš© ë°©ì•ˆ

### 1ï¸âƒ£ URLë³„ ì²˜ë¦¬ ì´ë ¥ ì¡°íšŒ

**ëª©ì **: íŠ¹ì • URLì´ ì–´ë–»ê²Œ ì²˜ë¦¬ë˜ì—ˆëŠ”ì§€ ì „ì²´ ì´ë ¥ í™•ì¸

**ì¿¼ë¦¬**:
```sql
-- URL í‚¤ë¡œ ê²€ìƒ‰
SELECT
    adl.created_at,
    adl.duplicate_type,
    adl.new_site_type,
    adl.new_site_code,
    adl.new_folder_name,
    adl.existing_site_type,
    adl.existing_site_code,
    adl.new_priority,
    adl.existing_priority,
    JSON_EXTRACT(adl.duplicate_detail, '$.decision') as decision,
    JSON_EXTRACT(adl.duplicate_detail, '$.reason') as reason,
    app.title,
    app.origin_url
FROM announcement_duplicate_log adl
LEFT JOIN announcement_pre_processing app ON adl.preprocessing_id = app.id
WHERE adl.url_key_hash = MD5('ì •ê·œí™”ëœ_URL_í‚¤')
ORDER BY adl.created_at ASC;
```

**ê²°ê³¼ ì˜ˆì‹œ**:
```
created_at          | duplicate_type     | new_site_type | new_site_code | existing_site_type | decision
--------------------|-------------------|---------------|---------------|--------------------|-----------
2025-11-01 10:00:00 | new_inserted      | api_scrap     | bizInfo       | NULL               | NULL
2025-11-01 15:30:00 | replaced          | Homepage      | seoul         | api_scrap          | ê¸°ì¡´ ë°ì´í„° êµì²´
2025-11-02 09:15:00 | kept_existing     | api_scrap     | bizInfo       | Homepage           | ê¸°ì¡´ ë°ì´í„° ìœ ì§€
2025-11-03 14:20:00 | same_type_duplicate| Homepage     | seoul         | Homepage           | ìµœì‹  ë°ì´í„°ë¡œ ì—…ë°ì´íŠ¸
```

**í™œìš©**:
- âœ… í•œ URLì˜ ì „ì²´ ì²˜ë¦¬ íë¦„ íŒŒì•…
- âœ… ì–´ëŠ ì‚¬ì´íŠ¸ì—ì„œ ë¨¼ì € ìˆ˜ì§‘í–ˆëŠ”ì§€ í™•ì¸
- âœ… ë°ì´í„° êµì²´ ì´ë ¥ ì¶”ì 

---

### 2ï¸âƒ£ ì¼ë³„ ì¤‘ë³µ ë°œìƒ í†µê³„

**ëª©ì **: ë§¤ì¼ ì–¼ë§ˆë‚˜ ë§ì€ ì¤‘ë³µì´ ë°œìƒí•˜ëŠ”ì§€ ì¶”ì 

**ì¿¼ë¦¬**:
```sql
SELECT
    DATE(created_at) as date,
    duplicate_type,
    COUNT(*) as count,
    COUNT(DISTINCT url_key_hash) as unique_urls,
    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (PARTITION BY DATE(created_at)), 2) as percentage
FROM announcement_duplicate_log
WHERE created_at >= DATE_SUB(CURDATE(), INTERVAL 7 DAY)
GROUP BY DATE(created_at), duplicate_type
ORDER BY date DESC, count DESC;
```

**ê²°ê³¼ ì˜ˆì‹œ**:
```
date       | duplicate_type      | count | unique_urls | percentage
-----------|--------------------| ------|-------------|------------
2025-11-01 | new_inserted       | 1200  | 1200        | 75.00%
2025-11-01 | kept_existing      | 280   | 250         | 17.50%
2025-11-01 | replaced           | 80    | 75          | 5.00%
2025-11-01 | same_type_duplicate| 40    | 35          | 2.50%
```

**í™œìš©**:
- âœ… ì¤‘ë³µ ë°œìƒë¥  ì¶”ì´ ëª¨ë‹ˆí„°ë§
- âœ… ì‹ ê·œ ê³µê³  ë¹„ìœ¨ í™•ì¸
- âœ… ì´ìƒ íŒ¨í„´ ê°ì§€ (ê°‘ìê¸° ì¤‘ë³µë¥  ê¸‰ì¦ ë“±)

---

### 3ï¸âƒ£ ì‚¬ì´íŠ¸ë³„ ì¤‘ë³µ ë°œìƒë¥ 

**ëª©ì **: ì–´ë–¤ ì‚¬ì´íŠ¸ì—ì„œ ì¤‘ë³µì´ ë§ì´ ë°œìƒí•˜ëŠ”ì§€ íŒŒì•…

**ì¿¼ë¦¬**:
```sql
SELECT
    new_site_code,
    new_site_type,
    COUNT(*) as total_attempts,
    SUM(CASE WHEN duplicate_type = 'new_inserted' THEN 1 ELSE 0 END) as new_count,
    SUM(CASE WHEN duplicate_type IN ('replaced', 'kept_existing', 'same_type_duplicate') THEN 1 ELSE 0 END) as duplicate_count,
    ROUND(SUM(CASE WHEN duplicate_type IN ('replaced', 'kept_existing', 'same_type_duplicate') THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as duplicate_rate,
    COUNT(DISTINCT url_key_hash) as unique_urls
FROM announcement_duplicate_log
WHERE created_at >= DATE_SUB(CURDATE(), INTERVAL 30 DAY)
  AND url_key_hash IS NOT NULL
GROUP BY new_site_code, new_site_type
ORDER BY duplicate_rate DESC
LIMIT 20;
```

**ê²°ê³¼ ì˜ˆì‹œ**:
```
new_site_code | new_site_type | total_attempts | new_count | duplicate_count | duplicate_rate | unique_urls
--------------|---------------|----------------|-----------|-----------------|----------------|-------------
bizInfo       | api_scrap     | 5000           | 4000      | 1000            | 20.00%         | 4500
seoul         | Homepage      | 3500           | 3000      | 500             | 14.29%         | 3200
busan         | Homepage      | 2800           | 2500      | 300             | 10.71%         | 2650
```

**í™œìš©**:
- âœ… ì¤‘ë³µ ë°œìƒì´ ë§ì€ ì‚¬ì´íŠ¸ ì‹ë³„
- âœ… ìŠ¤í¬ë ˆì´í•‘ ì£¼ê¸° ì¡°ì • ê·¼ê±°
- âœ… ì‚¬ì´íŠ¸ë³„ ë°ì´í„° í’ˆì§ˆ ë¹„êµ

---

### 4ï¸âƒ£ ìš°ì„ ìˆœìœ„ ì ìš© ê²€ì¦

**ëª©ì **: ì§€ìì²´ > API ìš°ì„ ìˆœìœ„ê°€ ì œëŒ€ë¡œ ì ìš©ë˜ëŠ”ì§€ í™•ì¸

**ì¿¼ë¦¬**:
```sql
-- ìš°ì„ ìˆœìœ„ ë¹„êµ ê²°ê³¼
SELECT
    duplicate_type,
    new_site_type,
    existing_site_type,
    new_priority,
    existing_priority,
    COUNT(*) as count
FROM announcement_duplicate_log
WHERE duplicate_type IN ('replaced', 'kept_existing')
  AND created_at >= DATE_SUB(CURDATE(), INTERVAL 7 DAY)
GROUP BY duplicate_type, new_site_type, existing_site_type, new_priority, existing_priority
ORDER BY count DESC;
```

**ê²°ê³¼ ì˜ˆì‹œ**:
```
duplicate_type | new_site_type | existing_site_type | new_priority | existing_priority | count
---------------|---------------|--------------------|--------------|--------------------|-------
kept_existing  | api_scrap     | Homepage          | 1            | 3                  | 280
kept_existing  | api_scrap     | Eminwon           | 1            | 3                  | 150
replaced       | Homepage      | api_scrap         | 3            | 1                  | 80
replaced       | Eminwon       | api_scrap         | 3            | 1                  | 45
```

**ê²€ì¦**:
- âœ… `kept_existing`ì—ì„œ new_priority < existing_priority í™•ì¸
- âœ… `replaced`ì—ì„œ new_priority > existing_priority í™•ì¸
- âœ… ìš°ì„ ìˆœìœ„ ë¡œì§ ì •ìƒ ë™ì‘ ê²€ì¦

**ì´ìƒ ì¼€ì´ìŠ¤ íƒì§€**:
```sql
-- ìš°ì„ ìˆœìœ„ ì—­ì „ ì¼€ì´ìŠ¤ (ë²„ê·¸ ê°€ëŠ¥ì„±)
SELECT *
FROM announcement_duplicate_log
WHERE
    (duplicate_type = 'kept_existing' AND new_priority > existing_priority)
    OR
    (duplicate_type = 'replaced' AND new_priority < existing_priority)
ORDER BY created_at DESC;
```

---

### 5ï¸âƒ£ ìì£¼ ì¤‘ë³µë˜ëŠ” URL Top 20

**ëª©ì **: ì–´ë–¤ URLì´ ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ ì¤‘ë³µ ìˆ˜ì§‘ë˜ëŠ”ì§€ íŒŒì•…

**ì¿¼ë¦¬**:
```sql
SELECT
    adl.url_key_hash,
    COUNT(*) as occurrence_count,
    COUNT(DISTINCT adl.new_site_code) as site_count,
    GROUP_CONCAT(DISTINCT adl.new_site_type ORDER BY adl.new_site_type) as site_types,
    GROUP_CONCAT(DISTINCT adl.new_site_code ORDER BY adl.new_site_code) as site_codes,
    MAX(app.title) as title,
    MAX(app.origin_url) as origin_url,
    MIN(adl.created_at) as first_seen,
    MAX(adl.created_at) as last_seen
FROM announcement_duplicate_log adl
LEFT JOIN announcement_pre_processing app ON adl.preprocessing_id = app.id
WHERE adl.created_at >= DATE_SUB(CURDATE(), INTERVAL 30 DAY)
  AND adl.url_key_hash IS NOT NULL
GROUP BY adl.url_key_hash
HAVING occurrence_count >= 3
ORDER BY occurrence_count DESC, site_count DESC
LIMIT 20;
```

**ê²°ê³¼ ì˜ˆì‹œ**:
```
url_key_hash    | occurrence_count | site_count | site_types                  | site_codes          | title
----------------|------------------|------------|-----------------------------|--------------------|--------
abc123def456... | 8                | 4          | Homepage,Eminwon,api_scrap  | seoul,busan,bizInfo| ì°½ì—…ì§€ì›
xyz789ghi012... | 6                | 3          | Homepage,api_scrap          | seoul,bizInfo,smes24| ì¤‘ì†Œê¸°ì—…
```

**í™œìš©**:
- âœ… ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ ìˆ˜ì§‘ë˜ëŠ” ì¸ê¸° ê³µê³  ì‹ë³„
- âœ… ìŠ¤í¬ë ˆì´í•‘ ìµœì í™” (ì¤‘ë³µ ì œê±°)
- âœ… ë°ì´í„° ì†ŒìŠ¤ë³„ ì»¤ë²„ë¦¬ì§€ ë¶„ì„

---

### 6ï¸âƒ£ ì—ëŸ¬ ë¶„ì„

**ëª©ì **: URL ì •ê·œí™” ì‹¤íŒ¨ ë“± ì—ëŸ¬ íŒ¨í„´ íŒŒì•…

**ì¿¼ë¦¬**:
```sql
SELECT
    error_message,
    new_site_code,
    new_site_type,
    COUNT(*) as error_count,
    MAX(created_at) as last_occurrence
FROM announcement_duplicate_log
WHERE duplicate_type = 'error'
  AND created_at >= DATE_SUB(CURDATE(), INTERVAL 7 DAY)
GROUP BY error_message, new_site_code, new_site_type
ORDER BY error_count DESC
LIMIT 20;
```

**ê²°ê³¼ ì˜ˆì‹œ**:
```
error_message              | new_site_code | new_site_type | error_count | last_occurrence
---------------------------|---------------|---------------|-------------|------------------
URL ì •ê·œí™” ì‹¤íŒ¨            | bizInfo       | api_scrap     | 25          | 2025-11-01 15:30
domain_key_config ì—†ìŒ     | unknown_site  | Scraper       | 18          | 2025-11-01 14:20
origin_urlì´ ìœ íš¨í•˜ì§€ ì•ŠìŒ  | seoul         | Homepage      | 12          | 2025-11-01 13:15
```

**í™œìš©**:
- âœ… ì—ëŸ¬ ë°œìƒ íŒ¨í„´ ë¶„ì„
- âœ… ìš°ì„ ìˆœìœ„ ìˆëŠ” ë²„ê·¸ ìˆ˜ì •
- âœ… URL ì •ê·œí™” ë¡œì§ ê°œì„ 

---

## ğŸ§ª í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤

### í…ŒìŠ¤íŠ¸ 1: ì‹ ê·œ ì‚½ì…

**ì…ë ¥**:
```python
folder_name = "bizInfo_20251101_001"
site_code = "bizInfo"
site_type = "api_scrap"
origin_url = "https://www.bizinfo.go.kr/web/lay1/bbs/S1T122C128/AS/74/view.do?pblancId=NEW001"
url_key = "bizinfo.go.kr/web/lay1/bbs/S1T122C128/AS/74/view.do?pblancId=NEW001"
url_key_hash = MD5(url_key) = "aaa111bbb222..."
```

**ì˜ˆìƒ ê²°ê³¼**:
```sql
-- announcement_pre_processing
INSERT ì„±ê³µ, id=2001

-- announcement_duplicate_log
INSERT INTO announcement_duplicate_log VALUES (
    preprocessing_id = 2001,
    existing_preprocessing_id = NULL,
    duplicate_type = 'new_inserted',
    url_key_hash = 'aaa111bbb222...',
    new_site_type = 'api_scrap',
    new_site_code = 'bizInfo',
    existing_site_type = NULL,
    existing_site_code = NULL,
    new_priority = 1,
    existing_priority = NULL,
    ...
)
```

**ê²€ì¦ ì¿¼ë¦¬**:
```sql
SELECT * FROM announcement_duplicate_log WHERE preprocessing_id = 2001;
-- duplicate_type = 'new_inserted' í™•ì¸
```

---

### í…ŒìŠ¤íŠ¸ 2: ì§€ìì²´ê°€ API êµì²´

**ì „ì œ ì¡°ê±´**:
```sql
-- ê¸°ì¡´ ë°ì´í„° (bizInfo)
INSERT INTO announcement_pre_processing (id, site_type, site_code, url_key_hash, ...)
VALUES (2001, 'api_scrap', 'bizInfo', 'aaa111bbb222...', ...);
```

**ì…ë ¥**:
```python
folder_name = "seoul_20251101_100"
site_code = "seoul"
site_type = "Homepage"
origin_url = "https://www.seoul.go.kr/support/announce/view.do?id=12345"
url_key = "bizinfo.go.kr/web/lay1/bbs/S1T122C128/AS/74/view.do?pblancId=NEW001"  # ë™ì¼
url_key_hash = "aaa111bbb222..."  # ë™ì¼
```

**ì˜ˆìƒ ê²°ê³¼**:
```sql
-- announcement_pre_processing
UPDATE id=2001 SET site_type='Homepage', site_code='prv_seoul', ...

-- announcement_duplicate_log
INSERT INTO announcement_duplicate_log VALUES (
    preprocessing_id = 2001,
    existing_preprocessing_id = 2001,
    duplicate_type = 'replaced',
    url_key_hash = 'aaa111bbb222...',
    new_site_type = 'Homepage',
    new_site_code = 'seoul',
    existing_site_type = 'api_scrap',
    existing_site_code = 'bizInfo',
    new_priority = 3,
    existing_priority = 1,
    duplicate_detail = '{"decision": "ê¸°ì¡´ ë°ì´í„° êµì²´", "reason": "ìš°ì„ ìˆœìœ„ ë†’ìŒ: Homepage(3) > api_scrap(1)", ...}',
    ...
)
```

**ê²€ì¦ ì¿¼ë¦¬**:
```sql
-- ë¡œê·¸ í™•ì¸
SELECT * FROM announcement_duplicate_log WHERE preprocessing_id = 2001 ORDER BY created_at DESC LIMIT 2;
-- ì²« ë²ˆì§¸: new_inserted (ìµœì´ˆ ì‚½ì…)
-- ë‘ ë²ˆì§¸: replaced (êµì²´)

-- ë°ì´í„° í™•ì¸
SELECT site_type, site_code FROM announcement_pre_processing WHERE id = 2001;
-- site_type = 'Homepage', site_code = 'prv_seoul' í™•ì¸
```

---

### í…ŒìŠ¤íŠ¸ 3: APIê°€ ì§€ìì²´ ìœ ì§€

**ì „ì œ ì¡°ê±´**:
```sql
-- ê¸°ì¡´ ë°ì´í„° (seoul)
UPDATE announcement_pre_processing SET site_type='Homepage', site_code='prv_seoul' WHERE id=2001;
```

**ì…ë ¥**:
```python
folder_name = "bizInfo_20251102_001"
site_code = "bizInfo"
site_type = "api_scrap"
url_key_hash = "aaa111bbb222..."  # ë™ì¼
```

**ì˜ˆìƒ ê²°ê³¼**:
```sql
-- announcement_pre_processing
ë³€ê²½ ì—†ìŒ (ê¸°ì¡´ ë°ì´í„° ìœ ì§€)

-- announcement_duplicate_log
INSERT INTO announcement_duplicate_log VALUES (
    preprocessing_id = 2001,  -- ë™ì¼ ID
    existing_preprocessing_id = 2001,
    duplicate_type = 'kept_existing',
    url_key_hash = 'aaa111bbb222...',
    new_site_type = 'api_scrap',
    new_site_code = 'bizInfo',
    existing_site_type = 'Homepage',
    existing_site_code = 'prv_seoul',
    new_priority = 1,
    existing_priority = 3,
    duplicate_detail = '{"decision": "ê¸°ì¡´ ë°ì´í„° ìœ ì§€", "reason": "ìš°ì„ ìˆœìœ„ ë‚®ìŒ: api_scrap(1) < Homepage(3)", ...}',
    ...
)
```

**ê²€ì¦ ì¿¼ë¦¬**:
```sql
-- ë¡œê·¸ í™•ì¸
SELECT duplicate_type, new_priority, existing_priority
FROM announcement_duplicate_log
WHERE preprocessing_id = 2001
ORDER BY created_at DESC
LIMIT 1;
-- duplicate_type = 'kept_existing', new_priority=1, existing_priority=3 í™•ì¸

-- ë°ì´í„° ë³€ê²½ ì—†ìŒ í™•ì¸
SELECT site_type FROM announcement_pre_processing WHERE id = 2001;
-- site_type = 'Homepage' (ë³€ê²½ ì—†ìŒ)
```

---

### í…ŒìŠ¤íŠ¸ 4: ë™ì¼ íƒ€ì… ì¬ìˆ˜ì§‘

**ì „ì œ ì¡°ê±´**:
```sql
UPDATE announcement_pre_processing SET site_type='api_scrap', site_code='bizInfo' WHERE id=2001;
```

**ì…ë ¥**:
```python
folder_name = "bizInfo_20251103_001"
site_code = "bizInfo"
site_type = "api_scrap"
url_key_hash = "aaa111bbb222..."  # ë™ì¼
```

**ì˜ˆìƒ ê²°ê³¼**:
```sql
-- announcement_pre_processing
UPDATE id=2001 SET content_md='ìµœì‹  ë‚´ìš©', updated_at=NOW(), ...

-- announcement_duplicate_log
INSERT INTO announcement_duplicate_log VALUES (
    preprocessing_id = 2001,
    existing_preprocessing_id = 2001,
    duplicate_type = 'same_type_duplicate',
    url_key_hash = 'aaa111bbb222...',
    new_site_type = 'api_scrap',
    new_site_code = 'bizInfo',
    existing_site_type = 'api_scrap',
    existing_site_code = 'bizInfo',
    new_priority = 1,
    existing_priority = 1,
    duplicate_detail = '{"decision": "ìµœì‹  ë°ì´í„°ë¡œ ì—…ë°ì´íŠ¸", "reason": "ìš°ì„ ìˆœìœ„ ë™ì¼: api_scrap(1) = api_scrap(1)", ...}',
    ...
)
```

**ê²€ì¦ ì¿¼ë¦¬**:
```sql
SELECT duplicate_type, new_priority, existing_priority
FROM announcement_duplicate_log
WHERE preprocessing_id = 2001
ORDER BY created_at DESC
LIMIT 1;
-- duplicate_type = 'same_type_duplicate', new_priority=1, existing_priority=1 í™•ì¸
```

---

## ğŸ“š ì¢…í•© í™œìš© ëŒ€ì‹œë³´ë“œ

### Grafana íŒ¨ë„ êµ¬ì„±

**íŒ¨ë„ 1: ì¼ì¼ ì²˜ë¦¬ í˜„í™©**
```sql
SELECT
    DATE(created_at) as time,
    duplicate_type,
    COUNT(*) as value
FROM announcement_duplicate_log
WHERE $__timeFilter(created_at)
GROUP BY time, duplicate_type
ORDER BY time;
```

**íŒ¨ë„ 2: ìš°ì„ ìˆœìœ„ ì ìš© í˜„í™©**
```sql
SELECT
    CONCAT(new_site_type, ' â†’ ', COALESCE(existing_site_type, 'NEW')) as transition,
    COUNT(*) as count
FROM announcement_duplicate_log
WHERE DATE(created_at) = CURDATE()
GROUP BY transition
ORDER BY count DESC;
```

**íŒ¨ë„ 3: ì¤‘ë³µë¥  ì¶”ì´**
```sql
SELECT
    DATE(created_at) as time,
    ROUND(SUM(CASE WHEN duplicate_type != 'new_inserted' THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as duplicate_rate
FROM announcement_duplicate_log
WHERE $__timeFilter(created_at)
GROUP BY time
ORDER BY time;
```

---

## âœ… êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸

### ì½”ë“œ êµ¬í˜„
- [ ] `_save_processing_result()` í•¨ìˆ˜ ìˆ˜ì •
  - [ ] url_key_hash ê³„ì‚°
  - [ ] url_key_hash ê¸°ë°˜ ì¤‘ë³µ ì²´í¬
  - [ ] ìš°ì„ ìˆœìœ„ ë¹„êµ ë¡œì§
  - [ ] announcement_duplicate_log í˜¸ì¶œ
- [ ] `_log_announcement_duplicate()` í•¨ìˆ˜ ì‘ì„±
  - [ ] duplicate_type ê²°ì • ë¡œì§
  - [ ] duplicate_detail JSON ìƒì„±
  - [ ] INSERT ì¿¼ë¦¬ ì‹¤í–‰
- [ ] `get_priority()` í•¨ìˆ˜ í™•ì¸

### ë°ì´í„°ë² ì´ìŠ¤
- [ ] announcement_duplicate_log í…Œì´ë¸” ìƒì„±
- [ ] ì¸ë±ìŠ¤ í™•ì¸ (url_key_hash)
- [ ] ì™¸ë˜í‚¤ í™•ì¸ (preprocessing_id)

### í…ŒìŠ¤íŠ¸
- [ ] ì‹ ê·œ ì‚½ì… í…ŒìŠ¤íŠ¸
- [ ] êµì²´ (replaced) í…ŒìŠ¤íŠ¸
- [ ] ìœ ì§€ (kept_existing) í…ŒìŠ¤íŠ¸
- [ ] ë™ì¼ íƒ€ì… ì¤‘ë³µ í…ŒìŠ¤íŠ¸
- [ ] ì—ëŸ¬ ì¼€ì´ìŠ¤ í…ŒìŠ¤íŠ¸

### ë¶„ì„ ì¿¼ë¦¬
- [ ] URLë³„ ì´ë ¥ ì¡°íšŒ
- [ ] ì¼ë³„ í†µê³„
- [ ] ì‚¬ì´íŠ¸ë³„ í†µê³„
- [ ] ìš°ì„ ìˆœìœ„ ê²€ì¦
- [ ] ì—ëŸ¬ ë¶„ì„

### ë¬¸ì„œí™”
- [ ] ì½”ë“œ ì£¼ì„
- [ ] README ì—…ë°ì´íŠ¸
- [ ] ì¿¼ë¦¬ ìƒ˜í”Œ ë¬¸ì„œ

---

**ì‘ì„±ì¼**: 2025-11-01
**ìµœì¢… ì—…ë°ì´íŠ¸**: 2025-11-03
**ë²„ì „**: 1.1 (ì‹¤ì œ êµ¬í˜„ ë°˜ì˜)
**í•µì‹¬**: url_key_hash ê¸°ë°˜ ì¤‘ë³µ ì²´í¬ ë° ë¡œê·¸ ê¸°ë¡ (UPSERT ë°©ì‹, domain_key_config ì—°ë™)
**ìƒíƒœ**: êµ¬í˜„ ì™„ë£Œ ë° ìš´ì˜ ì¤‘

## ğŸ”„ ë³€ê²½ ì´ë ¥

### v1.1 (2025-11-03)
- UPSERT ë°©ì‹ ë°˜ì˜ (INSERT ... ON DUPLICATE KEY UPDATE)
- `unconfigured_domain` duplicate_type ì¶”ê°€
- `domain`, `domain_configured` íŒŒë¼ë¯¸í„° ì¶”ê°€
- url_key_hash GENERATED COLUMN ë°©ì‹ ë°˜ì˜
- Fallback ë¹„í™œì„±í™” ì •ì±… ë°˜ì˜
- ì‹¤ì œ ì½”ë“œ êµ¬í˜„ ë‚´ìš© ê¸°ì¤€ìœ¼ë¡œ ë¬¸ì„œ ì „ë©´ ìˆ˜ì •

### v1.0 (2025-11-01)
- ì´ˆê¸° ì„¤ê³„ ë¬¸ì„œ ì‘ì„±
