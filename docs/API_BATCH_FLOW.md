# daily_api_batch.sh ì‹¤í–‰ ì‹œ DB ë“±ë¡ íë¦„ ì ê²€ ë³´ê³ ì„œ

## âœ… ê²°ë¡ 

**`daily_api_batch.sh` ì‹¤í–‰ ë§Œìœ¼ë¡œ `announcement_pre_processing` í…Œì´ë¸”ì— ë°ì´í„°ê°€ ë“±ë¡ë©ë‹ˆë‹¤.**

---

## ğŸ” ì‹¤í–‰ íë¦„ ë¶„ì„

### 1ï¸âƒ£ daily_api_batch.sh ì‹¤í–‰

```bash
#!/bin/bash

API_DIR="/home/zium/moabojo/incremental/api"
SITES=("bizInfo" "smes24" "kStartUp")

# ê° ì‚¬ì´íŠ¸ ìˆœì°¨ ì²˜ë¦¬
for site in "${SITES[@]}"; do
    python3 announcement_pre_processor.py -d "$API_DIR" --site-code "$site"
done
```

**ì‹¤ì œ ì‹¤í–‰ ëª…ë ¹:**
```bash
# bizInfo ì²˜ë¦¬
python3 announcement_pre_processor.py -d /home/zium/moabojo/incremental/api --site-code bizInfo

# smes24 ì²˜ë¦¬
python3 announcement_pre_processor.py -d /home/zium/moabojo/incremental/api --site-code smes24

# kStartUp ì²˜ë¦¬
python3 announcement_pre_processor.py -d /home/zium/moabojo/incremental/api --site-code kStartUp
```

---

### 2ï¸âƒ£ announcement_pre_processor.py ë©”ì¸ í•¨ìˆ˜

**íŒŒì¼**: `announcement_pre_processor.py:2055-2143`

```python
def main():
    args = parser.parse_args()

    # ë””ë ‰í† ë¦¬ ì¡´ì¬ í™•ì¸
    base_directory = current_dir / args.directory
    # â†’ /home/zium/moabojo/incremental/api

    # site_type ê²°ì •
    site_type = determine_site_type(args.directory, args.site_code)
    # directory: "/home/zium/moabojo/incremental/api"
    # site_code: "bizInfo" (ë˜ëŠ” smes24, kStartUp)
    # â†’ site_type = "api_scrap"

    # í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
    processor = AnnouncementPreProcessor(
        site_type="api_scrap",  # â† ì—¬ê¸°
        attach_force=args.attach_force,
        site_code="bizInfo",
        lazy_init=False,
    )

    # ì‚¬ì´íŠ¸ ë””ë ‰í† ë¦¬ ì²˜ë¦¬ ì‹¤í–‰
    results = processor.process_site_directories(
        base_directory,  # /home/zium/moabojo/incremental/api
        args.site_code,  # bizInfo
        args.force       # False (ê¸°ë³¸ê°’)
    )
```

---

### 3ï¸âƒ£ process_site_directories() - ë””ë ‰í† ë¦¬ ê²€ìƒ‰

**íŒŒì¼**: `announcement_pre_processor.py:315-394`

```python
def process_site_directories(self, base_dir: Path, site_code: str, force: bool = False):
    # ì²˜ë¦¬í•  ë””ë ‰í† ë¦¬ ëª©ë¡ ì°¾ê¸°
    target_directories = self._find_target_directories(base_dir, site_code, force)
    # â†’ /home/zium/moabojo/incremental/api/bizInfo ë‚´ì˜ ëª¨ë“  content.md ìˆëŠ” ë””ë ‰í† ë¦¬

    # ê° ë””ë ‰í† ë¦¬ ìˆœíšŒ
    for directory in target_directories:
        # í´ë”ëª… ìƒì„± (ìƒëŒ€ ê²½ë¡œ)
        relative_path = directory.relative_to(site_dir)
        folder_name = str(relative_path).replace("/", "_")
        # ì˜ˆ: "2025-11-01_BIZ_ANNOUNCEMENT_001"

        # ì´ë¯¸ ì²˜ë¦¬ë¨ í™•ì¸ (forceê°€ Falseì¼ ë•Œë§Œ)
        if not force and self._is_already_processed(folder_name, site_code):
            continue  # ê±´ë„ˆëœ€

        # ë””ë ‰í† ë¦¬ ì²˜ë¦¬ ì‹¤í–‰
        success = self.process_directory_with_custom_name(
            directory,      # /home/zium/moabojo/incremental/api/bizInfo/2025-11-01/...
            site_code,      # bizInfo
            folder_name,    # 2025-11-01_BIZ_ANNOUNCEMENT_001
            force           # False
        )
```

**ì£¼ìš” ë¡œì§:**
- âœ… `content.md` íŒŒì¼ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ë§Œ ì²˜ë¦¬
- âœ… ì´ë¯¸ ì²˜ë¦¬ëœ í•­ëª©ì€ ìë™ ê±´ë„ˆëœ€ (ì¤‘ë³µ ë°©ì§€)
- âœ… ê° ë””ë ‰í† ë¦¬ë³„ë¡œ `process_directory_with_custom_name()` í˜¸ì¶œ

---

### 4ï¸âƒ£ process_directory_with_custom_name() - ë°ì´í„° ì¶”ì¶œ

**íŒŒì¼**: `announcement_pre_processor.py:423-691`

```python
def process_directory_with_custom_name(self, directory_path, site_code, folder_name, force):
    # 1. ì œì™¸ í‚¤ì›Œë“œ ì²´í¬
    excluded_keywords = self._check_exclusion_keywords(folder_name)

    # 2. API ì‚¬ì´íŠ¸ íŠ¹ìˆ˜ ì²˜ë¦¬ (bizInfo, smes24, kStartUp)
    if site_code in ["kStartUp", "bizInfo", "smes24"]:
        # content.md ì½ê¸°
        content_md_path = directory_path / "content.md"
        with open(content_md_path, "r", encoding="utf-8") as f:
            content_md = f.read()

        # content.mdì—ì„œ ì •ë³´ ì¶”ì¶œ
        title = self._extract_title_from_content(content_md)
        origin_url = self._extract_origin_url_from_content(content_md)
        scraping_url = self._extract_scraping_url_from_content(content_md)

        # JSON íŒŒì¼ì—ì„œ announcement_date ì¶”ì¶œ
        # ìš°ì„ ìˆœìœ„: announcement.json â†’ data.json â†’ info.json â†’ ê¸°íƒ€ .json
        json_files = ["announcement.json", "data.json", "info.json"]
        for json_name in json_files:
            json_path = directory_path / json_name
            if json_path.exists():
                json_data = json.load(open(json_path))
                announcement_date = self._convert_to_yyyymmdd(
                    json_data.get("announcementDate", "")
                )
                break

    # 3. URL ì •ê·œí™” (url_key ìƒì„±)
    url_key = self.url_key_extractor.extract_url_key(origin_url, site_code)

    # 4. ì²¨ë¶€íŒŒì¼ ì²˜ë¦¬
    combined_content, attachment_filenames, attachment_files_info = \
        self._process_attachments_separately(directory_path)

    # 5. ì œì™¸ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ì œì™¸ ì²˜ë¦¬ë¡œ ì €ì¥
    if excluded_keywords:
        return self._save_processing_result(
            folder_name, site_code, content_md, combined_content,
            status="ì œì™¸", exclusion_keywords=excluded_keywords, ...
        )

    # 6. ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ â† ì—¬ê¸°ì„œ DB ë“±ë¡!
    record_id = self._save_processing_result(
        folder_name,           # 2025-11-01_BIZ_ANNOUNCEMENT_001
        site_code,             # bizInfo
        content_md,            # content.md ë‚´ìš©
        combined_content,      # ì²¨ë¶€íŒŒì¼ ë‚´ìš©
        attachment_filenames,  # ["file1.pdf", "file2.hwp"]
        attachment_files_info, # [{"filename": "file1.pdf", "content": "..."}, ...]
        title,                 # ê³µê³  ì œëª©
        announcement_date,     # 20251101
        origin_url,            # https://www.bizinfo.go.kr/...
        url_key,               # ì •ê·œí™”ëœ URL
        scraping_url,          # ìŠ¤í¬ë˜í•‘í•œ URL
        status="ì„±ê³µ",
        force=False
    )
```

**í•µì‹¬ ë°ì´í„° ì¶”ì¶œ:**
- âœ… `content.md` íŒŒì¼ ì½ê¸°
- âœ… JSON íŒŒì¼ì—ì„œ ë‚ ì§œ ì •ë³´ ì¶”ì¶œ
- âœ… URL ì •ê·œí™” (url_key ìƒì„±)
- âœ… ì²¨ë¶€íŒŒì¼ ì²˜ë¦¬
- âœ… ì œì™¸ í‚¤ì›Œë“œ ì²´í¬

---

### 5ï¸âƒ£ _save_processing_result() - DB ì €ì¥

**íŒŒì¼**: `announcement_pre_processor.py:1650-1850`

#### INSERT ì¿¼ë¦¬ (force=False ì¼ ë•Œ)

```python
def _save_processing_result(self, folder_name, site_code, content_md, ...):
    with self.db_manager.SessionLocal() as session:
        # forceê°€ Falseì¸ ê²½ìš° (ê¸°ë³¸ê°’)
        sql = text("""
            INSERT INTO announcement_pre_processing (
                folder_name, site_type, site_code, content_md, combined_content,
                attachment_filenames, attachment_files_list, exclusion_keyword, exclusion_reason,
                title, origin_url, url_key, scraping_url, announcement_date,
                processing_status, error_message, created_at, updated_at
            ) VALUES (
                :folder_name, :site_type, :site_code, :content_md, :combined_content,
                :attachment_filenames, :attachment_files_list, :exclusion_keyword, :exclusion_reason,
                :title, :origin_url, :url_key, :scraping_url, :announcement_date,
                :processing_status, :error_message, NOW(), NOW()
            )
        """)

        # íŒŒë¼ë¯¸í„° ë°”ì¸ë”©
        params = {
            "folder_name": folder_name,               # 2025-11-01_BIZ_ANNOUNCEMENT_001
            "site_type": self.site_type,              # "api_scrap"
            "site_code": site_code,                   # "bizInfo"
            "content_md": content_md,                 # content.md ë‚´ìš©
            "combined_content": combined_content,     # ì²¨ë¶€íŒŒì¼ ë‚´ìš©
            "attachment_filenames": ",".join(attachment_filenames),
            "attachment_files_list": json.dumps(attachment_files_info),
            "exclusion_keyword": ",".join(exclusion_keywords) if exclusion_keywords else None,
            "exclusion_reason": exclusion_reason,
            "title": title,                           # ê³µê³  ì œëª©
            "origin_url": origin_url,                 # ì›ë³¸ URL
            "url_key": url_key,                       # ì •ê·œí™”ëœ URL
            "scraping_url": scraping_url,             # ìŠ¤í¬ë˜í•‘ URL
            "announcement_date": announcement_date,   # 20251101
            "processing_status": status,              # "ì„±ê³µ"
            "error_message": error_message,           # None
        }

        # ì¿¼ë¦¬ ì‹¤í–‰
        result = session.execute(sql, params)
        session.commit()

        # ì‚½ì…ëœ ë ˆì½”ë“œ ID ë°˜í™˜
        return result.lastrowid
```

#### UPSERT ì¿¼ë¦¬ (force=True ì¼ ë•Œ)

```python
# force=Trueì¸ ê²½ìš° ON DUPLICATE KEY UPDATE ì‚¬ìš©
sql = text("""
    INSERT INTO announcement_pre_processing (...)
    VALUES (...)
    ON DUPLICATE KEY UPDATE
        site_type = IF(
            VALUES(site_type) IN ('Eminwon', 'Homepage', 'Scraper') OR
            site_type NOT IN ('Eminwon', 'Homepage', 'Scraper'),
            VALUES(site_type),
            site_type
        ),
        content_md = IF(...),
        ... (ëª¨ë“  í•„ë“œì— ëŒ€í•´ ë™ì¼í•œ ìš°ì„ ìˆœìœ„ ë¡œì§)
        updated_at = NOW()
""")
```

**UPSERT ìš°ì„ ìˆœìœ„:**
- âœ… ì§€ìì²´ ì‚¬ì´íŠ¸ (Eminwon, Homepage, Scraper) > API ì‚¬ì´íŠ¸ (api_scrap)
- âœ… ì§€ìì²´ ë°ì´í„°ê°€ ìˆìœ¼ë©´ API ë°ì´í„°ë¡œ ë®ì–´ì“°ì§€ ì•ŠìŒ
- âœ… API ë°ì´í„°ê°€ ìˆê³  ì§€ìì²´ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì—…ë°ì´íŠ¸

---

## ğŸ“Š DB ì €ì¥ ë°ì´í„° ìƒì„¸

### announcement_pre_processing í…Œì´ë¸”ì— ì €ì¥ë˜ëŠ” ì»¬ëŸ¼

| ì»¬ëŸ¼ëª… | ë°ì´í„° ì˜ˆì‹œ | ì¶œì²˜ |
|--------|-----------|------|
| folder_name | `2025-11-01_BIZ_ANNOUNCEMENT_001` | ë””ë ‰í† ë¦¬ ìƒëŒ€ ê²½ë¡œ |
| site_type | `api_scrap` | `determine_site_type()` |
| site_code | `bizInfo` | ëª…ë ¹í–‰ ì¸ì |
| content_md | `# ê³µê³  ì œëª©\n\nê³µê³  ë‚´ìš©...` | content.md íŒŒì¼ |
| combined_content | `ì²¨ë¶€íŒŒì¼1 ë‚´ìš©\n\nì²¨ë¶€íŒŒì¼2 ë‚´ìš©...` | ì²¨ë¶€íŒŒì¼ ë³€í™˜ ê²°ê³¼ |
| attachment_filenames | `file1.pdf,file2.hwp` | attachments ë””ë ‰í† ë¦¬ |
| attachment_files_list | `[{"filename":"file1.pdf","content":"..."}]` | ì²¨ë¶€íŒŒì¼ ìƒì„¸ ì •ë³´ |
| exclusion_keyword | `NULL` ë˜ëŠ” `keyword1,keyword2` | ì œì™¸ í‚¤ì›Œë“œ ë§¤ì¹­ |
| exclusion_reason | `NULL` ë˜ëŠ” `ì œì™¸ ì‚¬ìœ ` | ì œì™¸ ì‚¬ìœ  |
| title | `2025ë…„ ì§€ì›ì‚¬ì—… ê³µê³ ` | content.mdì—ì„œ ì¶”ì¶œ |
| origin_url | `https://www.bizinfo.go.kr/...` | content.mdì—ì„œ ì¶”ì¶œ |
| url_key | `bizinfo.go.kr_web_lay1_bbs.do_bid=123` | URL ì •ê·œí™” ê²°ê³¼ |
| scraping_url | `https://www.bizinfo.go.kr/scrape/...` | content.mdì—ì„œ ì¶”ì¶œ |
| announcement_date | `20251101` | JSON íŒŒì¼ì—ì„œ ì¶”ì¶œ |
| processing_status | `ì„±ê³µ` | ì²˜ë¦¬ ê²°ê³¼ |
| error_message | `NULL` ë˜ëŠ” ì—ëŸ¬ ë©”ì‹œì§€ | ì˜ˆì™¸ ë°œìƒ ì‹œ |
| created_at | `2025-11-01 10:00:00` | NOW() |
| updated_at | `2025-11-01 10:00:00` | NOW() |

---

## ğŸ”„ ì „ì²´ íë¦„ ìš”ì•½

```
daily_api_batch.sh
    â†“
API_DIR="/home/zium/moabojo/incremental/api"
    â†“
for site in bizInfo, smes24, kStartUp
    â†“
python3 announcement_pre_processor.py -d $API_DIR --site-code $site
    â†“
main()
    â”œâ”€ determine_site_type() â†’ "api_scrap"
    â””â”€ AnnouncementPreProcessor(site_type="api_scrap")
        â†“
    process_site_directories()
        â”œâ”€ _find_target_directories() â†’ content.md ìˆëŠ” ë””ë ‰í† ë¦¬ ì°¾ê¸°
        â””â”€ for each directory:
            â†“
        process_directory_with_custom_name()
            â”œâ”€ content.md ì½ê¸°
            â”œâ”€ JSONì—ì„œ ë‚ ì§œ ì¶”ì¶œ
            â”œâ”€ URL ì •ê·œí™” (url_key)
            â”œâ”€ ì²¨ë¶€íŒŒì¼ ì²˜ë¦¬
            â”œâ”€ ì œì™¸ í‚¤ì›Œë“œ ì²´í¬
            â””â”€ _save_processing_result()
                â†“
            INSERT INTO announcement_pre_processing (
                folder_name, site_type, site_code, content_md, ...
            ) VALUES (...)
                â†“
            âœ… DB ì €ì¥ ì™„ë£Œ
```

---

## âœ… ì ê²€ ê²°ê³¼

### 1. DB ë“±ë¡ ì—¬ë¶€
**âœ… YES** - `daily_api_batch.sh` ì‹¤í–‰ ë§Œìœ¼ë¡œ `announcement_pre_processing` í…Œì´ë¸”ì— ë°ì´í„°ê°€ ë“±ë¡ë©ë‹ˆë‹¤.

### 2. ë“±ë¡ë˜ëŠ” í…Œì´ë¸”
- **í…Œì´ë¸”ëª…**: `announcement_pre_processing`
- **ìŠ¤í‚¤ë§ˆ**: folder_name (PK), site_type, site_code, content_md, combined_content, ...

### 3. site_type ê°’
- **ê°’**: `"api_scrap"`
- **ê²°ì • ê·¼ê±°**:
  1. site_codeê°€ `["bizInfo", "smes24", "kStartUp"]` ì¤‘ í•˜ë‚˜
  2. directoryì— `/incremental/api` í¬í•¨

### 4. ì¤‘ë³µ ì²˜ë¦¬
- **ì¤‘ë³µ ë°©ì§€**: `folder_name` + `site_code` ì¡°í•©ìœ¼ë¡œ ì´ë¯¸ ì²˜ë¦¬ëœ í•­ëª© ìë™ ê±´ë„ˆëœ€
- **ê°•ì œ ì¬ì²˜ë¦¬**: `--force` ì˜µì…˜ ì‚¬ìš© ì‹œ UPSERT ë¡œì§ìœ¼ë¡œ ì—…ë°ì´íŠ¸

### 5. ì²˜ë¦¬ ë²”ìœ„
- **ëŒ€ìƒ**: `/home/zium/moabojo/incremental/api/{bizInfo,smes24,kStartUp}/` ë‚´ì˜ ëª¨ë“  `content.md` ìˆëŠ” ë””ë ‰í† ë¦¬
- **ì œì™¸**: ì´ë¯¸ ì²˜ë¦¬ëœ í•­ëª©, ì œì™¸ í‚¤ì›Œë“œê°€ ìˆëŠ” í•­ëª©

---

## ğŸ§ª ê²€ì¦ ë°©ë²•

### í”„ë¡œë•ì…˜ ì„œë²„ì—ì„œ í…ŒìŠ¤íŠ¸

```bash
# SSH ì ‘ì†
ssh zium@server

# ë°°ì¹˜ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
cd /home/zium/classfy_scraper
./daily_api_batch.sh

# DB í™•ì¸
mysql -u $DB_USER -p$DB_PASSWORD $DB_NAME -e "
SELECT
    COUNT(*) as total,
    site_type,
    site_code,
    processing_status
FROM announcement_pre_processing
WHERE site_type = 'api_scrap'
GROUP BY site_type, site_code, processing_status
ORDER BY site_code, processing_status;
"

# ìµœê·¼ ë“±ë¡ëœ ë°ì´í„° í™•ì¸
mysql -u $DB_USER -p$DB_PASSWORD $DB_NAME -e "
SELECT
    id,
    folder_name,
    site_type,
    site_code,
    title,
    announcement_date,
    processing_status,
    created_at
FROM announcement_pre_processing
WHERE site_type = 'api_scrap'
ORDER BY created_at DESC
LIMIT 10;
"
```

### ì˜ˆìƒ ì¶œë ¥

```
[INFO] bizInfo ì²˜ë¦¬ ì‹œì‘
[1/50 : 2.0%] 2025-11-01_BIZ_ANNOUNCEMENT_001
  âœ“ ì²˜ë¦¬ ì™„ë£Œ (2.3ì´ˆ)
[2/50 : 4.0%] 2025-11-01_BIZ_ANNOUNCEMENT_002
  âœ“ ì´ë¯¸ ì²˜ë¦¬ë¨, ê±´ë„ˆëœ€ (0.1ì´ˆ)
...
[SUCCESS] bizInfo ì²˜ë¦¬ ì™„ë£Œ (ì´ 50ê°œ, ì„±ê³µ 30ê°œ, ê±´ë„ˆëœ€ 20ê°œ)

[INFO] smes24 ì²˜ë¦¬ ì‹œì‘
...
```

---

## ğŸ“‹ ìµœì¢… ê²°ë¡ 

### âœ… í™•ì¸ ì‚¬í•­
1. **DB ë“±ë¡**: YES - `announcement_pre_processing` í…Œì´ë¸”ì— ìë™ ë“±ë¡
2. **site_type**: `"api_scrap"` ê³ ì •ê°’
3. **ì¤‘ë³µ ë°©ì§€**: folder_name + site_code ì¡°í•©ìœ¼ë¡œ ìë™ ê±´ë„ˆëœ€
4. **ë°ì´í„° ì¶”ì¶œ**: content.md + JSON íŒŒì¼ì—ì„œ ëª¨ë“  í•„ë“œ ìë™ ì¶”ì¶œ
5. **ì²¨ë¶€íŒŒì¼ ì²˜ë¦¬**: attachments ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  íŒŒì¼ ë³€í™˜ ë° ì €ì¥

### ğŸ¯ ë™ì‘ ë³´ì¥
`daily_api_batch.sh` ì‹¤í–‰ ë§Œìœ¼ë¡œ:
- âœ… bizInfo, smes24, kStartUp ì„¸ ì‚¬ì´íŠ¸ì˜ ëª¨ë“  ê³µê³  ìë™ ì²˜ë¦¬
- âœ… `announcement_pre_processing` í…Œì´ë¸”ì— ìë™ ë“±ë¡
- âœ… ì¤‘ë³µ ë°ì´í„° ìë™ ë°©ì§€
- âœ… ì—ëŸ¬ ë°œìƒ ì‹œì—ë„ ë¡œê·¸ ë° error_message ì €ì¥

---

**ì‘ì„±ì¼**: 2025-11-01
**ë¶„ì„ ëŒ€ìƒ**: `daily_api_batch.sh` + `announcement_pre_processor.py`
**ê²°ë¡ **: âœ… ì •ìƒ ë™ì‘ - DB ìë™ ë“±ë¡ í™•ì¸
**ìš°ì„ ìˆœìœ„**: ğŸŸ¢ ì •ë³´ ì œê³µ
