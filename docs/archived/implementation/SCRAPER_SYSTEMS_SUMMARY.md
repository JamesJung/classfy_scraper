# ìŠ¤í¬ë˜í¼ ì‹œìŠ¤í…œ ì „ì²´ ì •ë¦¬

## ğŸ¯ êµ¬ì¶•ëœ 3ê°€ì§€ ì‹œìŠ¤í…œ

---

## 1ï¸âƒ£ ì‹¤íŒ¨ ê³µê³  ì¶”ì  ë° ì¬ì‹œë„ ì‹œìŠ¤í…œ

### ê°œìš”
ìŠ¤í¬ë˜í•‘ ì¤‘ ì‹¤íŒ¨í•œ ê°œë³„ ê³µê³ ë¥¼ DBì— ê¸°ë¡í•˜ê³  ì¬ì‹œë„í•˜ëŠ” ì‹œìŠ¤í…œ

### í•µì‹¬ íŒŒì¼
- `create_failed_announcements_table.sql` - ì‹¤íŒ¨ ê³µê³  ì €ì¥ í…Œì´ë¸”
- `node/scraper/failure_logger.js` - ì‹¤íŒ¨ ë¡œê¹… ëª¨ë“ˆ
- `patch_scrapers.js` - 156ê°œ ìŠ¤í¬ë˜í¼ ìë™ íŒ¨ì¹˜
- `patch_17_scrapers.js` - 17ê°œ íŠ¹ìˆ˜ ì¼€ì´ìŠ¤ ìˆ˜ë™ íŒ¨ì¹˜
- `retry_failed_announcements.py` - ì‹¤íŒ¨ ê³µê³  ì¬ì‹œë„
- `test_failure_logger.js` - í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

### DB í…Œì´ë¸”
```sql
scraper_failed_announcements
- id, batch_date, site_code
- announcement_title, announcement_url, detail_url
- error_type, error_message
- retry_count, status (pending/success/permanent_failure)
```

### ì‚¬ìš©ë²•
```bash
# 1. í…Œì´ë¸” ìƒì„±
mysql -u root -p classfy < create_failed_announcements_table.sql

# 2. ìŠ¤í¬ë˜í¼ ì‹¤í–‰ (ìë™ìœ¼ë¡œ ì‹¤íŒ¨ ê¸°ë¡)
node node/scraper/andong_scraper.js --site andong --year 2024

# 3. ì‹¤íŒ¨ ê³µê³  ì¬ì‹œë„
python3 retry_failed_announcements.py --site andong --date 2025-01-15
```

### ìƒíƒœ
âœ… **ì™„ë£Œ** - 156ê°œ ìŠ¤í¬ë˜í¼ ëª¨ë‘ íŒ¨ì¹˜ ì™„ë£Œ, êµ¬ë¬¸ ì˜¤ë¥˜ ì—†ìŒ

### ë¬¸ì„œ
ğŸ“„ [SCRAPER_FAILURE_RETRY_README.md](./SCRAPER_FAILURE_RETRY_README.md)

---

## 2ï¸âƒ£ ê±´ìˆ˜ ê²€ì¦ ì‹œìŠ¤í…œ

### ê°œìš”
ì˜ˆìƒ ê±´ìˆ˜ì™€ ì‹¤ì œ ìŠ¤í¬ë˜í•‘ ê±´ìˆ˜ë¥¼ ë¹„êµí•˜ì—¬ ë¶€ë¶„ ì‹¤íŒ¨ ê°ì§€

### í•µì‹¬ íŒŒì¼
- `create_count_validation_table.sql` - ê±´ìˆ˜ ê²€ì¦ í…Œì´ë¸”
- `node/scraper/count_validator.js` - ê±´ìˆ˜ ê²€ì¦ ëª¨ë“ˆ
- `run_scraper_with_validation.js` - ê²€ì¦ í¬í•¨ ìŠ¤í¬ë˜í•‘
- `test_count_validation.js` - í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

### DB í…Œì´ë¸”
```sql
scraper_count_validation
- id, batch_date, site_code
- expected_count, actual_count, failed_count
- status (counting/scraping/completed/mismatch)
- count_started_at, scrape_completed_at
```

### andong_scraper ì¶”ê°€ ë©”ì†Œë“œ
```javascript
// ì˜ˆìƒ ê±´ìˆ˜ ì¹´ìš´íŠ¸ (ë‹¤ìš´ë¡œë“œ ì—†ìŒ)
await scraper.countExpectedAnnouncements()
// â†’ { totalCount, pageCount }

// ì‹¤ì œ ìŠ¤í¬ë˜í•‘ (ì„±ê³µ ê±´ìˆ˜ ë°˜í™˜)
await scraper.scrape()
// â†’ { successCount, startCounter, endCounter }
```

### ì‚¬ìš©ë²•
```bash
# 1. í…Œì´ë¸” ìƒì„±
mysql -u root -p classfy < create_count_validation_table.sql

# 2. ê²€ì¦ í¬í•¨ ìŠ¤í¬ë˜í•‘ (ì¹´ìš´íŠ¸ â†’ ìŠ¤í¬ë˜í•‘ â†’ ê²€ì¦)
node run_scraper_with_validation.js --site andong --year 2024

# 3. ì¹´ìš´íŠ¸ ìƒëµí•˜ê³  ë°”ë¡œ ìŠ¤í¬ë˜í•‘
node run_scraper_with_validation.js --site andong --skip-count
```

### ê²€ì¦ ë¡œì§
```javascript
// 1. ì˜ˆìƒ ê±´ìˆ˜ ì¹´ìš´íŠ¸
const { totalCount } = await scraper.countExpectedAnnouncements();
await CountValidator.completeCounting(siteCode, totalCount, pageCount);

// 2. ì‹¤ì œ ìŠ¤í¬ë˜í•‘
const { successCount } = await scraper.scrape();

// 3. ê²€ì¦
const validation = await CountValidator.completeScraping(siteCode, successCount);
if (validation.mismatch) {
    console.log(`âš ï¸ ì˜ˆìƒ ${validation.expectedCount}ê°œ ì¤‘ ${validation.actualCount}ê°œë§Œ ì„±ê³µ`);
}
```

### ìƒíƒœ
âœ… **ì™„ë£Œ** - andong_scraperì— í†µí•© ì™„ë£Œ

---

## 3ï¸âƒ£ URL ì¶”ì¶œ ë° ì¤‘ë³µ ì²´í¬ ì‹œìŠ¤í…œ â­ NEW

### ê°œìš”
ì‹¤ì œ ìŠ¤í¬ë˜í•‘ ì „ì— ìƒì„¸ URLë§Œ ì¶”ì¶œí•˜ì—¬ DBì— ì €ì¥í•˜ê³  ì¤‘ë³µ ì²´í¬

### í•µì‹¬ íŒŒì¼
- `create_detail_urls_table.sql` - URL ì €ì¥ í…Œì´ë¸”
- `node/scraper/url_manager.js` - URL ê´€ë¦¬ ëª¨ë“ˆ
- `extract_urls.js` - URL ì¶”ì¶œ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
- `test_url_normalization.js` - URL ì •ê·œí™” í…ŒìŠ¤íŠ¸

### DB í…Œì´ë¸”
```sql
scraper_detail_urls
- id, batch_date, site_code
- title, list_url, detail_url
- normalized_url (page íŒŒë¼ë¯¸í„° ì œê±°)
- url_hash (SHA256)
- scraped (0: ë¯¸ì™„ë£Œ, 1: ì™„ë£Œ)
- UNIQUE KEY (site_code, url_hash, batch_date)
```

### URL ì •ê·œí™” ë¡œì§
**ì œê±°ë˜ëŠ” íŒŒë¼ë¯¸í„°:**
- page, pageNum, pageNo, pageIndex, startPage, currentPage, p, pg
- offset, start, from
- pageSize, size, limit
- isManager, isCharge

**ì˜ˆì‹œ:**
```
ì›ë³¸ 1: https://www.gg.go.kr/.../boardView.do?bIdx=123&page=10
ì›ë³¸ 2: https://www.gg.go.kr/.../boardView.do?bIdx=123&page=99
ì •ê·œí™”: https://www.gg.go.kr/.../boardView.do?bIdx=123
â†’ ê°™ì€ url_hash ìƒì„± â†’ ì¤‘ë³µìœ¼ë¡œ íŒë‹¨
```

### andong_scraper ì¶”ê°€ ë©”ì†Œë“œ
```javascript
// URLë§Œ ì¶”ì¶œí•˜ì—¬ DB ì €ì¥
await scraper.extractAndSaveUrls(batchDate)
// â†’ { totalCount, savedCount, pageCount }
```

### ì‚¬ìš©ë²•
```bash
# 1. í…Œì´ë¸” ìƒì„±
mysql -u root -p classfy < create_detail_urls_table.sql

# 2. URL ì •ê·œí™” í…ŒìŠ¤íŠ¸
node test_url_normalization.js
# â†’ âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼ í™•ì¸

# 3. URL ì¶”ì¶œ
node extract_urls.js --site andong --year 2024

# 4. ë‹¤ë¥¸ ì‚¬ì´íŠ¸ ì½”ë“œë¡œë„ ì‹¤í–‰
node extract_urls.js --site gg --year 2024
node extract_urls.js --site geoje --date 20240101

# 5. í†µê³„ í™•ì¸
SELECT site_code, COUNT(*) as total,
       SUM(scraped) as completed,
       SUM(1-scraped) as pending
FROM scraper_detail_urls
WHERE batch_date = '2025-01-15'
GROUP BY site_code;
```

### UrlManager API
```javascript
const UrlManager = require('./node/scraper/url_manager');

// URL ì •ê·œí™”
const normalized = UrlManager.normalizeUrl(url);

// URL ì €ì¥
await UrlManager.saveDetailUrl({
    site_code: 'andong',
    title: 'ê³µê³  ì œëª©',
    detail_url: detailUrl,
    batch_date: '2025-01-15'
});

// ì¤‘ë³µ ì²´í¬
const isDup = await UrlManager.isDuplicate('andong', detailUrl);

// ìŠ¤í¬ë˜í•‘ ì™„ë£Œ í‘œì‹œ
await UrlManager.markAsScraped('andong', detailUrl);

// ë¯¸ìŠ¤í¬ë˜í•‘ URL ì¡°íšŒ
const unscraped = await UrlManager.getUnscrapedUrls('andong', '2025-01-15', 100);

// í†µê³„
const stats = await UrlManager.getStats('andong', '2025-01-15');
// â†’ { total, scraped, unscraped }
```

### ìƒíƒœ
âœ… **ì™„ë£Œ** - í…ŒìŠ¤íŠ¸ í†µê³¼, andong_scraperì— í†µí•© ì™„ë£Œ

### ë¬¸ì„œ
ğŸ“„ [URL_EXTRACTION_README.md](./URL_EXTRACTION_README.md)

---

## ğŸ”„ ì›Œí¬í”Œë¡œìš° í†µí•©

### ì „ì²´ ìŠ¤í¬ë˜í•‘ í”„ë¡œì„¸ìŠ¤

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1ë‹¨ê³„: URL ì¶”ì¶œ                             â”‚
â”‚ extract_urls.js                            â”‚
â”‚ - ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ ìˆœíšŒ                        â”‚
â”‚ - ìƒì„¸ URL ìƒì„± ë° ì •ê·œí™”                   â”‚
â”‚ - scraper_detail_urlsì— ì €ì¥               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2ë‹¨ê³„: ê±´ìˆ˜ ì¹´ìš´íŠ¸ (ì„ íƒ)                   â”‚
â”‚ countExpectedAnnouncements()               â”‚
â”‚ - ì˜ˆìƒ ê±´ìˆ˜ í™•ì¸                            â”‚
â”‚ - scraper_count_validationì— ê¸°ë¡          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3ë‹¨ê³„: ì‹¤ì œ ìŠ¤í¬ë˜í•‘                        â”‚
â”‚ scrape()                                   â”‚
â”‚ - URL ì¤‘ë³µ ì²´í¬ (UrlManager)               â”‚
â”‚ - ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ                         â”‚
â”‚ - content.md ìƒì„±                          â”‚
â”‚ - ì‹¤íŒ¨ ì‹œ scraper_failed_announcements ê¸°ë¡â”‚
â”‚ - ì„±ê³µ ì‹œ markAsScraped() í˜¸ì¶œ             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4ë‹¨ê³„: ê²€ì¦ ë° ì¬ì‹œë„                       â”‚
â”‚ - ê±´ìˆ˜ ê²€ì¦ (expected vs actual)           â”‚
â”‚ - ì‹¤íŒ¨ ê³µê³  ì¬ì‹œë„ (retry_failed_...)      â”‚
â”‚ - ë¯¸ìŠ¤í¬ë˜í•‘ URL ì¬ì²˜ë¦¬                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š DB í…Œì´ë¸” ê´€ê³„

```
scraper_detail_urls (URL ëª©ë¡)
â”œâ”€ site_code, batch_date
â”œâ”€ detail_url, normalized_url, url_hash
â””â”€ scraped (0/1)
    â”‚
    â”œâ”€â†’ scraper_count_validation (ê±´ìˆ˜ ê²€ì¦)
    â”‚   â”œâ”€ expected_count (URL ì¶”ì¶œ ê±´ìˆ˜)
    â”‚   â”œâ”€ actual_count (ìŠ¤í¬ë˜í•‘ ì„±ê³µ ê±´ìˆ˜)
    â”‚   â””â”€ failed_count
    â”‚
    â””â”€â†’ scraper_failed_announcements (ì‹¤íŒ¨ ê³µê³ )
        â”œâ”€ detail_url (scraper_detail_urlsì™€ ì—°ê²°)
        â”œâ”€ error_type, error_message
        â””â”€ retry_count, status
```

---

## ğŸš€ ê¶Œì¥ ì‹¤í–‰ ìˆœì„œ

### ì‹ ê·œ ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í•‘

```bash
# 1. URL ì¶”ì¶œ (ë¹ ë¦„, ë‹¤ìš´ë¡œë“œ ì—†ìŒ)
node extract_urls.js --site andong --year 2024

# 2. ì¶”ì¶œëœ URL í™•ì¸
SELECT COUNT(*) FROM scraper_detail_urls
WHERE site_code = 'andong' AND batch_date = CURDATE();

# 3. ê±´ìˆ˜ ê²€ì¦ í¬í•¨ ìŠ¤í¬ë˜í•‘
node run_scraper_with_validation.js --site andong --year 2024

# 4. ì‹¤íŒ¨ ê³µê³  ì¬ì‹œë„
python3 retry_failed_announcements.py --site andong

# 5. ìµœì¢… ê²€ì¦
SELECT
    (SELECT COUNT(*) FROM scraper_detail_urls WHERE site_code='andong' AND scraped=1) as scraped,
    (SELECT COUNT(*) FROM scraper_detail_urls WHERE site_code='andong' AND scraped=0) as unscraped,
    (SELECT COUNT(*) FROM scraper_failed_announcements WHERE site_code='andong' AND status='pending') as failed;
```

### ê¸°ì¡´ ìŠ¤í¬ë˜í•‘ (URL ì¶”ì¶œ ìƒëµ)

```bash
# ë°”ë¡œ ìŠ¤í¬ë˜í•‘ (ê¸°ì¡´ ë°©ì‹ê³¼ ë™ì¼)
node node/scraper/andong_scraper.js --site andong --year 2024

# ì‹¤íŒ¨ ê³µê³  ì¬ì‹œë„
python3 retry_failed_announcements.py --site andong
```

---

## ğŸ“‹ ì²´í¬ë¦¬ìŠ¤íŠ¸

### ì‹œìŠ¤í…œ 1: ì‹¤íŒ¨ ê³µê³  ì¶”ì 
- [x] DB í…Œì´ë¸” ìƒì„±
- [x] failure_logger.js êµ¬í˜„
- [x] 156ê°œ ìŠ¤í¬ë˜í¼ íŒ¨ì¹˜
- [x] ì¬ì‹œë„ ë¡œì§ êµ¬í˜„
- [x] í…ŒìŠ¤íŠ¸ ì™„ë£Œ

### ì‹œìŠ¤í…œ 2: ê±´ìˆ˜ ê²€ì¦
- [x] DB í…Œì´ë¸” ìƒì„±
- [x] count_validator.js êµ¬í˜„
- [x] andong_scraper í†µí•©
- [x] ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±
- [x] í…ŒìŠ¤íŠ¸ ì™„ë£Œ

### ì‹œìŠ¤í…œ 3: URL ì¶”ì¶œ ë° ì¤‘ë³µ ì²´í¬
- [x] DB í…Œì´ë¸” ìƒì„±
- [x] url_manager.js êµ¬í˜„
- [x] URL ì •ê·œí™” ë¡œì§
- [x] andong_scraper í†µí•©
- [x] ì¶”ì¶œ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±
- [x] í…ŒìŠ¤íŠ¸ ì™„ë£Œ (8/8 í†µê³¼)

---

## ğŸ› ï¸ ìœ ì§€ë³´ìˆ˜

### ìƒˆë¡œìš´ page ê´€ë ¨ íŒŒë¼ë¯¸í„° ì¶”ê°€
`node/scraper/url_manager.js` ìˆ˜ì •:
```javascript
const pageParams = [
    'page', 'pageNum', // ê¸°ì¡´
    'newPageParam' // ì¶”ê°€
];
```

### ìƒˆë¡œìš´ ì‚¬ì´íŠ¸ ì½”ë“œ ì¶”ê°€
```bash
# URL ì¶”ì¶œ
node extract_urls.js \
  --site new_site \
  --url "https://..." \
  --list-selector "..." \
  --title-selector "..." \
  --date-selector "..." \
  --year 2024
```

### ì‹¤íŒ¨ìœ¨ ëª¨ë‹ˆí„°ë§
```sql
-- ì¼ë³„ ì‹¤íŒ¨ìœ¨
SELECT
    batch_date,
    site_code,
    COUNT(*) as total,
    SUM(CASE WHEN status='pending' THEN 1 ELSE 0 END) as failed,
    ROUND(SUM(CASE WHEN status='pending' THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as fail_rate
FROM scraper_failed_announcements
WHERE batch_date >= DATE_SUB(CURDATE(), INTERVAL 7 DAY)
GROUP BY batch_date, site_code
ORDER BY fail_rate DESC;
```

---

## ğŸ“š ê´€ë ¨ ë¬¸ì„œ

1. [SCRAPER_FAILURE_RETRY_README.md](./SCRAPER_FAILURE_RETRY_README.md) - ì‹¤íŒ¨ ê³µê³  ì¬ì‹œë„ ì‹œìŠ¤í…œ
2. [URL_EXTRACTION_README.md](./URL_EXTRACTION_README.md) - URL ì¶”ì¶œ ë° ì¤‘ë³µ ì²´í¬ ì‹œìŠ¤í…œ
3. [test_failure_logger.js](./test_failure_logger.js) - ì‹¤íŒ¨ ë¡œê±° í…ŒìŠ¤íŠ¸
4. [test_count_validation.js](./test_count_validation.js) - ê±´ìˆ˜ ê²€ì¦ í…ŒìŠ¤íŠ¸
5. [test_url_normalization.js](./test_url_normalization.js) - URL ì •ê·œí™” í…ŒìŠ¤íŠ¸

---

## ğŸ‰ ì™„ì„±ëœ ê¸°ëŠ¥

âœ… **ê°œë³„ ê³µê³  ì‹¤íŒ¨ ì¶”ì ** - ì–´ë–¤ ê³µê³ ê°€ ì‹¤íŒ¨í–ˆëŠ”ì§€ ì •í™•íˆ íŒŒì•…
âœ… **ìë™ ì¬ì‹œë„** - ì‹¤íŒ¨í•œ ê³µê³  ìë™ ì¬ì²˜ë¦¬
âœ… **ê±´ìˆ˜ ê²€ì¦** - ì˜ˆìƒ vs ì‹¤ì œ ê±´ìˆ˜ ë¹„êµë¡œ ë¶€ë¶„ ì‹¤íŒ¨ ê°ì§€
âœ… **URL ì¤‘ë³µ ì œê±°** - page íŒŒë¼ë¯¸í„° ë¬´ì‹œ, ê°™ì€ ê³µê³  ì¤‘ë³µ ë°©ì§€
âœ… **ì§„í–‰ ìƒí™© ì¶”ì ** - ì–´ë””ê¹Œì§€ ìŠ¤í¬ë˜í•‘í–ˆëŠ”ì§€ ì‹¤ì‹œê°„ í™•ì¸
âœ… **ë‹¤ì¤‘ ì‚¬ì´íŠ¸ ì§€ì›** - ëª¨ë“  site_codeì— ì ìš© ê°€ëŠ¥

---

**êµ¬ì¶• ì™„ë£Œ!** ğŸŠ
